% -*- latex -*-

%if style == newcode
module LinearConstraints where

\begin{code}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE ConstraintKinds #-}
{-# LANGUAGE RankNTypes #-}
{-# LANGUAGE TypeOperators #-}
{-# LANGUAGE KindSignatures #-}
{-# LANGUAGE MultiParamTypeClasses #-}

import Data.Kind (Constraint)
--import GHC.IO.Unsafe
import GHC.Base
\end{code}
%endif

\documentclass[acmsmall,usenames,dvipsnames,natbib=false,acmthm=false,screen]{acmart}
% \settopmatter{printacmref=false,printfolios=true}

% \usepackage[backend=biber,citestyle=authoryear,style=alphabetic]{biblatex}
\usepackage[backend=biber,datamodel=acmdatamodel, style=acmauthoryear,uniquename=false]{biblatex}
\addbibresource{bibliography.bib}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
  }
\usepackage[plain]{fancyref}
\usepackage[capitalize]{cleveref}
\usepackage{mathpartir}
\usepackage{newunicodechar}
\input{newunicodedefs}

%%%%%%%%%%%%%%%%% ott %%%%%%%%%%%%%%%%%

\usepackage[supertabular,implicitLineBreakHack]{ottalt}
\inputott{ott.tex}

%%%%%%%%%%%%%%%%% /ott %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% Workaround %%%%%%%%%%%%%%%%%

% This should be handled by the acmclass article, there are a couple
% of issues about
% this. https://github.com/borisveytsman/acmart/issues/271,
% https://github.com/borisveytsman/acmart/issues/327 . Both have been
% merged long ago, and the version of acmart in the shell.nix is from
% 2020.

%% \usepackage{fontspec}
%% \setmainfont{Linux Libertine O}
%% \setsansfont{Linux Biolinum O}
%% \setmonofont{inconsolata}

%%%%%%%%%%%%%%%%% /Workaround %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% lhs2tex %%%%%%%%%%%%%%%%%

\let\Bbbk\undefined    % see https://github.com/kosmikus/lhs2tex/issues/82
%include polycode.fmt
%if style == poly
%format a_i = "\Varid a \ensuremath{_i}"
%format a_j = "\Varid a \ensuremath{_j}"
%format ->. = "⊸"
%format => = "\FatArrow "
%format =>. = "\Lolly "
%format poly_arrow = "\mathop{\to_{\multiplicityfont{m}}}"
%format .<= = "\RLolly"
%format <== = "\RFatArrow"
%format omega = "\omega"
%format IOL = "IO_L"
%format . = "."
%format exists = "\exists"
%format forall = "\forall"
%format pack x = "\packbox" x
%format pack' = "\kpack!"
%format Unique = "\constraintfont{" Linearly "}"
%format unique = linearly
%format constraint (c) = "\constraintfont{" c "}"
%format multiplicity (p) = "\multiplicityfont{" p "}"
%
%format a1
%format a_n
%format an = a_n
%format ^^ = "\,"
%format e1
%format e2
%
%format ! = "!"
  %% ^^ this suppresses some space. I think lhs2TeX would otherwise use \mathop
%format spack = "!\kpack"
%format (at n) = "{@}" n
%format phantom x = "\phantom{" x "}"
%endif

%let full = False

%%%%%%%%%%%%%%%%% /lhs2tex %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  % TOGGLE ME to turn off all the commentary:
  \InputIfFileExists{no-editing-marks}{
    \def\noeditingmarks{}
  }

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      \setlength{\marginparwidth}{1.2cm} % A size that matches the new PACMPL format
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\csongor}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\rae}[2][1=]{\todo[linecolor=magenta,backgroundcolor=magenta!25,bordercolor=magenta,#1]{RAE: #2}}
      \newcommandx{\nw}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{NW: #2}}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2][1=]{}
      \newcommand{\info}[2][1=]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}

      \newcommand{\csongor}[2][1=]{}
      \newcommand{\jp}[2][1=]{}
      \newcommandx{\rae}[2][1=]{}
      \newcommandx{\nw}[2][1=]{}
  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Domain-specific macros %%%%%%%%%%%%%%%%%

  % Fonts and colours
  \newcommand{\constraintcolour}{\color{RoyalBlue}}
  \newcommand{\constraintfont}[1]{{\constraintcolour#1}}
  \newcommand{\multiplicitycolour}{\color{RoyalBlue}}
  \newcommand{\multiplicityfont}[1]{{\multiplicitycolour#1}}

  % Utilities
  \newcommand{\revop}[1]{\mathop{\reflectbox{\ensuremath{#1}}}}

  % Notations
  \newcommand{\cscheme}[1]{\mathcal{#1}}
  \newcommand{\rlolly}{\mathop{\revop{\multimap}}}
  \newcommand{\subst}[2]{[#1]#2}
  \newcommand{\sby}[2]{#1 ↦ #2}
  % \newcommand{\vdashi}{⊢_{\mathsf{i}}}
  \newcommand{\vdashi}{%
      \mathrel{%
          \vdash\hspace*{-4pt}%
          \raisebox{0.9pt}{\scalebox{.66}{\(\blacktriangleright\)}}%
      }%
  }
  \newcommand{\vdashs}{⊢_{\mathsf{s}}}
  \newcommand{\vdashsimp}{⊢_{\mathsf{s}}^{\mathsf{a}}}
  \newcommand{\scale}{\constraintfont{\cdot}}
  %% Constraint notations
  \newcommand{\constraintop}[1]{\mathop{\constraintfont{#1}}}
  \newcommand{\aand}{\constraintop{\&}}
  \DeclareMathOperator*{\bigaand}{\vcenter{\hbox{\Large\&}}}
  \newcommand{\lollycirc}{\raisebox{-0.255ex}{\scalebox{1.4}{$\circ$}}}
  \newcommand{\Lolly}{\constraintop{=\kern-1.1ex \lollycirc}}
  \newcommand{\FatArrow}{\constraintop{\Rightarrow}}
  \newcommand{\RLolly}{\mathop{\constraintfont \circledless}}
  \newcommand{\RFatArrow}{\constraintop{\rtimes}}
  \newcommand{\qtensor}{\constraintop{\otimes}}
  %% Desugarer notations
  \newcommand{\dsterm}[2]{\llbracket #2 \rrbracket_{#1}}
  \newcommand{\dstype}[1]{\llbracket #1 \rrbracket}
  \newcommand{\dsevidence}[1]{\llbracket #1 \rrbracket^{\mathbf{ev}}}

  % language keywords
  \newcommand{\keyword}[1]{\mathbf{#1}}
  \newcommand{\klet}{\keyword{let}}
  \newcommand{\kcase}{\keyword{case}}
  \newcommand{\kwith}{\keyword{with}}
  % \newcommand{\packbox}{\square}
  \newcommand{\packbox}{\raisebox{-0.08ex}{\scalebox{0.88}{$\square$}}}
  \newcommand{\kin}{\keyword{in}}
  \newcommand{\kof}{\keyword{of}}
  %% Pseudo-keywords for use in text
  \newcommand{\kunpack}{\klet\packbox}

  % defining grammars
  \newcommand{\bnfeq}{\mathrel{\Coloneqq}}
  \newcommand{\bnfor}{\mathrel{\mid}}

  % theorems
  \theoremstyle{acmplain}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{corollary}[theorem]{Corollary}
  \newtheorem{property}[theorem]{Property}

  \theoremstyle{acmdefinition}
  \newtheorem{definition}[theorem]{Definition}

%%%%%%%%%%%%%%%%% /Domain-specific macros %%%%%%%%%%%%%%%%%

\setcopyright{rightsretained}
\acmPrice{}
\acmDOI{10.1145/3547626}
\acmYear{2022}
\copyrightyear{2022}
\acmSubmissionID{icfp22main-p15-p}
\acmJournal{PACMPL}
\acmVolume{6}
\acmNumber{ICFP}
\acmArticle{95}
\acmMonth{8}

% Commented out because of biblatex
% \citestyle{acmauthoryear}

\begin{document}

\title[Linearly Qualified Types: Generic Inference for Capabilities and Uniqueness]{Linearly Qualified Types}
\subtitle{Generic Inference for Capabilities and Uniqueness}

\author{Arnaud Spiwack}
\orcid{0000-0002-5985-2086}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{arnaud.spiwack@@tweag.io}
\author{Csongor Kiss}
\orcid{0000-0002-0195-2420}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{csongor.kiss14@@imperial.ac.uk}
\author{Jean-Philippe Bernardy}
\orcid{0000-0002-8469-5617}
\affiliation{
  \institution{University of Gothenburg}
  \city{Gothenburg}
  \country{Sweden}
}
\email{jean-philippe.bernardy@@gu.se}
\author{Nicolas Wu}
\orcid{0000-0002-4161-985X}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{n.wu@@imperial.ac.uk}
\author{Richard A.~Eisenberg}
\orcid{0000-0002-7669-9781}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{rae@@richarde.dev}

\keywords{GHC, Haskell, linear logic, linear types,
  constraints, qualified types, inference}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024</concept_id>
       <concept_desc>Software and its engineering~Language features</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011039</concept_id>
       <concept_desc>Software and its engineering~Formal language definitions</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Language features}
\ccsdesc[300]{Software and its engineering~Functional languages}
\ccsdesc[300]{Software and its engineering~Formal language definitions}

\begin{abstract}
  A linear parameter must be consumed exactly once in the body of its
  function. When declaring resources such as file handles and manually
  managed memory as linear arguments, a linear type system can verify
  that these resources are used safely.
  However, writing code with explicit linear arguments requires bureaucracy.
  %
  This paper presents \emph{linear constraints}, a front-end feature for
  linear typing that
  decreases the bureaucracy of working with linear types.
  %
  Linear constraints are implicit linear arguments that are
  filled in automatically by the compiler.
  %
  We present linear constraints as a qualified type system,
  together with an inference algorithm which extends
  \textsc{ghc}'s existing constraint solver algorithm. Soundness of
  linear constraints is ensured by the fact that they desugar into
  Linear Haskell.
\end{abstract}

\maketitle

% The full names fit on a line. As per the author instructions, we
% don't need to shorten the authors' names.
% \renewcommand{\shortauthors}{Bernardy, Eisenberg, Kiss, Spiwack, Wu}
\newcommand{\maybesmall}{\small}

\section{Introduction}
\label{sec:introduction}

Linear type systems have seen a renaissance in recent years in
various programming communities. Rust's ownership system guarantees
memory safety for systems programmers, Haskell's \textsc{ghc}~9.0 includes
support for linear types, and even
dependently typed programmers can now use linear types with Idris 2.
All of these systems are vastly different in ergonomics and
scope. Rust uses dedicated syntax and code generation
to support management of resources, while Linear Haskell is a
type system change without any other impact on the compiler, such as in the
code generator or runtime system.\rae{But actually we'd \emph{like} to have
an impact on the runtime system to allow update-in-place.} Linear Haskell
is designed to be general purpose, but
using its linear arguments to emulate Rust's ownership model is a tedious exercise. It
requires
% the compiler doesn't know how to help, requiring
the programmer to carefully thread resource tokens.

To get a sense of the power and the tedium of programming with linear types, consider the
following function:
\begin{code}
read2AndDiscard :: MArray a ->. (Ur a, Ur a)
read2AndDiscard arr0 =
  let  (arr1, x)  = read arr0 0
       (arr2, y)  = read arr1 1
       ()         = free arr2
  in (x, y)
\end{code}
This function reads the first two elements of an array and returns them after
deallocating the array.
Linearity enables the array library to ensure that there is only one reference
to the array, and therefore it can be mutated in-place without violating
referential transparency. Let us stress that this uniqueness property
is an invariant of the array library, not an intrinsic property of
linear functions.

After the array has been freed, it is no longer
possible to read or write to it.
%
Notice that the |read| function consumes the array and returns a
fresh array, to be used in future operations. Operationally, the array remains
the same, but each operation assigns a new name to it, thus facilitating tracking
references statically. Finally, |free| consumes the array without returning a
new one, statically guaranteeing that it can no longer be used.
The values |x| and |y| read from the array are returned; their
types include elements wrapped by the |Ur|
(pronounced ``unrestricted'',  and corresponding to the
``!'' operator of \citet{girard-linear-logic}) type, allowing them to be used
arbitrarily many times. This works because |read2AndDiscard| takes a restricted-use array
containing unrestricted elements.
%
In a non-linear language, one would have to forgo referential transparency to
handle mutable operations either by using a monadic interface or allowing
arbitrary effects.
Compare the above function with what one would write in a non-linear, impure
language:
\begin{code}
read2AndDiscard :: MArray a -> (a, a)
read2AndDiscard arr =
  let   x    = read arr 0
        y    = read arr 1
        ()   = unsafeFree arr
  in (x, y)
\end{code}
This non-linear version does not guarantee that there is a unique reference to
the array, so freeing the array is a potentially unsafe operation.
However, it is simpler because there is less bureaucracy to manage: we are
clearly interacting with the \emph{same} array throughout, and this version makes
that apparent.
We see here a clear tension between extra safety and clarity of code---one
we wish, as language designers, to avoid. %%  When
%% modelling a handle as a linear resource, the type system must know at all
%% times where it is being consumed, so the file handle is passed around
%% manually, resulting in extra noise.
%
% Reading the non-linear version, it is clear that it keeps a single reference to
% the array.
How can we get the compiler to see that the array is used safely
without explicit threading?

%% Rust introduces the \emph{borrow checker} for this very purpose. % seems out of place when referring to 1999 ideas.
Following well-known ideas~\citep{DBLP:conf/popl/CraryWM99,DBLP:conf/esop/SmithWM00,DBLP:conf/tic/WalkerM00}, our approach is to let arrays be unrestricted, but
associate linear capabilities (such as |constraint(Read)|, |constraint(Write)|) to them.
In fact, we show in this paper that such linear capabilities
are the natural analogue of Haskell's type class
constraints to the setting of linear types.
We call these new constraints \emph{linear constraints}.
Like class constraints,
linear constraints are propagated implicitly by the compiler.
Like linear arguments, they can safely be used to track resources such as arrays
or file handles. Thus, linear constraints are the combination of these two
concepts, which have been studied independently elsewhere
\citep{OutsideIn,LinearHaskell,hh-ll,resource-management-for-ll-proof-search}.

With our extension, we can write a new pure version of |read2AndDiscard| which does
not require explicit threading of the array:
\begin{code}
read2AndDiscard :: constraint ((Read n, Write n)) =>. UArray a n -> (Ur a, Ur a)
read2AndDiscard arr =
  let   pack x   =  read arr 0
        pack y   =  read arr 1
        pack ()  =  free arr
  in (x, y)
\end{code}
The only changes from the impure version are that this version explicitly requires having
read and write access to the array,
and pattern-matching against $\packbox$ (read ``pack'') is necessary in order to access the linear constraint
packed in the result of |read| and |free|. (\Fref{sec:implicit-existentials}
suggests how we can get rid of $\packbox$, too.)
Crucially, the resource representing the ownership of the
array is a linear constraint and is separate from the array itself, which no
longer needs to be threaded manually.

Our contributions are as follows:
\begin{itemize}
\item A system of qualified types that allows a constraint assumption
  to be given a multiplicity (linear or unrestricted). Linear assumptions are used precisely
  once in the body of a definition
  (\Fref{sec:qualified-type-system}). This system supports examples
  that have motivated the design of several resource-aware systems,
  such as ownership \textit{à la} Rust (\Fref{sec:memory-ownership}), or
  capabilities in the style of Mezzo~\cite{mezzo-permissions} or
  \textsc{ats}~\cite{AtsLinearViews}; accordingly, our system points
  towards a possible unification of these lines of research.

\item Applications of this qualified type system to allow writing
  \begin{itemize}
  \item resource-aware algorithms without explicit threading (\Fref{sec:memory-ownership}); and
  \item functions whose result can only be used linearly (\Fref{sec:Unique-constraint})
  \end{itemize}

\item An inference algorithm that respects the multiplicity of
  assumptions. We prove that this algorithm is sound with respect to
  our type system~(\Fref{sec:type-inference}). It consists of
  \begin{itemize}
  \item a constraint generation
    algorithm~(\cref{sec:constraint-generation}). The language of
    generated constraints tracks multiplicities.
  \item a solver~(\cref{sec:constraint-solver}) for the generated
    constraints, which restricts proof-search algorithms for linear
    logic in order to be \emph{guess
      free}~\cite[Section~6.4]{OutsideIn}. A guess-free algorithm
    ensures that constraint inference is predictable and insensitive
    to small changes in the source program; it is necessarily
    incomplete.
  \end{itemize}
\end{itemize}
%
Our language is given semantics by desugaring into a core language based
on that of \citet{LinearHaskell}.
Our design is intended to work well with other features of Haskell and
\textsc{ghc} extensions. Indeed, we have a prototype implementation (\cref{sec:implementation}).

\section{Background: Linear Haskell}
\label{sec:linear-types}

This section, mostly cribbed from \citet[Section 2.1]{LinearHaskell},
describes our baseline approach, as released in \textsc{ghc}~9.0.
%
Linear Haskell adds a new type of functions,
dubbed \emph{linear functions}, and written |a ⊸ b|.\footnote{The linear function
  type and its notation come from linear
  logic~\cite{girard-linear-logic}, to which the phrase \emph{linear
    types} refers. All the various design of linear typing in the
  literature amount to adding such a linear function type, but details
  can vary wildly. See~\citet[Section 6]{LinearHaskell} for an analysis
  of alternative approaches.} A linear function consumes its
  argument exactly once. Linear Haskell defines it as follows: % "thusly" is very rare; let's not summon our inner Shakespeare just yet.

\begin{quote}
% \emph{Meaning of the linear arrow}: %JP: redundant
|f :: a ⊸ b| guarantees that if |(f u)| is consumed exactly once,
then the argument |u| is consumed exactly once.
\end{quote}
To make sense of this statement we need to know what ``consumed exactly once'' means.
Our definition is based on the type of the value concerned:
\begin{definition}[Consume exactly once]~ \label{def:consume}
\begin{itemize}
\item To consume a value of atomic base type (like |Int|) exactly once, just evaluate it.
\item To consume a function exactly once, apply it to one argument, and then consume its result exactly once.
\item To consume a pair exactly once, pattern-match on it, and then consume each component exactly once.
\item In general, to consume a value of an algebraic datatype exactly once, pattern-match on it,
  and then consume all its linear components exactly once.
\end{itemize}
\end{definition}
%
\subsection{Multiplicities}
\label{sec:multiplicities}
The usual arrow type |a -> b| can be recovered using |Ur|, as |Ur a ⊸ b|, but Linear
Haskell provides a first-class treatment of |a -> b|, thus ensuring
backwards compatibility with Haskell. In practice, the type-checker
records the \emph{multiplicity} of every introduced variable: \(1\) for linear
arguments and \(ω\) for unrestricted ones. This way, one can give a unified treatment
of both arrow types~\citep{linearity-and-pi-calculus}.
$$
\begin{array}{lcll}
  [[{pi}]], [[{rho}]] & \bnfeq & [[{1}]] \bnfor [[{omega}]] & \text{Multiplicities}
\end{array}
$$

We stress that a multiplicity of \(1\) restricts \emph{how the variable can be used}. It does not
restrict \emph{which values can be substituted for it}.
In particular, a linear function cannot assume that it is given the
unique pointer to its argument.  For example, if |f :: a ⊸ b|, then
the following is fine:
\begin{code}
g :: a -> (b, b)
g x = (f x, f x)
\end{code}
The type of |g| makes no guarantees about how it uses |x|.
In particular, |g| can pass |x| to |f|.

Pattern matching on a value of type |Ur a| yields a payload of multiplicity
$[[{omega}]]$, even when the scrutinee has multiplicity $[[{1}]]$. In general, given a
multiplicity set, the desired
(sub)structural rules can be obtained by endowing multiplicities with the appropriate
semiring structure~\citep{abel_unified_2020}. In
this paper, we use the same multiplicity structure as Linear Haskell:\footnote{Even though linear Haskell additionally supports multiplicity
polymorphism, we do not support multiplicity polymorphism on
constraint arguments.  Linear Haskell takes advantage of multiplicity
polymorphism to avoid duplication of higher-order functions. The
prototypical example is |map :: (a poly_arrow b) -> [a] poly_arrow
[b]|, where |poly_arrow| is the notation for a function arrow of
multiplicity |multiplicity(m)|.  First-order functions, on the other
hand, do not need multiplicity polymorphism, because linear functions
can be $\eta$-expanded into unrestricted functions as explained in
\cref{sec:linear-types}. Higher-order functions whose arguments are
themselves constrained functions are rare, so we do not yet see the
need to extend multiplicity polymorphism to apply to constraints.
Futhermore, it is not clear how to extend the constraint solver of
\cref{sec:constraint-solver} to support multiplicity-polymorphic
constraints.}${}^,$\footnote{Here and in the rest of the paper we adopt the convention that
equations defining a function by pattern matching are marked with a
$\left\{\right.$ to their left.}
%
$$
\begin{array}{c@@{\qquad\qquad}c}
\left\{
  \begin{array}{lcl}
    [[{pi + rho}]] & = & [[{omega}]]
  \end{array}
\right.
&
\left\{
  \begin{array}{lcl}
    [[{1 . pi}]] & = & [[{pi}]] \\
    [[{omega . pi}]] & = & [[{omega}]]
  \end{array}
\right.
\end{array}
$$

\subsection{Shortcomings of Linear Haskell that We Address}
The |read| function in \cref{sec:introduction} consumes the array it
operates on. Therefore, the same array can no longer be used in
further operations: doing so would result in a type error.  To resolve
this, a new name for the same array is produced by each operation.

From the perspective of the programmer, this is unwanted boilerplate.
Minimizing such boilerplate is the main aim of this paper.  Our
approach is to let the array be non-linear, and let its
capabilities (\emph{i.e.} having read or write access) be \emph{linear
constraints}. Once these capabilities are consumed, the array can no
longer be read from or written to without triggering a compile time
error.

A further drawback of today's Linear Haskell is that
the programmer cannot restrict how a linear function is used.
For example, suppose we want to use linear types to create
a pure interface to arrays that supports in-place mutation;
the interface is safe only if we guarantee that arrays cannot
be aliased. Because the result of a hypothetical |newArray|
function can be stored in an unrestricted variable (of multiplicity
$[[{omega}]]$), the linearity system cannot prevent its aliasing.
Instead, \citet[Fig.~2]{LinearHaskell} use a continuation-passing
style to enforce non-aliasing.

% In \cref{sec:introduction} and in the rest of the paper, we use pattern matches
% in $\klet$ bindings. By default in Haskell, patterns in $\klet$s are lazy, which means that
% |let  (a, b) = p in ()|
% will not actually evaluate the pair. To force evaluation, a strictness annotation can be added:
% |let  ^^ !(a, b) = p in ()|.
% Pattern matching in linear $\klet$ bindings must always be strict in Linear Haskell, so writing
% the lazy version would be rejected by the compiler. To simplify the
% presentation, we assume that all patterns in $\klet$ bindings are strict.
% Strict $\klet$ pattern bindings can be desugared into $\kcase$ expressions:
% |case p of (a, b) -> ()|.

\section{Working With Linear Constraints}
\label{sec:what-it-looks-like}
\jp{Why is this here? This current presentation is stuttering. IMO. It should go back somewhere in sec. 4.}
\begin{figure}%
  \maybesmall
  \centering
  \begin{subfigure}{.3\linewidth}%
    \noindent%
\begin{code}
phantom ( type constraint(RW n) = constraint((Read n, Write n)) )
new  :: Int -> (MArray a ->. Ur r) ->. Ur r
write :: MArray a ->. Int -> a -> MArray a
read :: MArray a ->. Int -> (MArray a, Ur a)
free :: MArray a ->. ()
\end{code}
\caption{Linear Types}
\label{fig:linear-interface}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.55\linewidth}
\begin{code}
type constraint(RW n) = constraint((Read n, Write n))
new  :: constraint (Unique) =>. Int -> exists n. UArray a n .<= constraint (RW n)
write :: constraint (RW n) =>. UArray a n -> Int -> a -> () .<= constraint (RW n)
read :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)
free :: constraint (RW n) =>. UArray a n -> ()
\end{code}
\caption{Linear Constraints}
\label{fig:constraints-interface}
  \end{subfigure}
\caption{Interfaces for mutable arrays}
\end{figure}

Consider the Haskell function |show|:
\begin{code}
show :: constraint (Show a) => a -> String
\end{code}
In addition to the function arrow |->|, common to all functional
programming languages, the type of this function features a constraint arrow |=>|.
Everything to the
left of a constraint arrow is called a \emph{constraint}, and will be
highlighted in \constraintfont{blue} throughout the paper. Here
|constraint (Show a)| is a
class constraint.

Constraints are handled implicitly by
the typechecker. That is, if we want to |show| the integer |n :: Int| we would write |show
n|, and the typechecker is responsible for proving that |constraint (Show Int)| holds, without
intervention from the programmer.

For our |read2AndDiscard| example, the |constraint((Read n, Write n))| (abbreviated as |constraint(RW n)|)
constraint represents read and write
access to the array tagged with the type variable |n|. (The full \textsc{api} under consideration appears
in \cref{fig:constraints-interface}.)
That is, the constraint |constraint(RW n)|
is provable if and only if the array tagged with |n| is readable and writable.
This constraint is linear: it must be consumed (that is, used as an assumption
in a function call) exactly once.
In order to manage linearity implicitly, this paper introduces a
linear constraint arrow (|=>.|), much like Linear Haskell introduces a linear
function arrow (|->.|). Constraints to the left of a linear constraint
arrow are \emph{linear constraints}.
Using the linear constraint |constraint(RW n)|, we can give
the following type to |free|:

\begin{code}
free :: constraint (RW n) =>. UArray a n -> ()
\end{code}

There are a few things to notice:
\begin{itemize}
\item We have introduced a new type variable |n|. In contrast,
  the version in \Cref{fig:linear-interface} without linear
  constraints  has type |free :: MArray a ->. ()|.
  The type variable |n| is a type-level tag used to identify the array.
  % Ideally, the linear constraint would refer directly to the
  % array value, and have the dependent type |free :: (n :: Array a) -> constraint (RW n) =>. ()|.
  % While giving a compile-time name to a function argument is common in
  % dependently typed languages such as \textsc{ats}~\cite{ats-lang} or Idris, our
  % approach, on the other hand, shows how we can still link a run-time value and a
  % compile-time tag without needing any dependent types.
\item The run-time variable representing the array can now be used multiple
  times. Instead of restricting the use of this variable,
  the linear constraint |constraint(RW n)| controls access to the
  array.
\item If we have a single, linear, |constraint (RW n)|
  available, then after |free| there will not be any |constraint (RW n)|
  left to use, thus preventing the array from being used after freeing.
  This is precisely what we were trying to achieve.
\end{itemize}

The above deals with freeing an array and ensuring that it cannot be used afterwards.
However, we still need to explain how a constraint |constraint (RW n)| can come
into scope.
The type of |new| with linear constraints is as follows:
\begin{code}
new  :: constraint (Unique) =>. Int -> exists n. UArray a n .<= constraint (RW n)
\end{code}

This type, too, illustrates several new aspects:
\begin{itemize}
\item The |constraint(Unique)| constraint is a linear constraint, though
it takes no type parameter. It restricts the result of |new|
to be used linearly, meaning that any variable that stores the result of |new| must
have multiplicity $[[{1}]]$. |constraint(Unique)| is explained more fully in \Fref{sec:Unique-constraint}.

\item Because |UArray| is now parameterised by a type-level tag |n|, |new| must
return a |UArray| with a fresh such |n|. This is achieved through returning an
existentially-quantified type\footnote{There is a variety of ways that existential types can
  be worked into a language. The existentials that we use here may be understood
  as a generalisation of those presented by \citet[Chapter
  24]{tapl}.
  However, \citet{existentials} work out an
  approach that makes linear constraints even easier to use, as we
  discuss in \Fref{sec:implicit-existentials}. In order to separate
  concerns, we do not build our formalism on \citet{existentials},
  instead modeling our existentials on the more widely known formulation described by \citet{tapl}.
  We additionally freely
omit the |exists a1 ... an.| or |.<= constraint (Q)| parts when they are
empty.
  The idea of using existentials to return linear capabilities can be attributed to \citet{DBLP:conf/esop/FluetMA06}.
} packing the type variable |n|. Such types are introduced with the $\packbox$ constructor.
% , which some readers might recognize as $\keyword{pack}$. % redundant with "packing". If there is a specific concept that this refers to, a typeface isn't enough to identify it (explicit reference is needed)

\item Not only do we need a fresh type variable |n|, but we also need to introduce
the linear constraint |constraint(RW n)| for use in subsequent calls to |read| and |free|.
Our existentials also allow packing a constraint, thanks to the |.<=| operator.
\end{itemize}

With all these features working together, we see that |new| returns a non-duplicable
|UArray| tagged with |n|, accessible only when the |constraint(RW n)| constraint is available.

We must also ensure that |read| can both promise to operate only on a readable array
and that the array remains readable afterwards. That is, |read| must both consume
a linear constraint |constraint(Read n)| and also produce a fresh linear constraint
|constraint(Read n)|, as we see in \cref{fig:constraints-interface}, and repeated
here:
\begin{code}
read :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)
\end{code}
%% The fact that existential quantification generates new type-level names
%% is folklore. It's used crucially in the interface of the
%% |ST| monad~\cite{st-monad} and in type-class
%% reflection~\cite{type-class-reflection} (in both of these cases, existential
%% quantification is encoded as rank-2 universal quantification).
%% We
%% shall use it in exactly this way: |openFile| uses an existential
%% quantifier to generate the type-level name of the file
%% handle. Existentially quantified types are paired with a constraint |Q|
%% which we understand as being returned by functions.
% This type has a new symbol, |.<=|, which allows us to pack a produced constraint
% with a returned value. We will see in \cref{sec:arrays} that these produced constraints
% will also sometimes need to come with fresh type variables. Combining these
% ideas, we
% introduce
% a type construction |exists a1 ... an. t .<= constraint(Q)|,
% where |constraint(Q)| is a linear constraint (with the |a1 ... an| in scope) that is
% paired with the type |t|.\footnote{} Such types are introduced with the $\packbox$ constructor.
% Today's Haskell does not have an existential quantifier. However,
% existentially quantified types can
% be encoded as datatypes. For instance, |exists h. Ur (Handle h) .<= constraint (Open h)| can
% be implemented as
% \begin{code}
% data PackHandle where
%   Pack :: forall h. constraint (Open h) =>. Handle h -> PackHandle
% \end{code}
% In our implementation (\Fref{sec:implementation}), packed linear constraints
% piggy-back on \textsc{ghc}'s standard \textsc{gadt} syntax. Correspondingly, existential types are
% introduced by a data constructor, which we write as $\packbox$.
% In order to simplify extracting out the payload of an existential, however, we
% introduce a new syntax |let x := f args|: this notation means that the result
% of |f args| is unpacked, putting its payload in |x|. The |:=| notation also
% denotes a \emph{strict} binding, necessary for binding the type variables and
% assuming the constraints carried with |f args|; it can be understood as
% syntactic sugar for a |case| expression.
We have now seen all the ingredients needed to write
the |read2AndDiscard| example as in \cref{sec:introduction}.

\subsection{Minimal Examples}
\label{sec:examples}

To get a sense of how the features that we introduce should behave, we now
look at some simple examples.\jp{This seems either way too late or redundant. How can anyone understand the previous section without already understanding this? I suggest either to 1. move this into the Linear Haskell section (using regular arguments). or 2. delete.} Using constraints to represent
limited resources allows the typechecker to reject certain classes
of ill-behaved programs. Accordingly, the following examples show the
different reasons a program might be rejected.

In what follows, we will be using a constraint |constraint (C)| that is consumed by the |useC|
function.
\begin{code}
useC :: constraint (C) =>. Int
\end{code}
The type of |useC| indicates that it consumes the linear resource |C| exactly once.

\subsubsection{Dithering}

We reject this program:
\begin{code}
dithering :: constraint (C) =>. Bool -> Int
dithering x = if x then useC else 10
\end{code}
The problem with |dithering| is that it does not unconditionally consume |constraint(C)|:
 the branch where |x == True| uses the resource |C|,
whereas the other branch does not.

\subsubsection{Neglecting}

Now consider the type of the linear version of |const|:
\begin{spec}
const :: a ->. b -> a
\end{spec}
This function uses its first argument linearly, and ignores the second. Thus,
the the second arrow is unrestricted.
%
One way to improperly use the linear |const| is by neglecting a linear variable:
\begin{code}
neglecting :: constraint (C) =>. Int
neglecting = const 10 useC
\end{code}
The problem with |neglecting| is that, although |useC| is mentioned in this program,
it is never consumed: |const| does not use its second argument.
The constraint |constraint(C)| is not consumed exactly once, and
thus this program is rejected.
%
The rule is that a linear constraint can only be consumed (linearly)
in a linear context. For example,
\begin{code}
notNeglecting :: constraint (C) =>. Int
notNeglecting = const useC 10
\end{code}
is accepted, because the |constraint (C)| constraint is passed on to |useC| which itself
appears as an argument to a linear function (whose result is itself consumed linearly).

\subsubsection{Overusing}
\label{sec:overusing}

Finally, the following program is rejected because it uses |C| twice:
\begin{code}
overusing :: constraint (C) =>. (Int, Int)
overusing = (useC, useC)
\end{code}
%
% However, the
% following version is accepted:
% \begin{code}
% notOverusing :: constraint ((C, C)) =>. (Int, Int)
% notOverusing = (useC, useC)
% \end{code}
% We see here that it is possible to have multiple copies of a given
% constraint.

% Because the consumption of constraints is implicit, when there are
% several possible ways to consume a constraint, the particular order
% chosen will depend on the details of the constraint resolution
% algorithm. We do not want the semantics of programs to depend on this
% order, which would not be at home in a declarative specification of
% the type system. Specifically, we require that the choice of
% given constraint (when there are multiple) cannot influence the
% behavior of the running program. In the domain of class constraints,
% this property is called \emph{coherence}: that only one instance of
% a specific type is in scope. Haskell already has mechanisms to ensure
% coherence~\cite{coherence}, though expert users have discovered ways
% to introduce incoherence\footnote{Kmett's \textsf{reflection} library,
% \url{https://hackage.haskell.org/package/reflection}, both uses incoherence
% to its advantage and also demonstrates techniques to keep its interface
% safe.}. As usual, any introduction of incoherence in the design of
% an \textsc{api} must be carefully considered for possible negative effects;
% the addition of linear constraints does not change this.

% \subsection{Linear I/O}
% \label{sec:linear-io}

% The file-handling example discussed in sections~\ref{sec:linear-types}
% and~\ref{sec:what-it-looks-like} uses a linear version of the |IO| monad, |IOL|.
% Compared to the traditional |IO| monad, the
% type of the monadic operations |>>=| (aka \emph{bind}) and |return| are changed to
% use linear arrows.
% %
% \begin{code}
% (>>=) :: IOL a ->. (a ->. IOL b) ->. IOL b
% return :: a ->. IOL a
% \end{code}
% %
% Bind must be linear because, as explained in the previous section, a linear
% constraint can be consumed in only a linear context. Consider
% the
% following program:
% \begin{code}
% readTwo ::  constraint (Open h) =>. Handle h -> IOL (Ur (String, String) .<= constraint (Open h))
% readTwo h =  readLine h >>= \case pack' xs ->
%              readLine h >>= \case pack' ys ->
%              return (pack' (xs, ys))
% \end{code}
% If bind were not linear, the first occurrence of |readLine h| would
% not be able to consume the |constraint (Open h)| constraint
% linearly.

\subsection{Restricting to a Linear Context with |constraint(Unique)|}
\label{sec:Unique-constraint}
A linear function makes a promise about how it is going to use its argument, but
linearity imposes no restrictions on how a function -- or its result -- is going to be used.
The caller may use the linear function's result unrestrictedly.  This poses a
challenge for providing a type-safe interface for libraries that rely on having a unique pointer to
some resource, such as safe mutable arrays, because the obvious definition of a
constructor function can immediately be misused, violating the assumption of uniqueness.
%
\begin{code}
new :: Int -> MArray a

bad = let arr = new 5 in (arr, arr)
badToo = Ur (new 5)
\end{code}
%
However, with linear constraints, we can overcome this problem by putting the
special |constraint(Unique)| constraint on |new|:
\begin{code}
new :: constraint(Unique) =>. Int -> MArray a
\end{code}
Suppose we have assumed the |constraint(Unique)| constraint linearly; that is,
we must use the |constraint(Unique)| assumption precisely once. Now, our definition
for |bad| is rejected: either we infer |arr| to have multiplicity~$[[{omega}]]$,
in which case its definition uses |constraint(Unique)| $[[{omega}]]$ times; or
we infer |arr| to have multiplicity~$[[{1}]]$, in which case its use (twice) violates
the linearity restriction. Likewise, the use of |Ur| in |badToo| requires using
the |constraint(Unique)| assumption $[[{omega}]]$ times.

This is promising so far, but several problems remain:

\begin{description}
\item[Duplicating |constraint(Unique)|] What if we want to create multiple
arrays, each of which having a unique pointer? If |constraint(Unique)| is assumed
linearly, then |let arr1 = new 5; arr2 = new 6| will fail, as it uses our
|constraint(Unique)| assumption twice. We thus stipulate that |constraint(Unique)|
must itself be duplicable: from one assumption of |constraint(Unique)|, we
must be able to satisfy any arbitrary fixed number of demands on that constraint.
By ``arbitrary fixed number'', we mean to say that we can duplicate |constraint(Unique)|
a finite number of times, but we may not use an assumption of |constraint(Unique)| with
multiplicity 1 to satisfy |constraint(Unique)| at multiplicity $[[{omega}]]$.

\item[Discarding |constraint(Unique)|] Similarly to allowing
  duplication, we must allow discarding, in case a function allocates
  no arrays at all. Accordingly, we allow a linear assumption of
  |constraint(Unique)| to be accepted even if the constraint is never
  used.

\item[Initial assumption of |constraint(Unique)|] For this approach to work,
we must have an assumption of |constraint(Unique)| of multiplicity 1. We can
achieve this via the following primitive:
\begin{code}
unique :: (Unique =>. Ur r) ->. Ur r
\end{code}
The argument to |unique| will be a continuation that assumes |constraint(Unique)|
with multiplicity 1. Because |unique| returns an unrestricted value, no restricted
values from the continuation can escape the scope of the |constraint(Unique)|
assumption. Thus, the continuation has exactly the condition we need: a
linear assumption of |constraint(Unique)|.
\end{description}

The pattern of using a continuation in |unique| mirrors the use
of that technique by \citet[Fig.~2]{LinearHaskell}. But
|unique| is, now, the only place where we need a continuation: once
we have our linear |constraint(Unique)| assumption, we can use it to produce
new values that must be unique.

With just these simple ingredients -- a duplicable, discardable constraint
that can be assumed linearly -- we can write \textsc{api}s that require
uniqueness without heavy use of continuations.

\section{Application: memory ownership}
\label{sec:memory-ownership}
Let us now turn back to the more substantial example introduced in
\cref{sec:introduction}: manual memory management.  In functional programming
languages like Haskell, memory deallocation is normally the responsibility of a
garbage collector. However, garbage collection is
not always desirable, either due to its (unpredictable) runtime costs, or because
pointers exist between separately-managed memory spaces
(for example when calling foreign functions~\cite{linear-inline-java}).
In either case, one must then
resort to explicit memory allocation and deallocation. This task is
error prone: one can easily forget a deallocation (causing a memory
leak) or deallocate several times (corrupting data). In this section we show how
to build a % Rust-style % tone down Rust fetishism. It's already mentioned just below.
memory management \textsc{api} as a \emph{library} using linear
constraints. The library is a generalisation of the array library
introduced in \cref{sec:introduction}.

\subsection{Capability Constraints}
\label{sec:atomic-references}

Our approach, inspired by Rust, is
to represent \emph{ownership} of a memory location, and more specifically,
whether the reference is mutable or read-only.
We use the linear constraints |constraint(Read n)| and |constraint(Write n)|,
guarding read access and write access to a reference respectively.
Because of linearity, these constraints
must be consumed, so the \textsc{api} can guarantee that memory
is deallocated correctly.
%
In |constraint(Read n)|, |n| is a type variable (of a special kind |Location|)
which represents a memory location. Locations mediate
the relationship between references and ownership constraints.
%

\begin{minipage}{0.5\linewidth}
> class constraint (Read (n :: Location))
\end{minipage}
\begin{minipage}{0.5\linewidth}
> class constraint (Write (n :: Location))
\end{minipage}
% \begin{minipage}{0.3\linewidth}
% > class constraint (Own (n :: Location))
% \end{minipage}
\\
To ensure referential transparency,
writes can be done only when we are sure that no other part of the program has
read access to the reference.
% Rust disallows mutable aliasing for the same reason: ensuring that writes cannot be observed through other references is what allows treating mutable structures as ``pure''.
% jp: commented this tangent because it obscures the logical connection. It could perhaps go in a parenthesis
Therefore, writing also requires the read capability. Thus we
systematically use |constraint(RW n)|, pairing both the read and write
capabilities.

With these components in place, we can provide an \textsc{api} for mutable
references.

> data AtomRef (a :: Type) (n :: Location)

The type |AtomRef| is the type of references to values of a type |a| at
location |n|. Allocation of a reference can be done using the
following function. % As with |new|, the return value must be unrestricted.

> newRef :: constraint (Unique) =>. exists n. AtomRef a n .<= constraint (RW n)

The function |newRef| creates a new atomic reference, initialised with
|undefined|; we could also pass in an initial value, but doing so in the more
general case below would add complication and obscure our main goal of demonstrating
linear constraints.

To read a reference, a |constraint(Read)| constraint is demanded, and
then returned back. Writing is similar.

> readRef :: constraint (Read n) =>. AtomRef a n -> Ur a .<= constraint (Read n)
> writeRef :: constraint (RW n) =>. AtomRef a n -> a -> () .<= constraint (RW n)

Note that the above primitives do not need to explicitly declare
effects in terms of a monad or another higher-order effect-tracking
device: because the |constraint(RW n)| constraint is linear, passing it suffices
to ensure proper sequencing of effects concerning location |n|.

Also note that |readRef| returns an unrestricted \emph{copy} of the element, and
|writeRef| \emph{copies} an unrestricted element into the location. This means
that while |AtomRef|s are mutable, their contents are always immutable structures.

% This is ensured by the combination of the language and library
% behaviour. For example, here is how to write two values (|a| and |b|) to the same reference |x|:

% > case writeRef x a of
% >   pack _ -> case writeRef x b of
% >     pack _ -> ...

% The language semantics forces the programmer to do case analysis to
% access the returned |Write| constraints, and |writeRef| must be strict
% in the |Write| constraint that it consumes.

Since there is a unique |constraint(RW n)| constraint per reference, we
can also use it to represent ownership of the reference: access to |constraint(RW
n)| represents responsibility (and obligation) to deallocate |n|:

> freeRef :: constraint (RW n) =>. AtomRef a n -> ()

% Instead of deallocating the reference, one could transfer ownership
% of the memory location to the garbage collector. This operation is
% sometimes called ``freezing'':

% > freezeRef :: constraint (RW n) =>. AtomRef a n -> Ur a

\subsection{Arrays}
\label{sec:arrays}

The above toolkit handles references to base types just fine.  But
what about storing references in objects managed by the ownership
system? In \cref{sec:introduction}, we presented an interface for mutable
arrays whose contents are themselves immutable. Our approach
scales beyond that use case, supporting arrays of
references, including arrays of (mutable) arrays.

% JP: the discussion below sounds quite petty. Not many (zero
% non-conflicting?)  people will be aware of the issue with linear
% arrays. So it would need a lot more space to be properly
% justified. Besides, the issue of storing references in structures
% should be motivation enough in general.

% A motivating example of plain Linear Haskell is a pure interface to
% mutable arrays~\cite[Section 2.2]{LinearHaskell}. There we have two
% types: first |MArray a|, used linearly, for mutable arrays; and
% second |Array a|, used unrestricted, for immutable arrays. Mutable
% arrays can be frozen using |freeze :: MArray a ->. Ur (Array
% a)|. Note that this version of |freeze| can not support mutable
% arrays of mutable arrays.  The core of the issue is that mutable
% arrays and immutable arrays have different types.
% The ownership \textsc{api}, on the other hand, is readily extended to
% nested mutable arrays.
% Crucially, and in contrast to the plain linear types \textsc{api} the
% type |PArray| is both the type of mutable arrays and immutable arrays,
% |freezePArray| only changes the permissions. And since permissions are
% linear constraints, this is all managed automatically by the compiler.

> data PArray (a :: Location -> Type) (n :: Location)
> newPArray :: constraint (Unique) =>. Int -> exists n. PArray a n .<= constraint (RW n)

For this purpose we introduce the type |PArray a n|, where the kind of
|a| is |Location -> Type|: this way we can easily enforce that each
reference in the array refers to the same location |n|. Both types
|AtomRef a| and |PArray a| have kind |Location -> Type|, and therefore
one can allocate, and manipulate arrays of arrays with this
\textsc{api}. For example, an array of integers has type
|PArray (AtomRef Int) n|, and indeed, the |UArray| type from
\cref{sec:introduction} is a synonym for an array of atomic references.
An array of arrays of integers would has type |PArray (PArray (AtomRef Int)) n|. Thus,
the framework handles nested mutable structures without any additional
difficulty. %% As discussed in \cref{sec:what-it-looks-like}, the scope of |newPArray| returns
%% an unrestricted value to ensure that the linear constraint is consumed within
%% the scope (since linear values cannot be embedded into an unrestricted value).
%% As this is the only introduction form of |constraint (RW n)|, it can safely be
%% assumed to be unique within the scope. An alternative design would be to
%% require that the scope returns the linear constraint: |forall n. constraint (RW
%% n) =>. PArray a n -> b .<= constraint (RW n)|. This version is less flexible,
%% because it doesn't allow the scope to deallocate or freeze the array.

The actual runtime value of a |PArray| is a pointer to a contiguous block of
memory together with the size of the memory block. This means that the length of the
array can be accessed without having ownership of the array: |length :: PArray a n -> Int|.
While the |PArray| reference itself is managed by the garbage collector, the
pointer it contains points to manually managed memory.

% JP: I propose to cut down the following, since it appears unused in the rest of the paper.
% (The "quicksort" example is a lot more convincing as an example)

% For the special case where the array stores atomic references (|UArray|) which we know how
% to deallocate, the |write| function can be implemented using |replace|:


% > write :: constraint (RW n) =>. UArray a n -> Int -> a -> () .<= constraint (RW n)
% > write arr i x =
% >   let   pack' ref = newRefBeside arr
% >         pack () = writeRef ref x
% >         pack () = replace arr i ref
% >   in freeRef ref

% Because having access to a |constraint(RW n)| is proof that we are inside a scope
% with a unique resource, we can also allocate a reference to the same scope as |n|:

% > newRefBeside :: constraint (RW n) =>. a n -> exists m. Ur (AtomRef m) .<= constraint ((RW n, RW m))

% \csongor{I think this is now the first time we talk about existentials? Need to be careful not to forget introducting them}

\subsubsection{Borrowing}

The |lendMut arr i k| primitive lends access to the reference at index |i| in
|arr|, to a continuation function |k| (in Rust terminology, the function
\emph{borrows} an element of the array). Note that the continuation must return the
read-write capability, so that the ownership transfer is indeed temporary. The
type system guarantees that the borrowed reference cannot be shared or deallocated.
%
% Here, the return type of the scope, |r|,
% is not in |Ur|: since the scope must return the |constraint(RW p)| constraint, it is
% not possible to leak it out by packing it into |r|, so it's not necessary to
% wrap the result in |Ur|.
Indeed, with this \textsc{api}, |constraint(RW n)| and |constraint(RW p)| are
never simultaneously available.

> lendMut  :: constraint (RW n) =>. PArray a n -> Int -> (forall p. ^^ constraint (RW p) =>. a p -> r .<= constraint (RW p)) ⊸ r .<= constraint (RW n)
%
Because the elements of an array can be mutable structures (such as
other arrays), reading can be done safely only if we can ensure that
no one else has access to the array while the element is accessed. Otherwise,
the array -- including the element being read -- could be mutated.
Therefore,
gaining simple read access to an element needs to be done using a
scoped \textsc{api} as well:

> lend  ::  constraint (Read n) =>. PArray a n -> Int ->  (forall p. ^^ constraint (Read p) =>. a p -> r .<= constraint (Read p)) ⊸ r .<= constraint (Read n)

For the special case of |UArray|s, a more traditional reading
operation can be implemented, by lending the reference to |readRef|
which creates an unrestricted \emph{copy} of the value. This copy is
under control of the garbage collector, and can escape the scope of
the borrowing freely.

> read :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)
> read arr i = lend arr i readRef


% \subsubsection{Freezing}

% Finally, we can freeze arrays, using the
% following primitive:

% > freezePArray :: constraint (RW n) =>. PArray a n -> () .<= constraint (omega ⋅ (Read n))

% Where |constraint(omega ⋅ (Read n))| means that the returned constraint is not linear. That is, after |freezePArray n|, we have unrestricted read access to |n| (and
% any element of |n|), as expected. We describe this syntax in more
% details in the next
% section, where, similarly, we shall treat |constraint(Read n) =>| as
% an abbreviation for |constraint(omega ⋅ (Read n)) =>.|.

% With an unrestricted |constraint(Read n)| capability, we can read from the array
% more directly with the following primitive

% > readP :: constraint (Read n) => PArray a n -> Int -> a n

\subsubsection{Slices}

It is also possible to give a safe interface to array
\emph{slices}. A slice represents a part of an array and allows
splitting the ownership of the array into multiple parts,
shared between different consumers.  The ownership system
means that slicing does not require copying.

Splitting consumes all capabilities of an array and returns two new
arrays that represent the contiguous blocks of memory before and
starting at a given index.

> split :: constraint (RW n) =>. PArray a n -> Int -> exists l r. Ur (PArray a l, PArray a r) .<= constraint ((RW l, RW r, Slices n l r))

In addition to the array capabilities, the output constraints also include
|constraint (Slices n l r)|, witnessing the fact that locations
|l| and |r| are components of |n|, so that they can be joined back
together:

> join :: constraint ((Slices n l r, RW r, RW l)) =>. PArray a l -> PArray a r -> Ur (PArray a n) .<= constraint (RW n)

%format a_i_val = a_i "\_val"
%format a_j_val = a_j "\_val"
\begin{figure}
\maybesmall
\begin{code}
swap :: constraint(RW n) =>. PArray AtomRef n -> Int -> Int -> () .<= constraint(RW n)
swap arr i j  | i == j   =  pack ()
              | i > j    =  swap arr j i
              | i < j    =  let   pack (Ur (l, r))  =  split arr (i + 1)
                                  pack ()           =  lendMut l i (\a_i ->
                                                         let pack () = lendMut r (j - (i + 1)) (\a_j ->
                                                           let  pack (Ur a_i_val)  =  readRef a_i
                                                                pack (Ur a_j_val)  =  readRef a_j
                                                                pack ()            =  writeRef a_j a_i_val
                                                                pack ()            =  writeRef a_i a_j_val
                                                           in pack ()) in pack ())
                                  pack (Ur _)       =  join l r
                            in pack ()
\end{code}
\caption{Swapping two elements of an array}
\label{fig:swap}
\end{figure}


With these building blocks, we can now implement various utility functions on
arrays, such as swapping two elements of an array, which is shown in
\Cref{fig:swap}.  It is
not so simple to implement\footnote{Indeed, Rust's implementation uses
an \emph{unsafe} block.}, because we need two elements of an array
simultaneously, but only one element can be borrowed at a time. To
solve this problem, we split the array into two slices such that the
two indices fall in two different slices. Then simply borrow the
element |i| from the first slice, and |j| from the second slice (using
|lendMut|). Finally, we join the two slices back together.

\subsubsection{In-Place Quicksort}

As an example of using the machinery defined above, we implement an in-place, pure
quicksort algorithm, given in \Cref{fig:quicksort}.
%
\begin{figure}
\maybesmall
\noindent
\begin{minipage}[t]{0.45\linewidth}
\begin{code}
sort :: constraint (RW n) =>. UArray Int n -> () .<= constraint (RW n)
sort arr = let len = length arr in
  if len <= 1 then pack ()
  else   let  pack pivotIdx       =  partition arr
              pack (Ur (l, r))    =  split arr pivotIdx
              pack ()             =  sort l
              pack ()             =  sort r
              pack (Ur _)         =  join l r
         in pack ()
\end{code}
\end{minipage}
\begin{minipage}[t]{0.4\linewidth}
\begin{code}
partition :: constraint (RW n) =>. UArray Int n -> Int .<= constraint (RW n)
partition arr =
  let  last             =  length arr - 1
       pack (Ur pivot)  =  read arr last
       go :: constraint (RW n) =>. Int -> Int -> Int .<= constraint (RW n)
       go l r
         |  l > r
         =  let pack () = swap arr last l in pack l
         |  otherwise
         =  let pack (Ur lVal) = read arr l in
            if lVal > pivot
            then  let pack () = swap arr l r
                  in go l (r - 1)
            else  go (l + 1) r
  in go 0 (last - 1)
\end{code}
\end{minipage}
\caption{In-place quicksort}
\label{fig:quicksort}
\end{figure}
%
The |partition| function is responsible for picking a pivot element and
reorganising the array elements such that each element preceding the pivot will
be less than or equal to it, and the elements after will be greater than the
pivot. Once finished, it returns the index of the pivot element; |sort| then
splits the array at the pivot element and recursively operates on the two
slices.

\section{A qualified type system for linear constraints}
\label{sec:qualified-type-system}

We
now present our design for a qualified type system~\cite{QualifiedTypes} that supports
linear constraints. Our design, based on the work of \citet{OutsideIn}, is compatible with Haskell and \textsc{ghc}.


\subsection{Simple Constraints and Entailment}
\label{sec:constraint-domain}

We call constraints such as
|constraint(Read n)| or |constraint(Write n)| \emph{atomic
  constraints}. The set of atomic constraints is a parameter of our
qualified type system.

\begin{definition}[Atomic constraints]
  The qualified type system is parameterised by a set, whose elements
  are called \emph{atomic constraints}. We use the variable $[[{q}]]$
  to denote atomic constraints.
\end{definition}

Atomic constraints are assembled into \emph{simple constraints}
$[[{Q}]]$, which play the hybrid role of constraint contexts and
(linear) logic formulae.
The following operations work with simple constraints:
\begin{description}
\item[Scaled atomic constraints] $[[{pi.q}]]$ is a simple constraint,
  where $[[{pi}]]$ specifies whether $[[{q}]]$ is to be used linearly
  or not.
\item[Conjunction] Two simple constraints can be paired up
  $[[{Q1 * Q2}]]$. Semantically, this corresponds to the multiplicative
  conjunction of linear logic. Tensor products represent pairs of
  constraints such as |constraint((Read n, Write n))| from Haskell.
\item[Empty conjunction] Finally we need a neutral element
  $[[{Empty}]]$ to the tensor product. The empty conjunction is used
  to represent functions which don't require any constraints.
\end{description}
%
However, we do not define $[[{Q}]]$ inductively, because we
require certain equalities to hold:

\smallskip
\begin{minipage}[c]{0.5\linewidth}
$$
\begin{array}{rcl}
  [[{Q1 * Q2}]] & = & [[{Q2 * Q1}]] \\
  [[{(Q1*Q2)*Q3}]] & = & [[{Q1*(Q2*Q3)}]]
\end{array}
$$
\end{minipage}
\begin{minipage}[c]{0.4\linewidth}
$$
\begin{array}{rcl}
  [[{omega.q * omega.q}]] & = & [[{omega.q}]] \\
  [[{Q * Empty}]] & = & [[{Q}]]
\end{array}
$$
\end{minipage}
\smallskip

We thus say that a simple constraint is a pair combining a set of unrestricted
constraints $[[{UCtx}]]$ and a multiset of linear constraints $[[{LCtx}]]$.
The linear constraints must
be stored in a multiset, because assuming the same constraint twice is distinct
from assuming it only once.

\begin{definition}[Simple constraints]
$$
\begin{array}{rcll}
  [[{UCtx}]] & \bnfeq & \ldots & \text{set of atomic constraints $[[{q}]]$} \\
  [[{LCtx}]] & \bnfeq & \ldots & \text{multiset of atomic constraints $[[{q}]]$} \\
  [[{Q}]] & \bnfeq & [[{(UCtx,LCtx)}]] & \text{simple constraints}
\end{array}
$$
We can now straightforwardly define the operations we need on simple
constraints:
$$
\begin{array}{ccc}
  \begin{array}{r@@{\;}c@@{\;}l}
    [[{Empty}]] &=& [[{(emptyset, emptyset)}]]
  \end{array}
  &
    \left\{
    \begin{array}{r@@{\;}c@@{\;}l}
      [[{1.q}]] &=& [[{(emptyset, q)}]] \\
      [[{omega.q}]] &=& [[{(q, emptyset)}]]
    \end{array}
                        \right.
  &
    \begin{array}{r@@{\;}c@@{\;}l}
      [[{(UCtx1,LCtx1)*(UCtx2,LCtx2)}]] &=& [[{(UCtx1 \u UCtx2, LCtx1 \u LCtx2)}]]
    \end{array}
\end{array}
$$
\end{definition}

In practice, we do not need to concern ourselves with the
concrete representation of $[[{Q}]]$ as a pair of sets, instead
using the operations defined just above.

The semantics of simple constraints (and, indeed, of atomic
constraints) is given by an \emph{entailment relation}. Just like the
set of atomic constraints, the entailment relation is a parameter of
our system.

\begin{figure}
  \maybesmall
  \begin{enumerate}
  \item $[[Q ||- Q]]$.
  \item If $[[Q1 ||- Q2]]$  and $[[Q * Q2 ||- Q3]]$, then $[[Q * Q1 ||- Q3]]$.
  \item If $[[Q ||- Q1 * Q2]]$, then there exist $[[{Q'}]]$, $[[{Qdup}]]$, and $[[{Q''}]]$
        such that:
        $$
         [[Qdup \in Dup]], \quad [[{Q}]]=[[{Q' * Qdup * Q''}]], \quad [[Q' * Qdup ||- Q1]]\text{, and } \quad
        [[Qdup * Q'' ||- Q2]].
        $$
  \item If $[[Q ||- Empty]]$, then $[[Q \in Dup]]$.
  \item If $[[Q1 ||- Q1']]$ and $[[Q2 ||- Q2']]$, then $[[Q1 * Q2 ||- Q1' * Q2']]$.
  \item If $[[Q ||- rho. q]]$, then $[[pi . Q ||- (pi.rho). q]]$.
  \item If $[[Q ||- (pi.rho) . q]]$, then there exists $[[{Q'}]]$ such that $[[{Q}]] = [[{pi. Q'}]]$ and $[[Q' ||- rho . q]]$.
  \item If $[[Q1 ||- Q2]]$, then $[[omega.Q1 ||- Q2]]$.
  \item If $[[Q1 ||- Q2]]$, then for all $[[{Q'}]]$, it is the case that $[[omega.Q' * Q1 ||- Q2]]$.
  \item If $[[q \in Dup]]$, then $[[1.q ||- 1.q * 1.q]]$.
  \item If $[[q \in Dup]]$, then $[[1.q ||- Empty]]$.
  \item If $[[Q \in Dup]]$ and $[[Q' ||- Q]]$, then $[[Q' \in Dup]]$.
  \end{enumerate}
\caption{Requirements for the entailment relation $[[Q1 ||- Q2]]$}
\label{fig:entailment-relation}
\end{figure}

\begin{definition}[Entailment relation]
  \label{def:entailment-relation}
  The qualified type system is parameterised by a relation
  $[[Q1 ||- Q2]]$ between two simple constraints, as well as by a
  distinguished set $[[{Dup}]]$ of duplicable atomic constraints.

  We write, abusing notation, $[[Q \in Dup]]$ for a simple constraint
  $[[{Q}]]=[[{(UCtx, LCtx)}]]$ if for all $[[{q}]]\in[[{LCtx}]]$ we
  have $[[q \in Dup]]$.

  The entailment relation must obey the laws listed in
  \Fref{fig:entailment-relation}.
\end{definition}
%
\info{See Fig 3, p14 of OutsideIn\cite{OutsideIn}.}
%
The set $[[{Dup}]]$ is a set of constraints which can be duplicated and
discarded (see~\cref{fig:entailment-relation}). We use $[[{Dup}]]$ to
model the |constraint(Unique)| constraint. Crucially, it is \emph{not
  the case} that $[[1.q ||- omega.q]]$ for $[[q\in Dup]]$; such an
entailment is, in fact, prohibited (\cref{lem:q:scaling-inversion}) by
the rules of~\cref{fig:entailment-relation}. While it may seem
counter-intuitive, there is nothing in linear logic mandating that a
formula that can be duplicated and discarded be (equivalent to) an
unrestricted formula. This observation has been exploited, for
instance, to introduce so-called
subexponentials~\cite{subexponentials}. For our use case, it lets the
typechecker dispatch |constraint(Unique)| constraints (using
duplication), but prevents the result of constrained functions to be used
unrestrictedly.

An important feature of simple constraints is that, while scaling
syntactically happens at the level of atomic constraints, these properties
of scaling extend to scaling of arbitrary constraints. Define
$[[{pi.Q}]]$ as:

$$
                      \left\{
                      \begin{array}{r@@{\;}c@@{\;}l}
                        [[{1.(UCtx,LCtx)}]] &=& [[{(UCtx,LCtx)}]] \\
                        [[{omega.(UCtx,LCtx)}]] &=& [[{(UCtx \u LCtx, emptyset)}]]
                      \end{array}
                                                    \right.
$$
%
Then the following properties hold
%
\begin{lemma}[Scaling]
  \label{lem:q:scaling}
  If $[[Q1 ||- Q2]]$, then $[[pi.Q1 ||- pi.Q2]]$.
\end{lemma}

\begin{lemma}[Inversion of scaling]
  \label{lem:q:scaling-inversion}
  If $[[Q1 ||- pi.Q2]]$, then $[[{Q1}]]=[[{pi.Q'}]]$ and $[[Q' ||- Q2]]$ for some $[[{Q'}]]$.
\end{lemma}

\begin{corollary}[Linear assumptions]
If $[[Q1 ||- omega.Q2]]$, then $[[{Q1}]]$ contains no linear assumptions.
\end{corollary}

Proofs of these lemmas (and others) appear in Appendix~\ref{sec:appendix:proofs-lemmas};
they can be proved by straightforward use of the properties in \Cref{fig:entailment-relation}.

\subsection{Typing Rules}
\label{sec:typing-rules}

With this material in place, we can now present our type system. The
grammar is given in \Cref{fig:declarative:grammar}, which also
includes the definitions of scaling on contexts $[[{pi.G}]]$ and addition
of contexts $[[{G1+G2}]]$. Note that addition on contexts is actually
a partial function, as it requires that, if a variable $[[{x}]]$ is bound
in both $[[{G1}]]$ and $[[{G2}]]$, then $[[{x}]]$ is assigned the same
type in both (but perhaps different multiplicities). This partiality
is not a problem in practice, as the required condition for combining
contexts is always satisfied.

\begin{figure}
  \maybesmall
  $$
  \begin{array}{@@{}c@@{}}
  [[{a}]], [[{b}]] \quad \text{Type vars} \qquad
  [[{x}]], [[{y}]] \quad \text{Expression vars} \qquad
  [[T]] \quad \text{Type constructors} \qquad
  [[{K}]] \quad \text{Data constructors} \\
  \begin{array}[b]{lcll}
    % [[{a}]], [[{b}]] & \bnfeq & \ldots & \text{Type variables} \\
    % [[{x}]], [[{y}]] & \bnfeq & \ldots & \text{Expression variables} \\
    % [[T]] & \bnfeq & \ldots & \text{Type constructors} \\
    % [[{K}]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[{s}]] & \bnfeq & [[{forall as. Q =o t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & [[{a}]] \bnfor [[{exists as. t o= Q}]] \bnfor [[{t1 ->_pi t2}]]
                            \bnfor [[{T ts}]] & \text{Types} \\
    [[{G}]], [[{D}]] & \bnfeq & [[{empty}]] \bnfor [[{G, x:_pi s}]] &
                                                              \text{Contexts} \\
    [[{e}]] & \bnfeq & [[{x}]] \bnfor [[{K}]] \bnfor [[{\x. e}]] \bnfor [[e1
                     e2]] \bnfor [[{pack e}]] & \text{Expressions}\\
                 &\bnfor & [[unpack x=e1 in
                     e2]] \bnfor [[{case_pi e of { alts }}]] &\\
                 &\bnfor & [[let_pi
                     x=e1 in e2]] \bnfor [[{let_pi x : s = e1 in e2}]] &
  \end{array} \\[1ex]
  \end{array}
  $$
  Context scaling $[[{pi.G}]]$ and addition of contexts $[[{G1+G2}]]$ is defined as follows:
  $$
  \begin{array}{cc}
  \left\{
  \begin{array}{r@@{\;}c@@{\;}l}
  [[{pi.empty}]] &=& [[{empty}]] \\
  [[{pi.(G,x:_rho s)}]] &=& [[{pi.G, x:_( pi.rho ) s}]]
  \end{array}
  \right.
  &
  \left\{
  \begin{array}{r@@{\;}c@@{\;}lll}
  [[{(G1, x:_pi s) + G2}]] &=& [[{ G1 + G2', x:_( pi+rho ) s }]] & \text{where} & [[G2 = {x:_rho s} \u G2']] \\
  &&&& [[x \notin G2']] \\
  [[{(G1, x:_pi s) + G2}]] &=& [[{ G1 + G2, x:_pi s }]] & \text{where} & [[x \notin G2]] \\
  [[{empty + G2}]] &=& [[{G2}]]
  \end{array}
  \right.
  \end{array}
  $$
  \caption{Grammar of the qualified type system}
  \label{fig:declarative:grammar}
\end{figure}

\begin{figure}
  \centering
  \drules[E]{$[[Q;G |- e : t]]$}{Expression
    typing}{Var,Abs,App,Pack,Unpack,Let,LetSig,Case,Sub}
  \caption{Qualified type system}
  \label{fig:typing-rules}
\end{figure}

The typing rules are in \Cref{fig:typing-rules}.
A qualified type system~\cite{QualifiedTypes} such as ours introduces a
judgement of the form $[[Q;G |- e : t]]$, where $[[{G}]]$ is a standard
type context, and $[[{Q}]]$ is a constraint we have assumed to be true.
$[[{Q}]]$ behaves
much like $[[{G}]]$, which will be instrumental for
desugaring in \cref{sec:desugaring}; the main difference is
that $[[{G}]]$ is addressed explicitly, whereas $[[{Q}]]$
is used implicitly in \rref{E-Var}.

Because constraints are used implicitly, if there are several instances
of the same $[[{1.q}]]$, it is non-deterministic which one is used in
which instance of \rref*{E-Var}. As a consequence, we must require
that any two instances of $[[{1.q}]]$ in a constraint $[[{Q}]]$ have
the same computational content (see \cref{sec:desugaring}). How do we
reconcile this non-determinism with the use of linear constraints, in
\cref{sec:arrays} to thread mutations? We certainly don't want type
inference to non-deterministically reorder a |readRef| and a
|writeRef|! The solution is that the \textsc{api} is arranged so that
only a single instance of |constraint(RW n)| is ever
provided. Therefore there is a single possible threading of the reads
and writes. In contrast there will often be several instances of
|Unique| in scope.

The type system of \Cref{fig:typing-rules} is purely
declarative: note, for example, that \rref{E-App} does not describe
how to break the typing assumptions into constraints
$[[{Q1}]]$/$[[{Q2}]]$ and contexts $[[{G1}]]$/$[[{G2}]]$. We will see
how to infer constraints in \cref{sec:type-inference}. Yet,
this system is our ground truth: a system with a simple enough
definition that programmers can reason about typing. As is standard in
the qualified-type literature (since the original paper~\cite{QualifiedTypes}), we do not
directly give a dynamic
semantics to this language; instead, we will give it meaning via
desugaring to a simpler core language in \cref{sec:desugaring}.

We survey several distinctive features of our qualified type system below:

\info{See Fig 10, p25 of OutsideIn\cite{OutsideIn}.}
\paragraph{Linear functions.}
The type of linear functions is written $[[{a ->_1 b}]]$.
  Despite our focus on linear constraints,
  we still need linearity in ordinary arguments.
%
  Indeed, the linearity of arrows interacts in interesting
  ways with linear constraints: If $[[f : a ->_omega b]]$ and
  $[[x : 1.q =o a]]$, then calling $[[{f x}]]$ would actually use $[[{q}]]$
  many times. We must make sure it is impossible to derive
  $[[1.q ; f :_omega a ->_omega b, x :_omega 1.q =o a |- f x : b]]$.
  Otherwise we could make, for instance, the |overusing| function from
  \cref{sec:overusing}.
  You can check that $[[1.q ; f :_omega a ->_omega b, x :_omega 1.q =o a |- f x : b]]$
  indeed does not
  type check, because the scaling of $[[{Q2}]]$ in \rref{E-App} ensures that
  the constraint would be $[[{omega.q}]]$ instead. On the other hand,
  it is perfectly fine to have $[[1.q ; f :_omega a ->_1 b, x :_omega 1.q
  =o a |- f x : b]]$ when $[[{f}]]$ is a linear function.

\paragraph{Variables.}
As is standard, the \rref{E-Var} rule works in a context containing more
than just the used binding for $[[{x}]]$. However, crucially,
our rule allows only
\emph{unrestricted} variables to be discarded; linear variables \emph{must} be
used. We can see this in the rule by noticing that the context has an unrestricted
component $[[{omega.G_2}]]$. The $[[{G1}]]$ component might be restricted or might not,
allowing this rule to apply both for restricted and unrestricted $[[{x}]]$.

\paragraph{Data constructors.}
Data constructors $[[{K}]]$ don't have a dedicated typing
rule. Instead they are typed using the \rref{E-Var}, where they are
treated as if they were unrestricted variables.

\paragraph{Let-bindings.}
Bindings in a |let| may be for either linear or unrestricted variables.
  We could require all bindings to be linear and to implement unrestricted
  information only using |Ur|, but it is very easy to add a multiplicity
  annotation on |let|, and so we do.

\paragraph{Local assumptions.}
\Rref{E-Let} includes support for local
  assumptions. We thus have the ability to generalise a subset of
  the constraints needed by $[[{e1}]]$ (but not the type variables---no
  |let|-generalisation here, though it could be added). The inference algorithm of
  \cref{sec:type-inference} will not make use of this
  possibility. % but we revisit this capability in \cref{sec:let-generalisation}.
%% \item Data types are not \textsc{gadt}s.
%%   This serves to considerably simplify the \rref*{E-Case}
%%   rule. It would be
%%   straightforward, yet tedious, to extend data types here to full
%%   \textsc{gadt}s.

\paragraph{Existentials.}
 We include $[[{exists as. t o= Q}]]$, as
  introduced in \cref{sec:what-it-looks-like}, together with
  the $\packbox$ constructor. See rules~\rref*{E-Pack} and
  \rref*{E-Unpack}.
\info{No substitution on $[[{Q1}]]$ in the E-Unpack rule, because there is
  only existential quantification.}

\section{Constraint inference}
\label{sec:type-inference}

The type system of \Cref{fig:typing-rules} gives a declarative description
of what programs are acceptable. We now present the algorithmic counterpart to
this system. Our algorithm is structured, unsurprisingly, around generating and
solving constraints, broadly following the template of
\citet{essence-of-ml-type-inference}.
That is, our algorithm takes a pass over the abstract syntax entered by the
user, generating constraints as it goes. Then, separately, we solve those
constraints (that is, try to satisfy them) in the presence of a set of assumptions,
or we determine that the assumptions do not imply that the constraints hold. In the
latter case, we issue an error to the programmer.

The procedure is responsible for inferring both \emph{types} and \emph{constraints}.
For our type system, type inference can be done independently from constraint
inference. Indeed, we focus on the latter, and defer type inference to
an external oracle (such as~\cite{linear-types-inference}).
That is, we assume an algorithm that produces typing derivations for the
judgement $[[G |- e : t]]$, ignoring all the constraints. Then, we describe a
constraint generation algorithm that passes over these typing derivations.
%
% A typical generate-and-solve algorithm is built around unification variables.
% A unification variable stands for some unknown type. When we see, for example,
% |\x -> ...|, we let the type of |x| be a unification variable, and then
% constraints arising from the body of the expression (or its context) might tell
% us that the unknown type is |Int|. Our system does not do this, however:
% using constraints to solve for unification variables is well understood~\cite{OutsideIn}
% and orthogonal to the concerns that drive the innovations in this paper.
% Instead, the description of the aspects of our algorithm regarding inferring types---as opposed to
% inferring \emph{constraints}---nondeterministically just guess the correct
% type. A real implementation would use unification variables and constraints,
% but they just add clutter here and distract us from the novel parts of our
% algorithm.
%
We can make this simplification for two reasons:
\begin{itemize}
\item We do not formalise type equality constraints, and our implementation
  in \textsc{ghc} (\cref{sec:equality-constraints}) takes care to not allow linear equality constraints to influence type inference.
  Indeed, a typical treatment of unification
  would be unsound for linear equalities, because it reuses the same
  equality many times (or none at all). Linear equalities make sense
  (\citet{shulman2018linear} puts linear
  equalities to great use), but they do not seem to lend themselves to
  automation.
\item We do not support, or intend to support, multiplicity
  polymorphism in constraint arrows. That is, the multiplicity of a
  constraint is always syntactically known to be either linear or
  unrestricted. This way, no equality constraints (which might, conceivably,
  relate multiplicity variables) can interfere with
  constraint resolution.
\end{itemize}
%
%% Our current focus is more narrow than a general typechecking algorithm for
%% all of \textsc{ghc}'s features.
%% As a consequence of these two restrictions (no linear equalities and no
%% multiplicity polymorphic constraints), type inference and (linear) class
%% constraint resolution are completely orthogonal. Therefore, the syntax-directed
%% constraint generation system presented in this section can legitimately assume
%% that type inference is solved elsewhere, greatly simplifying the presentation.


\subsection{Wanted Constraints}
\label{sec:wanteds}

The constraints $[[{C}]]$ generated in our system have a richer
logical structure than the simple constraints $[[{Q}]]$, above. Following
\textsc{ghc} and echoing \citet{OutsideIn}, we call these \emph{wanted constraints}:
they are constraints which the constraint solver \emph{wants} to prove.
An unproved wanted constraint results in a type error reported to the programmer.
%
$$
\begin{array}{lcll}
  [[{C}]] & \bnfeq & [[{Q}]] \bnfor [[{C1*C2}]] \bnfor [[{C1&C2}]] \bnfor [[{pi.(Q=>C)}]]&
                                                                \text{Wanted constraints}
\end{array}
$$
%
A simple constraint is a valid wanted constraint, and we have two forms of
conjunction for wanted constraints:
the new
$[[{C1 & C2}]]$ construction (read $[[{C1}]]$ \emph{with} $[[{C2}]]$), alongside
the more typical $[[{C1 * C2}]]$. These are
connectives from linear logic: $[[{C1*C2}]]$ is the
\emph{multiplicative} conjunction, and $[[{C1&C2}]]$ is the \emph{additive}
conjunction. Both connectives are conjunctions, but they differ
% rather dramatically % weasel words combo
in meaning. To satisfy $[[{C1*C2}]]$ one consumes the (linear)
assumptions consumed by satisfying $[[{C1}]]$ and those consumed by $[[{C2}]]$;
if an assumed linear constraint is needed to prove both $[[{C1}]]$ and $[[{C2}]]$,
then $[[{C1*C2}]]$ will not be provable, because that linear assumption cannot
be used twice. On the
other hand, satisfying $[[{C1&C2}]]$ requires that satisfying $[[{C1}]]$
and $[[{C2}]]$ must each
consume the \emph{same} assumptions, which $[[{C1 & C2}]]$ consumes as well.
Thus, if $[[{C}]]$ is assumed linearly (and we have no other assumptions),
then $[[{C*C}]]$ is not provable, while $[[{C&C}]]$ is.
The intuition, here, is that in $[[{C1 & C2}]]$, only
one of $[[{C1}]]$ or $[[{C2}]]$ will be eventually used. ``With'' constraints
arise from the branches in a $\kcase$-expression.

The last form of wanted constraint $[[{C}]]$ is an implication
$[[{pi.(Q=>C)}]]$. The more interesting case is $[[{omega.(Q=>C)}]]$:
to prove $[[{omega.(Q=>C)}]]$, you need to prove $[[{C}]]$ under the
\emph{linear} assumption $[[{Q}]]$, but without using any other linear
assumptions.

These implications arise when we unpack an existential
package that contains a linear constraint and also when checking a |let|-binding.
We can define scaling over wanted constraints by recursion as follows, where we
use scaling over simple constraints in the simple-constraint case:
$$
\left\{
  \begin{array}{lcl}
    [[{pi.(C1 * C2)}]] & = & [[{pi.C1 * pi.C2}]] \\
    [[{1.(C1 & C2)}]] & = & [[{C1 & C2}]] \\
    [[{omega.(C1 & C2)}]] & = & [[{omega.C1 * omega.C2}]] \\
    [[{pi.(rho.(Q => C))}]] & = & [[{(pi.rho).(Q => C)}]]
  \end{array}
\right.
$$
For the most part, scaling of wanted constraints is straightforward. The only
peculiar case is when we scale the additive conjunction $[[{C1&C2}]]$ by
$[[{omega}]]$, the result is a multiplicative conjunction. The intuition here is
that when if we have both $[[{omega.C1}]]$ and $[[{omega.C2}]]$, then
a choice between $[[{C1}]]$ and $[[{C2}]]$ can be made $[[{omega}]]$ times.

We define an entailment relation over wanteds in \Cref{fig:wanted:entailment}.
Note that this relation uses only simple constraints $[[{Q}]]$ as assumptions, as
there is no way to assume the more elaborate $[[{C}]]$\footnote{Allowing the full wanted-constraint syntax
in assumptions is the subject of work by \citet{quantified-constraints}.}.
\begin{figure}
  \maybesmall
  \centering
  \drules[C]{$[[Q |- C]]$} {Wanted-constraint entailment}
  {Dom,Id,Tensor,With,Impl}
  \caption{Wanted-constraint entailment}
  \label{fig:wanted:entailment}
\end{figure}

Before we move on to constraint generation proper, let us highlight a few
technical, yet essential, lemmas about the wanted-constraint
entailment relation.

\begin{lemma}[Inversion]
  \label{lem:inversion}
  The inference rules of $[[Q |- C]]$ can be read bottom-up (up to the
  set $[[{Dup}]]$) as well
  as top-down, as is required of $[[Q1 ||- Q2]]$ in
  \Cref{fig:entailment-relation}. That is:
  \begin{itemize}
  \item If $[[Q |- C1*C2]]$, then there exists $[[{Q1}]]$, $[[{Qdup}]]$ and $[[{Q2}]]$
    such that $[[Qdup \in Dup]]$, $[[Q1 * Qdup |- C1]]$, $[[Qdup * Q2 |- C2]]$, and
    $[[{Q}]] = [[{Q1 * Qdup * Q2}]]$.
  \item If $[[Q |- C1 & C2]]$, then $[[Q |- C1]]$ and $[[Q |- C2]]$.
  \item If $[[Q |- pi.(Q2 => C)]]$, then there exists $[[{Q1}]]$ such
    that $[[Q1 * Q2 |- C]]$ and  $[[{Q}]] = [[{pi.Q1}]]$
  \end{itemize}
\end{lemma}

\begin{lemma}[Scaling]
  \label{lem:wanted:promote}
  If $[[Q |- C]]$, then $[[pi.Q |- pi.C]]$.
\end{lemma}

\begin{lemma}[Inversion of scaling]
  \label{lem:wanted:demote}
  If $[[Q |- pi.C]]$ then $[[Q' |- C]]$ and $[[{Q}]] = [[{pi.Q'}]]$ for some $[[{Q'}]]$.
\end{lemma}

\subsection{Constraint Generation}
\label{sec:constraint-generation}
\label{sec:constraint-generation-soundness}

The process of inferring constraints is split into two parts: generating
constraints, which we do in this section, then solving them in
\cref{sec:constraint-solver}. Constraint generation is described by
the judgement $[[G |-> e : t ~> C]]$ (defined in
\Cref{fig:constraint-generation}) which outputs a constraint $[[{C}]]$
required to make $[[e]]$ typecheck.
The definition
$[[G |-> e : t ~> C]]$ is syntax directed, so it can directly be read as an
algorithm, taking as input a \emph{typing derivation} for $[[G |- e : t]]$
(produced by an external type inference oracle as discussed above). Notably, the
algorithm has access to the context splitting from the (previously computed)
typing derivation, and is
thus indeed syntax directed.
\info{See Fig.13, p39 of OutsideIn~\cite{OutsideIn}}

\info{Not caring about inferences simplifies $\packbox$ quite a bit, we
  are using the pseudo-inferred type to generate constraint. In a real
  system, we would need $\packbox$ to know its type (\emph{e.g.} using
  bidirectional type checking).}
\begin{figure}
  \maybesmall
  \centering
  \drules[G]{$[[G |-> e : t ~> C]]$}{Constraint generation}{Var, Abs,
    App, Pack, Unpack, Case, Let, LetSig}

  \caption{Constraint generation}
  \label{fig:constraint-generation}
\end{figure}

The rules of \Cref{fig:constraint-generation} constitute a mostly
unsurprising translation of the rules of \Cref{fig:typing-rules},
except for the following points of interest:

\emph{Case expressions.}
Note the use of $[[&]]$ in the conclusion of \rref{G-Case}.
We require that each branch of a $\kcase$ expression use the exact
same (linear) assumptions; this is enforced by combining the
emitted constraints with $[[&]]$, not $[[*]]$.
%
  This can also be understood in terms of the array example of
  \cref{sec:introduction}:
  if an array is freed in one branch of a $\kcase$, we require it to be freed (or freezed) in
the other branches too.
  Otherwise, the array's state will be unknown to the type system
  after the $\kcase$.
%

\emph{Implications.} The introduction of constraints local to a
  definition (\rref{G-LetSig}) corresponds to
  emitting an implication constraint.

\emph{Unannotated |let|.}
 However, the~\rref*{G-Let} rule does not produce an implication
  constraint, as we do not model |let|-generalisation~\cite{let-should-not-be-generalised}.
  % \cref{sec:let-generalisation} discusses this design
  % choice further.


\vspace{1ex}
The key property of the constraint-generation algorithm is that,
if the generated constraint is solvable, then we can indeed type the
term in the qualified type system of
\cref{sec:qualified-type-system}. That is,
these rules are simply an implementation of our declarative qualified
type system.

\begin{lemma}[Soundness of constraint generation]\label{lem:generation-soundness}
  For all $[[{Q_g}]]$, if $[[G |-> e : t ~> C]]$ and $[[Q_g |- C]]$ then
  $[[Q_g; G |- e : t]]$.
\end{lemma}

\subsection{Constraint Solving}
\label{sec:constraint-solver}

In this section,
we build a \emph{constraint solver} that proves
that $[[Q_g |- C]]$ holds, as required by \cref{lem:generation-soundness}.
The constraint solver is represented by the following judgement:
%
$$
[[ UCtx ; DCtx; LCtx_i |-s C ~> LCtx_o]]
$$
%
The judgement
takes in three contexts: $[[{UCtx}]]$, which holds all the unrestricted
atomic constraint assumptions, $[[{DCtx}]]$ which holds the linear atomic
assumptions which are members of $[[{Dup}]]$ and $[[{LCtx_i}]]$, which holds the linear
atomic constraint assumptions which aren't members of $[[{Dup}]]$.
The linear contexts $[[{DCtx}]]$, $[[{LCtx_i}]]$, and $[[{LCtx_o}]]$ have been described
as multisets (\cref{sec:constraint-domain}), but we treat them
as ordered lists in the more concrete setting here; we will see soon
why this treatment is necessary.

Linearity requires treating constraints as consumable resources. This
is what $[[{LCtx_o}]]$ is for: it contains the hypotheses of
$[[{LCtx_i}]]$ which are not consumed when proving $[[{C}]]$. As
suggested by the notation, it is an output of the
algorithm. Constraints from $[[{DCtx}]]$ are never outputted in
$[[{LCtx_i}]]$: if constraints from $[[{DCtx}]]$ remain unused, we
weaken them instead.

If the constraint solver finds a solution, then the output linear constraints
must be a subset of the input linear constraints, and the solution must indeed
be entailed from the given assumptions.
\begin{lemma}[Constraint solver soundness]
\label{lem:solver-soundness} If $[[UCtx; DCtx; LCtx_i |-s C ~> LCtx_o]]$, then:
\begin{enumerate}
\item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
\item $[[(UCtx, DCtx \u LCtx_i) |- C * (emptyset, LCtx_o)]]$
\end{enumerate}
\end{lemma}

To handle simple wanted constraints, we will need  a domain-specific
\emph{atomic-constraint solver} to be the algorithmic counterpart of the
abstract entailment relation of \cref{sec:constraint-domain}. The
main solver will appeal to this atomic-constraint solver when solving atomic
constraints.  The atomic-constraint solver is represented by the following
judgement:
%
$$
[[ UCtx ; DCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]
$$

It has a similar structure to the main solver, but only deals with atomic
constraints. Even though the main solver is parameterised by this
atomic-constraint solver, we will give an instantiation in
\cref{sec:simple-constr-solv}.
%
We require the following property of the atomic-constraint solver:

\begin{property}[Atomic-constraint solver soundness]
\label{prop:atomic-solver-soundness} If $[[UCtx; DCtx; LCtx_i |-simp pi.q ~> LCtx_o]]$, then:
\begin{enumerate}
\item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
\item $[[(UCtx, DCtx \u LCtx_i) ||- pi.q * (emptyset, LCtx_o)]]$
\end{enumerate}
\end{property}

\subsubsection{Constraint Solver Algorithm}

Building on this atomic-constraint solver, we use a linear proof
search algorithm based on the recipe given
by~\citet{resource-management-for-ll-proof-search}. \Cref{fig:constraint-solver}
presents the rules of the constraint solver.

\begin{figure}
  \maybesmall
  \centering
  \drules[S]{$[[UCtx ; DCtx ; LCtx_i |-s C_w ~> LCtx_o ]]$}{Constraint solving}{Atom, Mult, ImplOne, Add, ImplMany}
  \caption{Constraint solver}
  \label{fig:constraint-solver}
\end{figure}

\begin{itemize}
  \item The~\rref*{S-Mult} rule proceeds by solving one side of a
conjunction first, then passing the output constraints to the other side.
Both the unrestricted context and the duplicable context are shared between both sides.
  \item The~\rref*{S-Add} rule handles additive conjunction. The linear
constraints are shared between the branches (since additive conjunction is
generated from $\kcase$ expressions, only one of them is actually going to be
executed). Both branches must consume exactly the same
resources.
  \item The~\rref*{S-ImplOne} rules handles linear implications. The
    unrestricted and linear components of the assumption are unioned
    with their respective context when solving the conclusion. Note
    how the linear constraints, in particular, are classified
    according to whether they are members of $[[{Dup}]]$ or not. Importantly (see
\cref{sec:simple-constr-solv}), the linear assumptions are
added to the front of the lists.
 The side condition that the output
context is a subset of the input context ensures that the implication
fully consumes its assumption and does not leak it to the ambient
context.
\item The~\rref*{S-ImplMany} rules handles unrestricted implication.
  The conclusion uses its own linear assumption, but none of the other
  linear constraints.  This is because, as per~\rref*{C-Impl},
  unrestricted implications can only use an unrestricted context. In
  particular, crucially, the constraints from $[[{DCtx}]]$, despite
  being duplicable and discardable, are not (and cannot) be
  used to prove an unrestricted implication, as first discussed
  in~\cref{sec:constraint-domain}.
\end{itemize}

\subsubsection{An Atomic-Constraint Solver}
\label{sec:simple-constr-solv}

So far, the atomic-constraint domain has been
an abstract parameter. In this section, though, we offer a concrete
domain which supports our examples.

For the sake of our examples, we need very little: linear
constraints can remain abstract. It is thus sufficient for the entailment relation
(\Cref{fig:simpl-entailment}) to prove $[[{q}]]$ if and only if it
is already assumed---while respecting linearity. That is, with the
exception of a distinguished constraint $[[{Linearly}]]$, which can be
duplicated and discarded, and we use to model the |Unique|
constraint from~\ref{sec:Unique-constraint}. The set $[[{Dup}]]$ is
defined to only contain $[[{Linearly}]]$, therefore the $[[{DCtx}]]$
context is a sequence of 0 or more $[[{Linearly}]]$.

\begin{figure}
  \maybesmall
  \centering
  \begin{subfigure}{\linewidth}
    \drules[Q]{$[[Q1 ||- Q2]]$}{Entailment relation}{Hyp,Prod,Empty,
      DiscardD, DupD}
    \caption{Entailment relation}
    \label{fig:simpl-entailment}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \drules[Atom]{$[[UCtx ; DCtx ; LCtx |-simp pi.q ~>
      LCtx_o]]$}{Atomic-constraint solver}{Many,OneL,OneD, OneU}
    \caption{Atomic-constraint solver}
    \label{fig:simpl-solver}
  \end{subfigure}
  \caption{A stripped-down constraint domain}
  \label{fig:predicate-domain}
\end{figure}

The corresponding atomic-constraint solver
(\Cref{fig:simpl-solver}) is more interesting.
It is deterministic: in all circumstances,
only one of the three rules can apply. This means that the
algorithm does not guess, thus never needs to backtrack.
Avoiding guesses is a key property of \textsc{ghc}'s solver~\cite[Section~6.4]{OutsideIn},
one we must maintain if we are to be compatible with \textsc{ghc}.

\Cref{fig:simpl-solver} is also where the fact that the
$[[{LCtx}]]$ are lists comes into play. Indeed, \rref{Atom-OneL}
takes care to use the most recent occurrence of $[[{q}]]$
(remember that \rref{S-ImplOne} adds the new hypotheses on the front of
the list). To understand why, consider the following example:
\begin{code}
f =   unique $
        let    pack (Ur arr) = new 10
               fr   ::  constraint (RW n) =>. ()   ^^
               fr   =   free arr
               ()   =   fr                         in Ur ()

\end{code}
In this example, the programmer meant for
|free| to use the |constraint(RW n)| constraint introduced locally in the type
of |fr|. Yet
there are actually two linear |constraint(RW n)| constraints: this local one and the
one assumed when unpacking |arr|. The wrong
choice among the constraints will lead the algorithm to fail.
Choosing the first $[[{q}]]$ linear assumption guarantees we get the
most local one.

Another interesting feature of the solver (\Cref{fig:simpl-solver}) is that
no rule solves a linear constraint if it appears both in the
unrestricted and a linear context.
Consider the following (contrived) \textsc{api}:
\begin{code}
class constraint (C)
\end{code}

\vspace{-2ex}\noindent
\begin{minipage}{0.5\linewidth}
> giveC :: (constraint (C) => Int) -> Int
\end{minipage}
\begin{minipage}{0.5\linewidth}
> useC :: constraint (C) =>. Int
\end{minipage}
|giveC| gives an unrestricted copy of |constraint (C)| to some continuation, while |useC|
uses |constraint (C)| linearly. Now consider two potential consumers of this \textsc{api}:

\noindent
\begin{minipage}{0.5\linewidth}
> ambiguous1 :: constraint (C) =>. Int
> ambiguous1 = giveC useC
\end{minipage}
\begin{minipage}{0.5\linewidth}
> ambiguous2 :: constraint (C) =>. (Int, Int)
> ambiguous2 = (giveC useC, useC)
\end{minipage}
Looking at |ambiguous1|, the invocation of |useC| has both a linear |constraint (C)| in scope, and
a more local unrestricted |constraint (C)|. The strategy to pick the more local constraint
fails here, because it would leave the linear |constraint (C)| unconsumed. A
tempting refinement might be to always consume the most local \emph{linear}
constraint. That would handle handles |ambiguous1| correctly, but fail on |ambiguous2|. In
the case of the latter, if the invocation of |giveC useC| consumes the linear
|constraint (C)|, then the second |useC| invocation will fail. It is possible to
give a type derivation to |ambiguous2| in the qualified type system
of \cref{sec:qualified-type-system} by making the first |useC| consume the
unrestricted |constraint (C)| and the second |useC| consume the linear
|constraint (C)|.
This assignment, however, would require
the constraint solver to guess when solving the constraint from the first |useC|.
Accordingly, in order to both avoid backtracking and to keep type inference
independent of the order terms appear in the program text, |bad| is
rejected.
This introduces incompleteness with respect the entailment
relation. We conjecture that this is the only source of
incompleteness that we introduce beyond what is already in
\textsc{ghc}~\cite[Section~6]{OutsideIn}.

\section{Desugaring}
\label{sec:desugaring}

The semantics of our language is given by desugaring it into
a simpler core language: a variant of the $λ^q$
calculus~\cite{LinearHaskell}. We
define the core language's type system here; its operational semantics
is the same, \emph{mutatis mutandis}, as that of Linear Haskell.

\subsection{The Core Calculus}
\label{sec:core-calculus}
\label{sec:ds:inferred-constraints}

\begin{figure}
  \maybesmall
  \centering
  $$
  \begin{array}{lcll}
    [[{s}]] & \bnfeq & [[{forall as. t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & ... \bnfor [[{exists as. t o- u}]] & \text{Types} \\
    [[{e}]] & \bnfeq & ... \bnfor [[{pack (e1, e2)}]] \bnfor [[{unpack (x,y)=e1 in e2}]] & \text{Expressions}
  \end{array}
  $$

  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Pack,Unpack}
  \caption{Core calculus (subset)}
  \label{fig:core-typing-rules}\label{fig:core-grammar}
\end{figure}

The core calculus is a variant of the type system defined in
\cref{sec:qualified-type-system}, but without constraints. That is, the evidence for constraints is passed explicitly in this core calculus.
%
Following $λ^q$, we assume the existence of the following data types:
\begin{itemize}
\item $[[{t1 ** t2}]]$ with sole constructor
  $[[ (,) : forall a b. a ->_1 b ->_1 a ** b ]]$. We will write $[[(e1,
  e2)]]$ for $[[{(,) e1 e2}]]$.
\item $[[{unit}]]$ with sole constructor $[[() : unit]]$.
\item $[[{Ur t}]]$ with sole constructor $[[ Ur : forall a. a ->_omega
  Ur a]]$
\end{itemize}
%
\Cref{fig:core-typing-rules}
highlights the differences from the qualified system:
\begin{itemize}
  \item Type schemes $[[{s}]]$ do not support qualified types.
  \item Existentially quantified types ($[[{exists as. t o= Q}]]$) are now represented as an (existentially quantified, linear) pair of values ($[[{exists as. t2 o- t1}]]$).
Accordingly, $\packbox$ operates on pairs.
\end{itemize}
%
The differences between our core calculus and $λ^q$ are as follows:
\begin{itemize}
\item We do not support multiplicity polymorphism.
\item On the other hand, we do include type polymorphism.
\item Polymorphism is implicit rather than explicit. This is not an
  essential difference, but it simplifies the presentation. We could,
for example, include more details in the terms in order to make type-checking
more obvious; this amounts essentially to an encoding of typing derivations
in the terms\footnote{See, for example, \citet{weirich-icfp17} and their
comparison between an implicit core language D and an explicit one DC.}.
\item We have existential types. These can be realised in regular Haskell as a
  family of datatypes.
\end{itemize}

Using \cref{lem:generation-soundness} together with
\cref{lem:solver-soundness} we know that if
$[[G |-> e : t ~> C]]$ and $[[UCtx ; DCtx ; LCtx |-s C ~> emptyset ]]$, then
$[[(UCtx, DCtx \u LCtx) ; G |- e : t]]$.
%
It only remains to desugar derivations of $[[Q;G|-e : t]]$ into the
core calculus.

\subsection{From Qualified to Core}
\label{sec:ds:from-qualified-core}

\subsubsection{Evidence}
In order to desugar derivations of the qualified system to the core calculus,
we pass evidence explicitly\footnote{This technique is also often called
  dictionary-passing style \cite{type-classes-impl} because, in the case of type classes, evidences are
  dictionaries, and because type classes were the original form of constraints
  in Haskell.}.
%
To do so, we require some more material from
constraints. Namely, we assume a type $[[{Ev(q)}]]$ for each atomic
constraint $[[{q}]]$,
defined in \Cref{fig:evidence}.  The $[[Ev(Hole)]]$ operation
extends to simple constraints as
$[[{Ev(Q)}]]$.
Furthermore, we require that for every $[[{Q1}]]$ and $[[{Q2}]]$
such that $[[Q1 ||- Q2]]$, there is a (linear) function
$[[Ev(Q1 ||- Q2) : Ev(Q1) ->_1 Ev(Q2)]]$.

Let us now define a family of functions $[[{Ds(Hole)}]]$ to translate
the type schemes, types, contexts, and typing derivations of the qualified system into the
types, type schemes, contexts, and terms of the core calculus.

\subsubsection{Translating Types}
Type schemes $[[{s}]]$ are translated by turning the implicit argument $[[{Q}]]$
into an explicit one of type $[[{Ev(Q)}]]$. Translating types $[[{t}]]$
and contexts $[[{G}]]$ proceeds as
expected.

\begin{minipage}{0.5\linewidth}
%
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(forall as. Q =o t)}]] & = & [[{forall as. Ev(Q) ->_1 Ds(t)}]] \\
  \end{array}
\right.
$$
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(t1 ->_pi t2)}]] & = & [[{Ds(t1) ->_pi Ds(t2)}]] \\
    [[{Ds(exists as. t o= Q)}]] & = & [[{exists as. Ds(t) o- Ev(Q)}]]
  \end{array}
\right.
$$
%
\end{minipage}%
\begin{minipage}{0.5\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(empty)}]] &=& [[{empty}]] \\
    [[{Ds(G, x :_pi t)}]] &=& [[{Ds(G), x :_pi Ds(t)}]]
  \end{array}
\right.
$$
\end{minipage}

\subsubsection{Translating Terms}
Given a derivation $[[Q;G |- e : t]]$, we can build an expression
$[[Ds(z;Q;G |- e : t)]]$, such that
$[[Ds(G), z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ (for some fresh variable
$[[{z}]]$). Even though we abbreviate the derivation as only its
concluding judgement, the translation is defined recursively on the
whole typing derivation: in particular, we have access to typing rule
premises in the body of the definition.
%
We present some of the interesting cases in \Cref{fig:desugaring}.
\begin{figure}
    \maybesmall
\centering
  \begin{subfigure}{0.3\linewidth}%
$$
\left\{
  \begin{array}{lcl}
    [[{Ev(1.q)}]] & = & [[{Ev(q)}]] \\
    [[{Ev(omega.q)}]] & = & [[{Ur (Ev(q))}]] \\
    [[{Ev(Empty)}]] & = & [[{unit}]] \\
    [[{Ev(Q1 * Q2)}]] & = & [[{Ev(Q1) ** Ev(Q2)}]]
  \end{array}
\right.
$$
  \caption{Evidence passing}
  \label{fig:evidence}
  \end{subfigure}\hfill
  \begin{subfigure}{0.7\linewidth}%
%{
%format |- = "[[|-]]"
%format ||- = "[[||-]]"
%format G = "[[{G}]]"
%format G1 = "[[{G1}]]"
%format G2 = "[[{G2}]]"
%format Q = "[[{Q}]]"
%format Q1 = "[[{Q1}]]"
%format Q2 = "[[{Q2}]]"
%format * = "[[*]]"
%format Ds(z)(j) = "\dsterm{\ottmv{" z "}}{" j "}"
%format Ev(x) = "\dsevidence{" x "}"
%format unpack(x) = "\klet\ \packbox " x
%format u = "[[{u}]]"
%format OneOf (a) (b) = a"\mathop{:_{1}}"b
%format Sub t as bs = t "[" as "/" bs "]"
%format case_1 = "[[case_1]]"
%format e1 = "[[{e1}]]"
%format e2 = "[[{e2}]]"
%format t1 = "[[{t1}]]"
%format as = "[[{as}]]"
%format ts = "[[{ts}]]"
%format us = "[[{us}]]"
%format let_1 = "[[let_1]]"
%format z1 = "[[{z1}]]"
%format z2 = "[[{z2}]]"
%format z2' = "[[{z2}]]''"
%format + = "\ottsym{+}"
%format t = "\tau"
$$
\left\{
  \;
  \begin{minipage}{0.5\linewidth}
\begin{code}
Ds(z)(Q;G |- x : Sub u ts as) = x z
Ds(z)(Q * Sub Q1 us as;G |- pack e : exists as. t .<= Q1) =
  case_1 z of { (z', z'') ->
    pack (z'', Ds(z')( Q ; G |- e : Sub t us as))}
Ds(z)(Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t)  =
  case_1 z of { (z1, z2) ->
    unpack (z',x) = Ds(z1)(Q1;G1 |- e1 : exists as. t1 .<= Q) in
    let_1 z2' = (z2,z') in
    Ds(z2')(Q2 * Q;G2, OneOf x t1 |- e2 : t) }
Ds(z)(Q;G |- e : t)  =  -- \rref{E-Sub}
  let_1 z' = Ev(Q ||- Q1) z in Ds(z')(Q1;G |- e : t)
...
\end{code}
  \end{minipage}
\right.
$$
%}
  \caption{Desugaring (subset)}
  \label{fig:desugaring}
  \end{subfigure}
  \caption{Evidence passing and desugaring}
\end{figure}

The cases correspond to the~\rref*{E-Var},~\rref*{E-Unpack}\footnote{The attentive
reader may note that the case for $\kunpack$ extracts out $[[{Q1}]]$ and $[[{Q2}]]$
from the provided simple constraint. Given that simple constraints $[[{Q}]]$ have no
internal ordering and allow duplicates (in the non-linear component), this splitting
is not well defined. To fix this, an implementation would have to \emph{name} individual
components of $[[{Q}]]$, and then the typing derivation can indicate which constraints
go with which sub-expression. Happily, \textsc{ghc} \emph{already} names its constraints,
and so this approach fits easily in the implementation. We could also augment our formalism
here with these details, but they add clutter with little insight.}, and~\rref*{E-Sub} rules, respectively.
Variables are stored with qualified types in the environment, so they get
translated to functions that take the evidence as argument. Accordingly, the evidence
is inserted by passing $[[{z}]]$ as an argument.
Handling \rref*{E-Unpack} requires splitting the context into two: $[[{e1}]]$ is desugared as a pair, and the evidence
it contains is passed to $[[{e2}]]$. Finally, subsumption summons the function corresponding to the entailment relation $[[Q ||- Q1]]$
and applies it to $[[{z}]]$ : $[[{Ev(Q)}]]$ then proceeds to desugar $[[{e}]]$ with the resulting evidence for $[[{Q1}]]$.
Crucially, since $[[{Ds(z;Hole)}]]$ is defined on \emph{derivations}, we can access the premises used in the rule.
Namely, $[[{Q1}]]$ is available in this last case from the~\rref*{E-Sub} rule's premise.

It is straightforward by induction, to verify that desugaring is correct:
%
\begin{theorem}[Desugaring]
If $[[Q;G |- e : t]]$, then
$[[Ds(G), z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$, for any fresh
variable $[[{z}]]$.
\end{theorem}

Thanks to the desugaring machinery, the semantics of a language with linear
constraints can be understood in terms of a simple core language with linear
types, such as $λ^q$, or indeed, \textsc{ghc} Core.

\section{Integrating into \textsc{ghc}}

One of the guiding principles behind our design was ease of integration with
modern Haskell. In this section we describe some of the particulars of adding
linear constraints to \textsc{ghc}.

\subsection{Implementation}
\label{sec:implementation}

We have written a prototype implementation~\cite{prototype} of linear constraints on top of \textsc{ghc} 9.1, a version that already
ships with the @LinearTypes@ extension. Function arrows (|->|) and context arrows
(|=>|) share the same internal representation in the typechecker, differentiated
only by a boolean flag. Thus, the @LinearTypes@ implementation effort has already
laid down the bureaucratic ground work of annotating these arrows with
multiplicity information.

The key changes affect constraint generation and constraint solving. Constraints
are now annotated with a multiplicity, according to the context from
which they arise. With @LinearTypes@, \textsc{ghc} already scales the usage
of term variables. We simply modified the scaling function to capture all the
generated constraints and re-emit a scaled version of them, which is a fairly local
change.

The constraint solver maintains a set of given constraints (the \emph{inert set}
in \textsc{ghc} jargon), which corresponds to the $[[{UCtx}]]$, $[[{DCtx}]]$, and $[[{LCtx}]]$
contexts in our solver judgements in \cref{sec:constraint-solver}. When
the solver goes under an implication, the assumptions of the implication are
added to set of givens. When a new given is added, we record the \emph{level} of
the implication (how many implications deep the constraint arises from) along
with the constraint. So that in case there are multiple matching
givens, the constraint solver selects the innermost one
(in \cref{sec:constraint-solver} we use an ordered list
for this purpose).

As constraint solving proceeds, the compiler pipeline constructs a
term in a typed language known as \textsc{ghc} Core~\cite{system-fc}.
In Core, type class constraints are turned into explicit evidence (see
\cref{sec:desugaring}). Thanks to being fully annotated, Core has
decidable typechecking, which is used to find and fix bugs in
the compiler (the Haskell type checker finds mistakes in user
programs). Thus, the Core typechecker verifies that the desugaring
procedure produced a linearity-respecting program before code
generation occurs.

\subsection{Interaction with Other Features}

Since constraints play an important role in \textsc{ghc}'s type system, we must
pay close attention to the interaction of linearity with other language features
related to constraints. Of these, we point out two that require some extra care.
\info{There isn't room to properly explain how we can implement
|Unique| constraints. We don't already speak of
desugaring in the implementation section, so I'd need to give a bit of
context about wrappers or something.

The right approach is to count the number of `Linearly` used. We need
wrappers at the toplevel of definitions, and at each branch of a match
(even if it doesn't introduce an implication, as we may need to adjust
the number of `Linearly` in some branches (presumably by
weakening)). And we do the appropriate amount of
duplications/discards in these wrappers.}

\subsubsection{Superclasses}

Haskell's type classes can have \emph{superclasses}, which place constraints on
all of the instances of that class. For example, the |Ord| class is defined as
\begin{code}
class constraint(Eq a) => constraint(Ord a) where ...
\end{code}
which means that every ordered type must also support equality. Such
superclass declarations extend the entailment relation: if we know that a type
is ordered, we also know that it supports equality. This is troublesome if we
have a linear occurrence of |constraint(Ord a)|, because then using this entailment, we could
conclude that a linear constraint (|constraint(Ord a)|) implies an unrestricted constraint
(|constraint(Eq a)|), which violates \cref{lem:q:scaling-inversion}.

But even linear superclass constraints cause trouble. Consider a version of |constraint(Ord a)|
that has |constraint(Eq a)| as a linear superclass.
\begin{code}
class constraint(Eq a) =>. constraint(Ord a) where ...
\end{code}
When given a linear |constraint(Ord a)|, should we keep it as |constraint(Ord a)|, or rewrite to
|constraint(Eq a)| using the entailment? Short of backtracking, the constraint solver
needs to make a guess, which \textsc{ghc} never does.

To address both of these issues at once, we make the following rule: the
superclasses of a linear constraint are ignored.

\subsubsection{Equality Constraints}
\label{sec:equality-constraints}

In \cref{sec:type-inference} we argued that \emph{type} inference and
\emph{constraint} inference can be performed independently. However, this is not
the case for \textsc{ghc}'s constraint domain, because it supports equality
constraints, which allows unification problems to be deferred, and potentially
be solvable only after solving other constraints first.

To reconcile this with our presentation, we need to ensure that
\emph{unrestricted constraint} inference and \emph{linear constraint} inference
can be performed independently. That is, solving a linear constraint should
never be required for solving an unrestricted constraint. This is ensured by
\cref{lem:q:scaling-inversion}.

They key is to represent unification problems as \emph{unrestricted} equality
constraints, so a given linear equality constraint cannot be used during type
inference.  This way, linear equalities require no
special treatment, and are harmless.

% \section{Extensions}
% \label{sec:design-decisions}

% The system presented in \cref{sec:qualified-type-system,sec:type-inference} is already capable of supporting the examples
% in \cref{sec:what-it-looks-like,sec:memory-ownership}. In this
% section, we consider some potential avenues for extensions.

% \subsection{let generalisation}
% \label{sec:let-generalisation}

% As discussed in \cref{sec:constraint-generation}, the~\rref*{G-Let} rule of our
% constraint generator does not generalise the type of |let|-bindings, which is in
% line with \textsc{ghc}'s existing behaviour~\cite[Section 4.2]{OutsideIn}.
% There, this behaviour was guided by concerns around inferring type variables,
% which is harder in the presence of local equality assumptions (\emph{i.e.}
% \textsc{gadt} pattern matching).

% In this section, however, we argue that generalising over linear constraints
% may, in fact, improve user experience.
% Let us revisit the |firstLine| example from \cref{sec:introduction}, but
% this time, instead of executing |closeFile| directly, we assign it to a variable
% in a |let|-binding:
% \begin{code}
% firstLine :: FilePath -> IOL String
% firstLine fp =   do  {  pack (Ur h) <- openFile fp
%                      ;  let closeOp = closeFile h
%                      ;  pack (Ur xs) <- readLine h
%                      ;  closeOp
%                      ;  return xs }
% \end{code}
% This program looks reasonable; however, it is rejected. The type of
% |closeOp| is |IOL ()|, which means that the definition of |closeOp|
% consumes the linear constraint |Open h|. So, by the time we attempt
% |readLine h|, the constraint is no longer available.

% What the programmer really meant, here, was for |closeOp| to have type
% |constraint (Open h) =>. IOL ()|. After all, a |let| definition is not part of the
% sequence of instructions: it is just a definition for later, not
% intended to consume the current state of the file. With no |let|-generalisation,
% the only way to give |closeOp| the type |constraint (Open h) =>. IOL
% ()| is to give |closeOp| a type signature. In current \textsc{ghc}, we
% can't write that signature down, since there is no syntax to bind
% the type variable |h| in the program text\footnote{\citet{variables-in-patterns} describe a way to fix this shortcoming.}. But
% even ignoring this, it would be rather unfortunate if the default
% behaviour of |let|, in the presence of linear constraints, almost never was
% what the programmer wants.

% To handle $\klet$-generalisation, let us consider the following rule
% %
% $$
% \drule{G-LetGen}
% $$
% %
% This rule is non-deterministic, because it requires finding $[[{Q_r}]]$ and
% $[[{Q}]]$. We can modify the constraint solver of
% \cref{sec:constraint-solver} to find $[[{Q_r*Q}]]$, but we
% still have to split the residual into $[[{Q_r}]]$ and
% $[[{Q}]]$ somehow.

% %% What we would like to say is ``$[[{Q}]]$ is the set of linear
% %% constraints''. But it's not clear how to make it formal.

% Any predictable strategy would do: as long as our rule an instance of the
% \rref*{G-LetGen} rule, constraint generation will be sound. Experience
% will tell whether we can find a better suited strategy than the current
% one, which never generalises any constraint.

% \subsection{Empty cases}
% \label{sec:empty-cases}

% Throughout the article we have assumed that $\kcase$-expressions
% always have a non-empty list of alternatives. This is, incidentally,
% also how Haskell originally behaved; though \textsc{ghc} now has an
% |EmptyCase| extension to allow empty lists of alternatives.

% % Not allowing empty lists of alternatives is, therefore not terrible in
% % principle. Though it makes empty types more awkward than they need to
% % be, and, of course, to support the entirety of \textsc{ghc}, we will
% % need to support empty lists of alternatives.

% The reason why it has been omitted from the rest of the article is
% that generating constraints for an empty $\kcase$ requires an $0$-ary version
% of $[[{C1&C2}]]$, usually written $[[{Top}]]$ in Linear Logic. The
% corresponding entailment rule would be
% $$
% \drule{C-Top}
% $$
% That is $[[{Top}]]$ is unconditionally true, and can consume any number
% of linear given constraints--- indeed, the corresponding program is already crashing.
% The \rref*{C-Top} rule thus induces a
% considerable amount of non-determinism in the constraint solver. Eliminating the
% non-determinism induced by $[[{Top}]]$ is ultimately
% what~\citet{resource-management-for-ll-proof-search} builds up
% to. Their methods can be adapted to the constraint solver of
% \cref{sec:constraint-solver} without any technical
% difficulty. We chose, however, to keep empty cases out the
% presentation because they have a very high overhead and would distract from
% the point. Instead, we refer readers
% to~\citet[Section 4]{resource-management-for-ll-proof-search} for a
% careful treatment of $[[{Top}]]$.

\subsection{Inferring Packing and Unpacking}
\label{sec:implicit-existentials}
Recent work~\cite{existentials} describes an algorithm (call it \textsc{edwl}, after the
authors' names) that
can infer the location of the pack and unpack annotations (our $\packbox$ and $\kunpack$)
in a program.%
\footnote{Actually, \citet{existentials} use an $\ottkw{open}$ construct instead of $\kunpack$
to access the contents of an existential package, but that distinction does
not affect our usage of existentials with linear constraints.}
In Section~9.2 of that paper,
the authors extend their system to include class constraints,
much as we allow our existential packages to carry linear constraints.

Accordingly, \textsc{edwl} would work well for us here and remove the need for these annotations.
The \textsc{edwl} algorithm is only a small change on the way some types are treated during
bidirectional type-checking. Though the presentation of linear constraints is not
written using a bidirectional algorithm, our implementation in \textsc{ghc}
is indeed bidirectional (as \textsc{ghc}'s existing type inference algorithm
is bidirectional, as described by \citet{practical-type-inference} and
\citet{visible-type-application}) and produces constraints much like we
have presented here, formally. None of this would change in adapting \textsc{edwl}.
Indeed, it would seem that the two extensions are orthogonal in implementation,
though avoiding the need for explicit packing and unpacking would
make linear constraints easier to use.

% One challenge if existential packing and unpacking are inferred is that
% the order in which a program is executed might become ambiguous.
% Picking up the example from \cref{sec:atomic-references},
% consider this function:

% > writeTwo :: constraint (RW n) =>. AtomRef a n -> a -> a -> () .<= constraint (RW n)
% > writeTwo ref val1 val2 =  let  unit1 = writeRef ref val1
% >                                unit2 = writeRef ref val2 in
% >                           ()

% In understanding this code, we first must worry about laziness. Haskell's |let|
% is lazy, meaning that the calls to |writeRef| might never be evaluated, if nothing
% forces them. Surprisingly, whether or not these get evaluated depends on the types
% that are inferred for |unit1| and |unit2|.

% One possibility is
% |unit1, unit2 :: constraint (RW n) =>. () .<= constraint (RW n)|. With this inferred type,
% the calls to |writeRef| are never evaluated: the provided |constraint (RW n)| is just
% packed with the |()| in the result, ignoring the two |unit|s.

% Another possibility is |unit1, unit2 :: ()|. With this inferred type, both |writeRef|s
% must consume and re-produce the |constraint (RW n)| constraint. Because the produced
% constraint is used, the |writeRef| call must be evaluated (so it can produce that |constraint (RW n)|
% constraint), and the reference |ref| will be updated. However, we still do not
% know the order in which the |writeRef|s will be evaluated: perhaps |unit1| consumes
% the |constraint (RW n)| produced by |unit2|, forcing the |writeRef|s in the
% opposite order to how they are written.

% This is disappointing: we cannot have type inference tell us what order
% our program is to be evaluated. One solution is to reject ambiguous programs
% like |writeTwo|. This could be achieved by changing \rref{Atom-OneL} of
% \Cref{fig:simpl-solver} to require that the desired constraint
% has precisely one matching assumption. Then, when there is any ambiguity,
% we simply reject. Would this be too restrictive in practice? A middle ground
% might be to track (in our list of linear assumptions) when we enter a more
% local scope (this is actually already done by \textsc{ghc}), and reject only
% programs with multiple identical assumptions in the same local scope.

% The issues here---similar to those raised in \cref{sec:let-generalisation}---have
% potential solutions, but the precise way forward is best informed by
% experience, as users begin to adopt these features and we get a better
% sense for prominent patterns and how our choices should be tuned.

\section{Related work}
\label{sec:related-work}

\paragraph{OutsideIn}
\label{sec:outsidein}

Our aim is to integrate the present work in \textsc{ghc}, and
accordingly the qualified type system in
\cref{sec:qualified-type-system} and the constraint inference
algorithm in \cref{sec:type-inference} follow a similar
presentation to that of OutsideIn~\cite{OutsideIn}, \textsc{ghc}'s
constraint solver algorithm.  Even though our presentation is
self-contained, we outline some of the differences from that work.

The solver judgement in OutsideIn takes the following form:
%
\[\mathcal{Q}\ ;\ Q_{\mathit{given}}\ ;\ \overline{\alpha}_{\mathit{tch}} \overset{\mathit{solv}}{\mapsto} C_{\mathit{wanted}} \leadsto Q_{\mathit{residual}}\  ; \ \theta\]
%
The main differences between OutsideIn's solver judgement and our solver judgements in \cref{sec:constraint-solver} are:
\begin{itemize}
  \item OutsideIn's judgement includes top-level axioms schemes separately
($\mathcal{Q}$), which we have omitted for the sake of brevity and are instead
included in $Q_{\mathit{given}}$.
  \item We present the \emph{given} constraints ($Q_{\mathit{given}}$ in OutsideIn) as two separate
constraint sets $[[{UCtx}]]$ and $[[{LCtx}]]$, standing for the unrestricted and linear
parts respectively.
  \item In addition to constraint inference, OutsideIn performs type
inference, requiring additional bookkeeping in the solver judgment. The solver
takes as input a set of \emph{touchable} variables $\overline{\alpha}_{tch}$
which record the type variables that can be unified at any given time, and
produces a type substitution $\theta$ as an output.
As discussed in \cref{sec:type-inference}, we do not perform type
inference, only constraint inference. Therefore, our solver need not return a
type assignment.
  \item Both
OutsideIn and our solver output a set of constraints, $Q_{\mathit{residual}}$ and
$[[{LCtx_o}]]$ respectively. However, the meaning of these contexts is different.
OutsideIn's \emph{residual} constraints $Q_{\mathit{residual}}$
correspond to the part of $C_{\mathit{wanted}}$ that could not be solved from the
assumptions. These residuals are then quantified over in the generalisation step
of the inference algorithm. We omit these residuals, which means that our
algorithm cannot infer qualified types.
Our \emph{output} constraints $[[{LCtx_o}]]$ instead correspond to the part of the
\emph{linear} givens $[[{LCtx_i}]]$ that were not used in the solution for $[[{C_w}]]$.

\item Finally, while OutsideIn has a single kind of conjunction, our constraint
language requires two: $[[{Q1*Q2}]]$ and $[[{Q1&Q2}]]$. This shows up when
generating constraints for $\kcase$ expressions in the~\rref{G-Case} rule.
OutsideIn accumulates constraints across branches (taking the union of each
branch), whereas we need to make sure that each branch of a $\kcase$-expression
consumes the same constraints.
\end{itemize}

%% I (csongor) don't think this is worth mentioning
%% \item A more minor difference with OutsideIn is that we have an
%%   explicit \rref*{E-Sub} rule, while OutsideIn uses simple constraint
%%   entailment directly in the relevant rules. In OutsideIn, only the
%%   \rref*{E-Var} rule needed subsumption; we would also need it for the
%%   \rref*{E-Pack} rule as well. So we preferred having one shared
%%   dedicated rule.
\paragraph{Ownership}

% The memory ownership example of \cref{sec:memory-ownership} is
% strongly inspired by Rust.
%
Ownership and borrowing are the key features of Rust's safe memory management model.
% As a consequence, it has a much more convenient syntax
% than Linear Haskell with linear constraints can propose.
% Rust's
% convenient syntax comes at the price that it is almost impossible to
% write tail-recursive functions, which is surprising from the
% perspective of a functional programmer.
% On the other hand, the focus of Linear Haskell, as well as this paper,
% is to provide programmers with the tools to create safe interfaces and
% libraries.
In \cref{sec:memory-ownership} we show how linear constraints can be used to
implement such an ownership model as a library.
% The language itself is agnostic about what linear
% constraints mean.
Although linear constraints do not have the
convenience of Rust's syntax, we expect that they will support a
greater variety of abstractions.
% Even though Rust programmers have
% come up with varied abstractions which leverage the borrowing
% mechanism to support applications going beyond memory management (for
% instance, safe file handling), it is unclear that all applications
% supported by linear constraints are reducible to the borrowing
% mechanism.

Clean is another language with built-in ownership typing. Like Haskell
it is a lazy language. Mutation is performed by returning a new
reference, like in Linear Haskell without linear constraints.

\paragraph{Languages with capabilities}

The idea of using capabilities to enforce high-level resource usage protocols is not new~\citep{DBLP:conf/pldi/DeLineF01},
and as such has been applied in practical programming languages before.
Both Mezzo~\cite{mezzo-permissions} and
\textsc{ats}~\cite{AtsLinearViews} served as inspiration for the
design of linear constraints. Of the two, Mezzo is more specialised,
being entirely built around its system of capabilities.  \textsc{Ats}
is the closest to our system because it appeals explicitly to linear
logic, and because the capabilities (known as \emph{stateful views})
are not tied to a particular use case. However,
\textsc{ats} does not have full inference of capabilities.

Other than that, the two systems have a lot of similarities. They have a
finer-grained capability system than is expressible in Rust (or our
encoding of it in \cref{sec:memory-ownership}) which makes it possible to change
the type of a reference cell upon write (though linear constraints could be used to implement such type-changing references too). They also eschew scoped
borrowing in favour of more traditional read and write capabilities.
% In exchange, neither Mezzo nor \textsc{ats} support $O(1)$ freezing like in
% \cref{sec:memory-ownership}.
% Mezzo, being geared towards functional programming, does support
% freezing, but freezing a nested data structure requires traversing it.
% As far as we know, \textsc{ats} doesn't support
% freezing. \textsc{Ats} is more oriented towards system programming.

%if False
We chose an example in the style of rust for
\cref{sec:memory-ownership} because freezing arrays of arrays
in $O(1)$ was one of the initial motivations of this article. However,
a Mezzo or \textsc{ats} style of capabilities could certainly be
encoded within linear constraints.
%endif

Linear constraints are more general than either Mezzo or \textsc{ats},
while maintaining a considerably simpler inference algorithm, and at
the same time supporting a richer set of constraints (such as \textsc{gadt}s). This
simplicity is a benefit of abstracting over the simple-constraint
domain. In fact, it should be possible to see Mezzo or \textsc{ats} as
particular instantiations of the simple-constraint domain, with linear
constraints providing the general inference mechanism.

% However, both Mezzo and \textsc{ats} have an advantage that we do not:
% they assume that their instructions are properly sequenced, whereas
% basing linear constraints on Haskell, a lazy language, we are forced
% to make sequencing explicit in \textsc{api}s.

\paragraph{Linearly typed languages}

Affe~\cite{kindly-bent} is a linearly typed \textsc{ml}-style core
language with mutable references and arrays, augmented with a notion
of borrowing. It has dedicated syntax for the scope of borrows. In
contrast, we represent scopes as functions. Affe is presented as a
fully integrated solution, while linear constraints is a small layer
on top of Linear Haskell.

\paragraph{Logic programming}

There are a lot of commonalities between \textsc{ghc}'s constraint and logic
programs. Traditional type classes can be seen as Horn clause programs, much
like Prolog programs. \textsc{ghc} puts further restrictions in order to
avoid backtracking for speed and predictability.

The recent addition of quantified
constraints~\cite{quantified-constraints} extends type class
resolution to Hereditary Harrop~\cite{hereditary-harrop} programs. A generalisation of the
Hereditary Harrop fragment to linear logic, described by~\citet{hh-ll},
is the foundation of the Lolli language~\cite{hodas-thesis-lolli}.
The authors also coin the notion of \emph{uniform} proof. A fragment where
uniform proofs are complete supports goal-oriented proof search, like
Prolog does.

Completeness of uniform proofs is equivalent to
\cref{lem:inversion}, which, in turn, is used in the proof of the
soundness \cref{lem:generation-soundness}. Therefore our linear
constraints are compatible with quantified constraints: we simply need
to adapt~\cref{lem:inversion}.

It is interesting that goal-oriented search is baked into the
definition of OutsideIn. It's not only used as the constraint solving
strategy, but it seems to required for the soundness of the constraint
generation algorithm. Or, if they are not required, uniform proofs are
at least an effective strategy to prove soundness.

% \paragraph{Resource usage analysis}

% \Citet{resource-usage-analysis} introduce a framework which can be
% instantiated into many resource analyses (such as proper deallocation
% of resources). In particular they give a
% decision procedure for a wide class of such analyses. Although our
% objects of study intersect, our purpose is quite different as theirs
% is a tool for language designer to design analyses, while ours is for
% programmers to implement new abstractions as libraries.

\section{Conclusion}
\label{sec:conclusion}

We showed how a simple linear type system like that of Linear
Haskell can be extended with an inference mechanism which lets the
compiler manage some of the additional complexity of linear types
instead of the programmer. Linear constraints narrow the gap between linearly
typed languages and dedicated linear-like typing disciplines such as Rust's,
Mezzo's, or \textsc{ats}'s.

% We also demonstrate how an existing constraint solver can be extended to handle
% linearity. Our design of
% linear constraints fits nicely into Haskell. Indeed, linear constraints can be
% thought of as an extension of Haskell's type class mechanism. This way, the
% design also integrates well into \textsc{ghc}, as demonstrated by our prototype
% implementation, which required modest changes to the compiler. Remarkably, all we needed to do was to
% adapt the work of \citet{resource-management-for-ll-proof-search} to the OutsideIn
% framework. It is also quite serendipitous that the notion of uniform
% proof from \citet{hh-ll}, which was introduced to prove the
% completeness of a proof search strategy, ends up being crucial to the
% soundness of constraint generation.

% The memory management \textsc{api} of \cref{sec:memory-ownership} shows how
% Rust's memory ownership model can be implemented as a library using our
% framework. This would not be practical without linear constraints. Certainly,
% ownership proofs could be managed manually, but it is hard to imagine a
% circumstance where this tedious task would be worth the cost.

% This, really, is the philosophy of linear constraints: lowering
% the cost of linear types so that more theoretical applications become
% practical applications. And we achieved this at a surprisingly low price: teaching
% linear logic to \textsc{ghc}'s constraint solver.

\begin{acks}
  Jean-Philippe Bernardy is supported by grant
  \grantnum{clasp}{2014-39} from the \grantsponsor{clasp}{Swedish
    Research Council}{https://www.vr.se}, which funds the Centre for
  Linguistic Theory and Studies in Probability (CLASP) in the
  Department of Philosophy, Linguistics, and Theory of Science at the
  University of Gothenburg.
%
  Nicolas Wu is supported by
  \grantsponsor{EPSRC}{EPSRC}{https://www.ukri.org/councils/epsrc/}
  Grant \grantnum{EPSRC}{EP/S028129/1}.


\end{acks}

% For natbib:
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{bibliography}
% For biblatex:
\printbibliography
\newpage

\appendix

\section{Full descriptions}
\label{sec:appendix:full-descriptions}

In this appendix, we give, for reference, complete descriptions of the
type systems, functions, etc. that we have abbreviated in the main % etc should never be followed by '…'
body of the article.

\subsection{Core calculus}
\label{sec:appendix:core-calculus}

This is the complete version of the core calculus described in
\cref{sec:core-calculus}. The full grammar is given by
\Cref{fig:full:core-grammar} and the type system by
\Cref{fig:full:core-typing-rules}.

\begin{figure}
  \maybesmall
  \centering
  $$
  \begin{array}{lcll}
    [[{a}]], [[{b}]] & \bnfeq & \ldots & \text{Type variables} \\
    [[{x}]], [[{y}]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[{K}]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[{s}]] & \bnfeq & [[{forall as. t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & [[{a}]] \bnfor [[{exists as. t o- u}]] \bnfor [[{t1 ->_pi t2}]]
                            \bnfor [[{T ts}]] & \text{Types} \\
    [[{G}]], [[{D}]] & \bnfeq & [[{empty}]] \bnfor [[{G, x:_pi s}]] &
                                                              \text{Contexts} \\
    [[{e}]] & \bnfeq & [[{x}]] \bnfor [[{K}]] \bnfor [[{\x. e}]] \bnfor [[e1
                     e2]] \bnfor [[{pack (e1, e2)}]] & \text{Expressions}\\
                 &\bnfor & [[unpack (y,x)=e1 in
                           e2]] \bnfor [[{case_pi e of { alts }}]] &\\
                 &\bnfor & [[let_pi
                           x=e1 in e2]] \bnfor [[{let_pi x : s = e1 in e2}]] &
  \end{array}
  $$
  \caption{Grammar of the core calculus}
  \label{fig:full:core-grammar}
\end{figure}

\begin{figure}
  \maybesmall
  \centering
  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Var,Abs,App,Pack,Unpack,Let,Case}
  \caption{Core calculus type system}
  \label{fig:full:core-typing-rules}
\end{figure}

\subsection{Desugaring}
\label{sec:appendix:desugaring}

The complete definition of the desugaring function from
\cref{sec:desugaring} can be found in
\Cref{fig:full:desugaring}.

For the sake of concision, we allow ourselves to write nested patterns
in $\kcase$ expressions of the core language. Desugaring nested patterns
into atomic $\kcase$ expression is routine.

In the complete description, we use a device which was omitted in the
main body of the article. Namely, we'll need a way to turn every
$[[{Ev(omega.Q)}]]$ into an $[[{Ur(Ev(Q))}]]$. For any
$[[e : Ev(omega.Q)]]$, we shall write $[[urify(Q;e) :
Ur(Ev(omega.Q))]]$. As a shorthand, particularly useful in nested
patterns, we will write $[[{case_pi e of {urified(Q;x) -> e'}}]]$ for
$[[{case_pi urify(Q;e) of {Ur x -> e'}}]]$.
%
$$
\left\{
  \begin{array}{lcl}
    [[{urify(Empty;e)}]]& = & [[{case_1 e of {() -> Ur ()}}]] \\
    [[{urify(1.q;e)}]] & = & [[{e}]] \\
    [[{urify(omega.q;e)}]] & = & [[{case_1 e of {Ur x -> Ur (Ur x)}}]] \\
    [[{urify(Q1*Q2;e)}]] & = & [[{case_1 e of {(urified(Q1;x), urified(Q2;y)) -> Ur (x,y)}}]]
  \end{array}
\right.
$$
We will omit the $[[{Q}]]$ in $[[{urify(Q;e)}]]$ and write
$[[{urify(e)}]]$ when it can be easily inferred from the context.


\begin{figure}
  \small
  \centering

%{
%format Ds(z)(j) = "\dsterm{\ottmv{" z "}}{" j "}"
%format * = "[[*]]"
%format ** = "\mathbin{⋅}"
%format ->. = "\to_{1}"
%format ->> = "\to_{\pi}"
%format ->>> = "\to_{\omega}"
%format Ev(x) = "\dsevidence{" x "}"
%format G = "[[{G}]]"
%format G1 = "[[{G1}]]"
%format G2 = "[[{G2}]]"
%format ListOf (b) = "\overline{" b "}"
%format Of (w) (a) (b) = a"\mathop{:_{" w "}}"b
%format OneOf (a) (b) = a"\mathop{:_{1}}"b
%format Q = "[[{Q}]]"
%format Q1 = "[[{Q1}]]"
%format Q2 = "[[{Q2}]]"
%format Sub t as bs = t "[" as "/" bs "]"
%format alts = "[[alts]]"
%format as = "[[{as}]]"
%format case_1 = "[[case_1]]"
%format case_omega = "[[case_omega]]"
%format e1 = "[[{e1}]]"
%format e2 = "[[{e2}]]"
%format ei = "[[{ei}]]"
%format let_1 = "[[let_1]]"
%format let_omega = "[[let_omega]]"
%format omega = "[[{omega}]]"
%format pi = "[[{pi}]]"
%format pii = "[[{pii}]]"
%format t = "[[{t}]]"
%format t1 = "[[{t1}]]"
%format t2 = "[[{t2}]]"
%format ts = "[[{ts}]]"
%format u = "[[{u}]]"
%format ui = "[[{ui}]]"
%format unpack(x) = "\klet\ \packbox " x
%format urified(x) = "\underline{" x "}"
%format us = "[[{us}]]"
%format xi = "[[{xi}]]"
%format xsi = "[[{xsi}]]"
%format z1 = "[[{z1}]]"
%format z2 = "[[{z2}]]"
%format z2' = "[[{z2}]]''"
%format |- = "[[|-]]"
%format ||- = "[[||-]]"
$$
\left\{\;
\begin{minipage}{0.8\linewidth}
\begin{code}
Ds(z)(Q;G |- x : u[ts/as])  =
    x z
Ds(z)(Q;G |- \x.e : t1 ->> t2) =
  \x. Ds(z)(Q;G, Of pi x t1 |- e : t2)
Ds(z)(Q1*Q2; G1+G2 |- e1 e2 : t) =
  case_1 z of { (z1, z2) ->
    (Ds(z1)(Q1;G1 |- e1 : t1 ->. t)) (Ds(z2)(Q2;G2 |- e2 : t1)) }
Ds(z)(Q1* omega ** Q2; G1+ omega ** G2 |- e1 e2 : t) =
  case_1 z of { (z1, urified(z2)) ->
    (Ds(z1)(Q1;G1 |- e1 : t1 ->>> t)) (Ds(z2)(Q2;G2 |- e2 : t1)) }
Ds(z)(Q * Sub Q1 us as;G |- pack e : exists as. t .<= Q1) =
  case_1 z of { (z', z'') ->
    pack (z'', Ds(z')( Q ; G |- e : Sub t us as))}
Ds(z)(Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    unpack (z',x) = Ds(z1)(Q1;G1 |- e1 : exists as. t1 .<= Q) in
    let_1 z2' = (z2,z') in
    Ds(z2')(Q2 * Q;G2, OneOf x t1 |- e2 : t)}
Ds(z)(Q1 * Q2 ;G1+G2 |- let_1 x = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    let_1 x : Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1)
    in Ds(z2)(Q2;G2, OneOf x t1 |- e2 : t)}
Ds(z)(omega ** Q1 * Q2 ;omega ** G1+G2 |- let_omega x = e1 in e2 : t) =
  case_1 z of { (urified(z1), z2) ->
    let_omega x : Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1) in
    Ds(z2)(Q2;G2, Of omega x t1 |- e2 : t)}
Ds(z)(Q1 * Q2 ;G1+G2 |- let_1 x : forall as. Q =>. t1 = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    let_1 x : forall as. Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1) in
    Ds(z2)(Q2;G2, OneOf x (forall as. Q =>. t1) |- e2 : t)}
Ds(z)(omega ** Q1 * Q2 ;omega ** G1+G2 |- let_omega x : forall as. Q =>. t1 = e1 in e2 : t) =
  case_1 z of { (urified(z1), z2) ->
    let_omega x : forall as. Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1) in
    Ds(z2)(Q2;G2, Of omega x t1 |- e2 : t)}
Ds(z)(omega ** Q1*Q2;omega ** G1+G2 |- case_1 e of { alts } : t)  =
  case_1 z of { (urified(z1), z2) ->
    case_1 (Ds(z1)(Q1;G1 |- e : T ts)) of
      { ListOf(K xsi -> Ds(z2)( Q2; G2, ListOf(Of ((pi ** pii)) xi (Sub ui ts as)) |- ei : t))}}
Ds(z)(Q1*Q2;G1+G2 |- case_omega e of { alts } : t)  =
  case_1 z of { (z1, z2) ->
    case_omega (Ds(z1)(Q1;G1 |- e : T ts)) of
      { ListOf(K xsi -> Ds(z2)( Q2; G2, ListOf(Of ((pi**pii)) xi (Sub ui ts as)) |- ei : t))}}
\end{code}
\end{minipage}
\right.
$$

%}

  \caption{Desugaring}
  \label{fig:full:desugaring}
\end{figure}

\section{Proofs}
\label{sec:appendix:proofs-lemmas}

\setcounter{subsection}{4}
\subsection{Lemmas on the qualified type system}
\label{sec:appendix:qual-type-syst}

\begin{proof}[Proof of \cref{lem:q:scaling}]
  Let us prove separately the cases $[[{pi}]]=[[{1}]]$ and
  $[[{pi}]]=[[{omega}]]$.
  \begin{itemize}
  \item When $[[{pi}]]=[[{1}]]$, then $[[{pi.Q}]]=[[{Q}]]$ for all
    $[[{Q}]]$, hence $[[Q1||-Q2]]$ implies $[[pi.Q1 ||- pi.Q2]]$.
  \item For the case $[[{pi}]]=[[{omega}]]$, let us consider a few
    properties. First note that, for any $[[{Q}]]$,
    $[[{omega.Q}]]=[[{omega.Q * omega.Q}]]$. From which it follows,
    using the laws of \cref{def:entailment-relation}, that
    $[[omega.Q ||- Q1 * Q2]]$ if and only if $[[omega.Q ||- Q1]]$ and
    $[[omega.Q ||- Q2]]$.

    This means that to verify that
    $[[omega.Q1 ||- omega.Q2]]$, it is equivalent to prove that $[[omega.Q1
    ||- omega.q2]]$ for each
    $[[{q2}]]∈[[{UCtx}]]$ (letting $[[{omega.Q2}]]=[[{(UCtx,
      emptyset)}]]$). In turn, by \cref{def:entailment-relation} and
    observing that $[[{omega.(omega.Q1)}]] = [[{Q1}]]$, this is
    equivalent to $[[omega.Q1 ||- 1.q2]]$.

    This follows from the fact that $[[Q1 ||- Q2]]$ implies
    $[[omega.Q1 ||- Q2]]$ (\cref{def:entailment-relation}) and the
    property, shown above, that $[[omega.Q1 ||- Q2 * Q2']]$ if and
    only if $[[omega.Q1 ||- Q2]]$ and $[[omega.Q1 ||- Q2']]$.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{lem:q:scaling-inversion}]
  Let us prove separately the cases $[[{pi}]]=[[{1}]]$ and
  $[[{pi}]]=[[{omega}]]$.
  \begin{itemize}
  \item When $[[{pi}]]=[[{1}]]$, then $[[{pi.Q}]]=[[{Q}]]$ for all
    $[[{Q}]]$, in particular $[[Q1||- 1.Q2]]$ implies that
    $[[{Q1}]]=[[{1.Q1}]]$ with $[[Q1 ||- Q2]]$.
  \item When $[[{pi}]]=[[{omega}]]$, then let us first remark, letting
    $[[{omega.Q2}]]=[[{(UCtx, emptyset)}]]$ that, by a straightforward
    induction on the cardinality of $[[{UCtx}]]$ it is sufficient to
    prove that the result holds for atomic constraints.

    That is, we need to prove that if $[[Q1 ||- omega.q2]]$ then
    there exists $[[{Q'}]]$ such that $[[{Q1}]]=[[{omega.Q'}]]$ and
    $[[Q' ||- rho.q2]]$ (for all $[[{rho}]]$).

    This result, in turns, holds by \cref{def:entailment-relation}.
  \end{itemize}
\end{proof}

\begin{lemma}\label{lem:simples:monoid-action}
  The following equality holds $[[{pi.(rho.Q)}]]=[[{(pi.rho).Q}]]$
\end{lemma}
\begin{proof}
  Immediate by case analysis of $[[{pi}]]$ and $[[{rho}]]$.
\end{proof}

\setcounter{subsection}{5}
\subsection{Lemmas on constraint inference}
\label{sec:appendix:constraint-inference}

\begin{lemma}[$[[{Dup}]]$ discarding]
  \label{lem:dup-weakening}
  The two following, equivalent, properties hold
  \begin{itemize}
  \item if $[[Qdup \in Dup]]$, then $[[Qdup ||- Empty]]$
  \item if $[[Q1 ||- Q2]]$ and $[[Qdup \in Dup]]$, then $[[Q1 * Qdup ||- Q2]]$
  \end{itemize}
\end{lemma}
\begin{proof}
  The two properties are equivalent
  \begin{itemize}
  \item using $[[{C1}]]=[[{C2}]]=[[{Empty}]]$, the latter implies the
    former
  \item The former implies the latter by tensoring together
    $[[Q1 ||- Q2]]$ and $[[Qdup ||- Empty]]$ following the rules
    of~\cref{fig:entailment-relation}.
  \end{itemize}
  Let $[[Qdup \in Dup]]$, then for each $[[{1.q}]] \in [[{Qdup}]]$,
  $[[1.q ||- Empty]]$ (per~\cref{fig:entailment-relation}), tensoring
  each of these entailments together and with the $[[{omega.q}]] \in [[{Qdup}]]$, we get $[[Qdup ||- Empty]]$.
\end{proof}

\begin{lemma}[$[[{Dup}]]$ duplication]
  \label{lem:dup-contraction}
  The two following, equivalent, properties hold
  \begin{itemize}
  \item if $[[Qdup \in Dup]]$, then $[[Qdup ||- Qdup * Qdup]]$
  \item if $[[Q1 * Qdup ||- Q2]]$, $[[Qdup * Q1' ||- Q2']]$, and
    $[[Qdup \in Dup]]$, then $[[Q1 * Qdup * Q1'||- Q2 * Q2']]$
  \end{itemize}
\end{lemma}
\begin{proof}
  The proof is similar to that of~\cref{lem:dup-weakening}
\end{proof}

\begin{lemma}[Transitive tensor decomposition]
  \label{lem:transitive-tensor-decomposition}
  if $[[Qdup \in Dup]]$ and $[[Q ||- Q1 * Qdup * Q2]]$, then there
  exists $[[Q1']]$, $[[Qdup']]$, $[[Q2']]$, such that
  \begin{itemize}
  \item $[[Q1' * Qdup' ||- Q1]]$
  \item $[[Qdup' * Q2' ||- Q2]]$
  \item $[[Qdup' ||- Qdup]]$
  % \item $[[{Q}]] = [[{Q1' * Qdup' * Q2'}]]$
  \end{itemize}
\end{lemma}
\begin{proof}
  By the inversion-of-tensor rule from~\cref{fig:entailment-relation},
  we get that ther exists $[[{Q'}]]$, $[[{Qdup'}]]$, and $[[{Q2'}]]$,
  such that
  \begin{itemize}
  \item $[[{Q}]] = [[{Q' * Qdup' * Q2'}]]$
  \item $[[Qdup' * Q2' ||- Q2]]$
  \item $[[Q' * Qdup' ||- Q1 * Qdup]]$. The inversion-of-tensor rule
    applies further to this case: there exists $[[{Q1'}]]$,
    $[[{Qdup''}]]$, and $[[{Q''}]]$ such that
    \begin{itemize}
    \item $[[{Q' * Qdup'}]] = [[{Q1' * Qdup'' * Q''}]]$
    \item $[[Q1' * Qdup'' ||- Q1']]$
    \item $[[Qdup'' * Q'' ||- Qdup']]$
    \end{itemize}
  \end{itemize}
  Observe the following
  \begin{itemize}
  \item $[[Qdup'' * Q'' \in Dup]]$ (because of the requirements
    of~\cref{fig:entailment-relation}). Let's write
    $[[{Qdup'''}]] = [[{Qdup'' * Q''}]]$.
  \item $[[{Q}]] = [[{Q1' * Qdup''' * Q2'}]]$
  \item We have
    \begin{itemize}
    \item $[[Q1' * Qdup''' ||- Q1]]$. Because
      \begin{itemize}
      \item $[[{Q1' * Qdup'''}]] = [[{Q' * Qdup'}]]$
      \item therefore $[[Q1' * Qdup''' ||- Q1 * Qdup]]$
      \item by~\cref{lem:dup-weakening}, and by transitivity of the
        entailment relation (per~\cref{fig:entailment-relation}), we
        can drop $[[{Qdup}]]$ from the conclusion.
      \end{itemize}
    \item $[[Qdup''' ||- Qdup']]$ (by definition of $[[{Qdup'''}]]$)
    \item $[[Qdup''' * Q2' ||- Q2]]$
      \begin{itemize}
      \item By tensoring together, per~\cref{fig:entailment-relation},
        $[[Qdup''' ||- Qdup']]$ and $[[Q2' ||- Q2']]$, we get
        $[[Qdup''' * Q2' ||- Qdup' * Q2']]$
      \item then we get the desired result by transitivity of the
        entailment relation.
      \end{itemize}
    \end{itemize}
  \end{itemize}

  This concludes the proof
\end{proof}


\begin{proof}[Proof of \cref{lem:inversion}]
  The cases $[[Q |- C1 & C2]]$ and $[[Q |- pi.(Q2 => C)]]$ are
  straightforward by induction, so let us prove them first
  \begin{itemize}
  \item Suppose $[[Q |- C1 & C2]]$, then there are two cases
    \begin{itemize}
    \item either it is the conclusion of a \rref*{C-With} rule,
      and the result is immediate.
    \item or it is the result of a \rref*{C-Dom} rule, then, there
      exists $[[{Q'}]]$, such that $[[Q ||- Q']]$ and
      $[[Q' |- C1 & C2]]$.

      By induction $[[Q' |- C2]]$ and $[[Q' |- C2]]$, applying
      \rref*{C-Dom} to both gives $[[Q |- C2]]$ and $[[Q |- C2]]$ as
      required.
    \end{itemize}

  \item Suppose $[[Q |- pi.(Q2 => C)]]$, then there are two cases
    \begin{itemize}
    \item either it is the conclusion of a \rref*{C-Impl} rule,
      and the result is immediate.
    \item or it is the result of a \rref*{C-Dom} rule, then, there
      exists $[[{Q'}]]$, such that $[[Q ||- Q']]$ and
      $[[Q' |- pi.(Q2 => C)]]$.

      By induction, there exists $[[{Q1'}]]$ such that $[[Q1' * Q2 |-
      C]]$ and $[[{Q'}]] = [[{pi.Q1'}]]$,
      by~\cref{fig:entailment-relation}, there exists $[[{Q1}]]$ such
      that $[[{Q}]] = [[{pi.Q1}]]$ and $[[Q1 ||- Q1']]$. Hence $[[Q1 *
      Q2 ||- Q1' * Q2]]$, which lets us conclude with \rref*{C-Dom}.
    \end{itemize}
  \end{itemize}

  For $[[Q |- C1 * C2]]$ we have the following cases:
  \begin{itemize}
  \item either it is the conclusion of a \rref*{C-Tensor} rule, and
    the result is immediate
  \item or it is the result of a \rref*{C-Id} rule, in which case
    $[[{Q}]] = [[{C1 * C2}]]$, which proves the result
  \item or it is the result of a \rref*{C-Dom} rule, in which case
    there is $[[{Q'}]]$ such that $[[Q ||- Q']]$ and $[[Q' |- C1 * C2]]$.

    By induction, there exist $[[{Q1'}]]$, $[[{Qdup'}]]$, and
    $[[{Q2'}]]$, such that $[[Qdup' \in Dup]]$, $[[Q1' * Qdup' |- C1]]$,
    $[[Qdup' * Q2' |- C2]]$, and $[[{Q'}]] = [[{Q1' * Qdup' * Q2'}]]$.

    Then~\cref{lem:transitive-tensor-decomposition}, gives us
    $[[{Q1}]]$, $[[{Qdup}]]$, and $[[{Q2}]]$ such that
    \begin{itemize}
    \item $[[{Q}]] = [[{Q1* Qdup * Q2}]]$
    \item $[[Qdup \in Dup]]$
    \item $[[Q1 * Qdup ||- Q1']]$
    \item $[[Qdup * Q2 ||- Q2']]$
    \item $[[Qdup ||- Qdup']]$
    \end{itemize}
    By~\ref{lem:dup-contraction}, we can further deduce that
    \begin{itemize}
    \item $[[Q1 * Qdup ||- Q1' * Qdup']]$
    \item $[[Qdup * Q2 ||- Qdup' * Q2']]$
    \end{itemize}
    Which concludes the proof, by the \rref*{C-Dom} rule
  \end{itemize}
\end{proof}


\begin{proof}[Proof of \cref{lem:wanted:promote}]
  By induction on the syntax of $[[{C}]]$
  \begin{itemize}
  \item If $[[{C}]]=[[{Q'}]]$, then the result follows
    from \cref{lem:q:scaling}
  \item If $[[{C}]]=[[{C1*C2}]]$, then we can prove the result like we
    proved the corresponding case in \cref{lem:q:scaling},
    using \cref{lem:inversion}.
  \item If $[[{C}]]=[[{C1&C2}]]$, then we the case where $[[{pi}]]=[[{1}]]$ is
    immediate, so we can assume without loss of generality that
    $[[{pi}]]=[[{omega}]]$, and, therefore, that $[[{pi.C}]] = [[pi.C1 *
    pi.C2]]$.
    By \cref{lem:inversion}, we have that $[[Q|-C1]]$ and
    $[[Q|-C2]]$; hence, by induction, $[[omega.Q |- omega.C1]]$ and
    $[[omega.Q |- omega.C1]]$.
    Then, by definition of the entailment relation, we have $[[omega.Q
    * omega.Q |- omega.C1 * omega.C2]]$, which concludes,
    since $[[{omega.Q}]] = [[{omega.Q * omega.Q}]]$.
  \item If $[[{C}]]=[[{rho.(Q1 => C')}]]$, then by
    \cref{lem:inversion}, there is a $[[{Q'}]]$ such that
    $[[{Q}]]=[[{pi.Q'}]]$ and $[[Q'*Q1 |- C']]$. Applying
    rule~\rref*{C-Impl} with $[[{pi.rho}]]$, we get
    $[[(pi.rho).Q' |- (pi.rho).(Q1 => C')]]$.

    In other words: $[[pi.Q |- pi.(rho.(Q=>C))]]$ as expected.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{lem:wanted:demote}]
  By induction on the syntax of $[[{C}]]$
  \begin{itemize}
  \item If $[[{C}]]=[[{Q'}]]$, then the result follows from
    \cref{lem:q:scaling-inversion}
  \item If $[[{C}]]=[[{C1*C2}]]$, then we can prove the result like we
    proved the corresponding case in
    \cref{lem:q:scaling-inversion} using
    \cref{lem:inversion}.
  \item If $[[{C}]]=[[{C1&C2}]]$, then we the case where $[[{pi}]]=[[{1}]]$ is
    immediate, so we can assume without loss of generality that
    $[[{pi}]]=[[{omega}]]$, and, therefore, that
    $[[{pi.C}]] = [[{pi.C1 * pi.C2}]]$. By \cref{lem:inversion},
    there exist $[[{Q1}]]$ and $[[{Q2}]]$ such that $[[Q1|- omega.C1]]$,
    $[[Q2|- omega.C2]]$ and $[[{Q}]]=[[{Q1 * Q2}]]$. By induction
    hypothesis, we get $[[{Q1}]] = [[{omega.Q1'}]]$ and $[[{Q2}]] = [[{omega.Q2'}]]$
    such that $[[Q1' |- C1]]$ and $[[Q2' |- C2]]$. From which it
    follows that $[[omega.Q1'*omega.Q2' |- C1]]$ and
    $[[omega.Q1'*omega.Q2' |- C1]]$ (by
    \cref{lem:wanteds:weakening}) and, finally,
    $[[{Q}]]=[[{omega.Q}]]$ (by \cref{lem:wanteds:module-action})
    and $[[Q |- C1 & C2]]$.
  \item If $[[{C}]]=[[{rho.(Q1 => C')}]]$, then
    $[[{pi.C}]] = [[{(pi.rho). (Q1 => C')}]]$. The result follows
    immediately by \cref{lem:inversion}.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{lem:generation-soundness}]
  By induction on $[[G |-> e : t ~> C]]$
  \begin{description}
  \item[\rref*{G-Var}] We have
    \begin{itemize}
    \item $[[G1 = x :_1 forall as. Q =o u]]$
    \item $[[G1 + omega.G2 |-> x : u[ts/as] ~> Q[ts/as] ]]$
    \item $[[Q_g |- Q[ts/as] ]]$
    \end{itemize}
    Therefore, by rules~\rref*{E-Var} and~\rref*{E-Sub}, it follows
    immediately that $[[Q_g ; G1 + omega.G2 |- x : u[ts/as] ]]$
  \item[\rref*{G-Abs}] We have
    \begin{itemize}
    \item $[[G |-> \x. e : t0 ->_pi t ~> C]]$
    \item $[[Q_g |- C]]$
    \item $[[G, x:_pi t0 |-> e : t ~> C]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_g; G, x:_pi t0 |- e : t]]$
    \end{itemize}
    From which follows that $[[Q_g; G |- \x. e : t0 ->_pi t]]$.
  \item[\rref*{G-Let}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x = e1 in e2 : t ~> pi.C1 * C2]]$
    \item $[[Q_g |- pi.C1 * C2]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \end{itemize}
    By \cref{lem:inversion,lem:wanted:demote}, there exist $[[{Q1}]]$,
    $[[{Qdup}]]$ and $[[{Q_2}]]$ such that\info{Here I used,
      implicitly (and without proof), the easy fact that if there
      exists $[[{Q'}]]$ such that $[[{Q}]]=[[{pi.Q'}]]$, then
      $[[{Q}]]=[[{pi.Q}]]$ (because $[[{pi.pi}]] = [[{pi}]]$) and $[[Q
      ||- Q']]$ (identity for $[[{pi}]]=[[{1}]]$ and dereliction for $[[{pi}]]=[[{omega}]]$).}
    \begin{itemize}
    \item $[[Q_1 * Qdup |- C1]]$
    \item $[[Qdup * Q_2 |- C2]]$
    \item $[[{Q_g}]] = [[{pi.Q_1 * Qdup * Q_2}]]$
    \item $[[Qdup \in Dup]]$
    \item $[[{pi.Qdup}]] = [[{Qdup}]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_1 * Qdup; G1 |- e1 : t1]]$
    \item $[[Qdup * Q_2; G2, x:_pi  t1 |- e1 : t1]]$
    \end{itemize}
    From which follows that $[[Q_g; pi.G1+G2 |- let_pi x = e1 in e2 :
    t]]$.
  \item[\rref*{G-LetSig}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x : forall as. Q =o t1 = e1 in e2 : t ~>
      C2 * pi.(Q => C1)]]$
    \item $[[Q_g |- C2 * pi.(Q => C1)]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \item $[[G2, x:_pi forall as. Q =o t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By \cref{lem:inversion,lem:wanted:demote}, there exist $[[{Q1}]]$,
    $[[Qdup]]$, $[[{Q2}]]$ such
    that
    \begin{itemize}
    \item $[[Qdup * Q2 |- C2]]$
    \item $[[Q1*Qdup*Q |- C]]$
    \item $[[{Q_g}]] = [[{pi.Q_1 * Qdup * Q_2}]]$
    \item $[[Qdup \in Dup]]$
    \item $[[{pi.Qdup}]] = [[{Qdup}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1*Qdup*Q;G1 |- e1 : t1]]$
    \item $[[Qdup*Q2; G2, x:_pi forall as. Q =o t1 |- e2 : t]]$
    \end{itemize}
    Hence $[[Q_g; pi.G1+G2 |- let_pi x : forall as. Q =o t1 = e1 in e2 : t]]$
  \item[\rref*{G-App}] \info{Most of the linearity problems are in the App
      rule. Unpack is also relevant.}
    We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By \cref{lem:inversion,lem:wanted:demote}, there exist
    $[[{Q1}]]$, $[[{Qdup}]]$, $[[{Q2}]]$ such that
    \begin{itemize}
    \item $[[Q1*Qdup |- C1]]$
    \item $[[Qdup*Q2 |- C2]]$
    \item $[[{Q_g}]] = [[{Q1 * Qdup * pi.Q2}]]$
    \item $[[Qdup \in Dup]]$
    \item $[[{pi.Qdup}]] = [[{Qdup}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1*Qdup; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Qdup* Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[\rref*{G-Pack}] We have
    \begin{itemize}
    \item $[[G |-> pack e : exists as. t o= Q ~> C * Q[us/as] ]]$
    \item $[[Q_g |- C * Q[us/as] ]]$
    \item $[[G |-> e : t[us/as] ~> C]]$
    \end{itemize}
    By \cref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Qdup}]]$, $[[{Q_2}]]$
    such that
    \begin{itemize}
    \item $[[Q_1 * Qdup |- C]]$
    \item $[[Qdup * Q_2 |- Q[us/as] ]]$
    \item $[[{Q_g}]] = [[{Q_1*Qdup*Q_2}]]$
    \item $[[Qdup \in Dup]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1 * Qdup; G |- e : t[us/as] ]]$
    \end{itemize}
    So we have $[[Q_1 * Qdup * Q[us/as] ; G |- pack e : exists as. t o=
    Q]]$. By~\cref{lem:dup-contraction} rule~\rref*{E-Sub}, we conclude
    $[[Q_g ; omega.G |- pack e : exists as. t o= Q]]$.
  \item[\rref*{G-Unpack}] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * 1.(Q' => C2)]]$
    \item $[[Q_g |- C1 * 1.(Q' => C2)]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By \cref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Qdup}]]$, $[[{Q_2}]]$
    such that
    \begin{itemize}
    \item $[[Q_1 * Qdup |- C1]]$
    \item $[[Qdup * Q_2 * Q' |- C2]]$
    \item $[[{Q_g}]] = [[{Q1 * Qdup * Q2}]]$
    \item $[[Qdup \in Dup]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1*Qdup; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Qdup*Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \item[\rref*{G-Case}] We have
    \begin{itemize}
    \item $[[pi.G + D |-> case_pi e of {alts} : t ~> pi.C * && Ci]]$
    \item $[[Q_g |- pi.C * && Ci]]$
    \item $[[G |-> e : T ss ~> C]]$
    \item For each $i$, $[[D, <xi:_(pi.pii) ui[ss/as]> |-> ei : t ~> Ci]]$
    \end{itemize}
    By repeated uses of \cref{lem:inversion} as well as \cref{lem:wanted:demote}, there exist
    $[[{Q}]]$, $[[{Qdup}]]$, $[[{Q'}]]$ such that
    \begin{itemize}
    \item $[[Q * Qdup |- C]]$
    \item For each $i$, $[[Qdup * Q' |- Ci]]$
    \item $[[{Q_g}]] = [[{pi.Q * Qdup * Q'}]]$
    \item $[[Qdup \in Dup]]$
    \item $[[{pi.Qdup}]] = [[{Qdup}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q*Qdup; G |- e : T ss]]$
    \item For each $i$, $[[Qdup*Q';D, <xi:_(pi.pii) ui[ss/as]> |- ei : t]]$
    \end{itemize}
    Therefore $[[Q_g ; pi.G + D |- case_pi e of {alts} : t]]$.
  \end{description}
\end{proof}

\begin{proof}[Proof of \cref{lem:solver-soundness}]
  By induction on $[[UCtx; DCtx; LCtx_i |-s C ~> LCtx_o]]$
  \begin{description}
  \item[\rref*{S-Atom}] We have
  \begin{itemize}
          \item $[[UCtx ; DCtx ; LCtx_i |-s pi.q ~> LCtx_o]]$
          \item $[[UCtx ; DCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]$
  \end{itemize}
  By \cref{prop:atomic-solver-soundness} we have
  \begin{enumerate}
  \item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
  \item $[[(UCtx, DCtx \u LCtx_i) ||- pi.q * (emptyset, LCtx_o)]]$
  \end{enumerate}
  Then by \rref*{C-Dom} we have $[[(UCtx, LCtx_i) |- pi.q * (emptyset, LCtx_o)]]$.
  \item[\rref*{S-Add}] We have
  \begin{itemize}
          \item $[[UCtx ; DCtx ; LCtx_i |-s C1 & C2 ~> LCtx_o]]$
          \item $[[UCtx ; DCtx ; LCtx_i |-s C1 ~> LCtx_o]]$
          \item $[[UCtx ; DCtx ; LCtx_i |-s C2 ~> LCtx_o]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
          \item $[[(UCtx, DCtx \u LCtx_i) |- C1 * (emptyset, LCtx_o)]]$
          \item $[[(UCtx, DCtx \u LCtx_i) |- C2 * (emptyset, LCtx_o)]]$
  \end{itemize}
  Then by \rref*{C-With} we have $[[(UCtx, DCtx \u LCtx_i) |- C1 & C2 * (emptyset, LCtx_o)]]$.
  \item[\rref*{S-Mult}] We have
  \begin{itemize}
          \item $[[UCtx ; DCtx ; LCtx_i |-s C1 * C2 ~> LCtx_o]]$
          \item $[[UCtx ; DCtx ; LCtx_i |-s C1 ~> LCtx_o']]$
          \item $[[UCtx ; DCtx ; LCtx_o' |-s C2 ~> LCtx_o]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[{LCtx_o}]]\subseteq [[{LCtx_o'}]]$
          \item $[[{LCtx_o'}]]\subseteq [[{LCtx_i}]]$
          \item $[[(UCtx, DCtx \u LCtx_i) |- C1 * (emptyset, LCtx_o')]]$
          \item $[[(UCtx, DCtx \u LCtx_o') |- C2 * (emptyset, LCtx_o)]]$
  \end{itemize}
  Then
  \begin{itemize}
  \item by transitivity of $\subseteq$ we have
    $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$, and by \rref*{C-Tensor} we
    have
    $[[(UCtx, DCtx \u LCtx_i) * (UCtx , DCtx \u LCtx_o') |- C1 * C2 *
    (emptyset, LCtx_o') * (emptyset, LCtx_o)]]$
  \item by~\ref{lem:dup-contraction} and the definition of tensor on
    unrestricted constraints, we have
    $[[(UCtx, DCtx \u LCtx_i) * (emptyset , LCtx_o') |- C1 * C2 *
    (emptyset, LCtx_o') * (emptyset, LCtx_o)]]$
  \item by~\cref{lem:inversion} we have
    $[[(UCtx, LCtx_i) |- C1 * C2 * (emptyset, LCtx_o)]]$.
  \end{itemize}
  \item[\rref*{S-ImplOne}] We have
  \begin{itemize}
          \item $[[UCtx ; DCtx ; LCtx_i |-s 1.((UCtx0, LCtx0) => C) ~> LCtx_o]]$
          \item $[[UCtx \u UCtx0 ; DCtx \u (LCtx0 \n Dup) ; LCtx_i \u
            (LCtx0 \m Dup) |-s C ~> LCtx_o]]$
          \item $[[LCtx_o \subseteq LCtx_i]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[(UCtx \u UCtx0, DCtx \u LCtx_i \u LCtx0) |- C * (emptyset, LCtx_o)]]$
          \item $[[LCtx_o \subseteq LCtx_i \u LCtx0]]$
  \end{itemize}
  Then we know that
  $[[{(emptyset, LCtx_i)}]] = [[{(emptyset, LCtx_o) * (emptyset, LCtx_i')}]]$ for
  some $[[{LCtx_i'}]]$.
  Then by \cref{lem:inversion} we know that $[[(UCtx \u UCtx0, DCtx \u
  LCtx_i' \u LCtx0) |- C]]$ and by
  \rref*{C-Impl} we have $[[(UCtx, LCtx_i') |- 1.((UCtx0, LCtx0) => C)]]$.
  Finally, by \rref*{C-Tensor} we conclude that $[[(UCtx, LCtx_i) |- 1.((UCtx0, LCtx0) => C) * (emptyset, LCtx_o)]]$
  \item[\rref*{S-ImplMany}] We have
  \begin{itemize}
          \item $[[UCtx ; DCtx ; LCtx_i |-s omega.((UCtx0, LCtx0) => C) ~> LCtx_i]]$
          \item $[[UCtx \u UCtx0 ; LCtx0 \n Dup ; LCtx0 \m Dup |-s C ~> emptyset]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[(UCtx \u UCtx0, LCtx0) |- C]]$
  \end{itemize}
  Then by
  \rref*{C-Impl} $[[(UCtx, emptyset) |- omega.((UCtx0, LCtx0) => C)]]$ and finally by
  \rref{C-Tensor} we have $[[(UCtx, LCtx_i) |- omega.((UCtx0, LCtx0) => C) * (emptyset, LCtx_i)]]$.
  $[[LCtx_i \subseteq LCtx_i]]$ holds trivially.
  \end{description}
\end{proof}

\begin{lemma}[Weakening of wanteds]\label{lem:wanteds:weakening}
  If $[[Q |- C]]$, then $[[omega.Q'*Q |- C]]$
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the derivation of
  $[[Q |- C]]$, using the corresponding property on the
  simple-constraint entailment relation from
  \cref{def:entailment-relation}, for the \rref*{C-Dom} case.
\end{proof}

\begin{lemma}\label{lem:wanteds:module-action}
  The following equality holds: $[[{pi.(rho.C)}]]=[[{(pi.rho).C}]]$.
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the structure of
  $[[{C}]]$, using \cref{lem:simples:monoid-action} for the case
  $[[{C}]]=[[{Q}]]$.
\end{proof}
\end{document}

% Local Variables:
% ispell-dictionary: "british"
% End:


% LocalWords:  sequent typechecker idempotence polymorphism desugar
% LocalWords:  desugaring ghc OutsideIn quotiented gadt typeable
% LocalWords:  combinator sigils equalities wanteds intuitionistic
% LocalWords:  sequents implicational deallocate deallocating monadic
% LocalWords:  deallocated instantiations desugars unioned
% LocalWords:  deterministically unrestrictedly subexponentials
