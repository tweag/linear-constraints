% -*- latex -*-

%if style == newcode
module LinearConstraints where

\begin{code}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE ConstraintKinds #-}
{-# LANGUAGE RankNTypes #-}
{-# LANGUAGE TypeOperators #-}
{-# LANGUAGE KindSignatures #-}
{-# LANGUAGE MultiParamTypeClasses #-}

import Data.Kind (Constraint)
--import GHC.IO.Unsafe
import GHC.Base
\end{code}
%endif

\documentclass[acmsmall,review,natbib=false,anonymous]{acmart}

\usepackage[backend=biber,citestyle=authoryear,style=alphabetic]{biblatex}
\bibliography{bibliography}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
  }
\usepackage[plain]{fancyref}
\usepackage{mathpartir}
\usepackage{newunicodechar}
\input{newunicodedefs}

%%%%%%%%%%%%%%%%% ott %%%%%%%%%%%%%%%%%

\usepackage[supertabular,implicitLineBreakHack]{ottalt}
\inputott{ott.tex}

%%%%%%%%%%%%%%%%% /ott %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% Workaround %%%%%%%%%%%%%%%%%

% This should be handled by the acmclass article, there are a couple
% of issues about
% this. https://github.com/borisveytsman/acmart/issues/271,
% https://github.com/borisveytsman/acmart/issues/327 . Both have been
% merged long ago, and the version of acmart in the shell.nix is from
% 2020.

%% \usepackage{fontspec}
%% \setmainfont{Linux Libertine O}
%% \setsansfont{Linux Biolinum O}
%% \setmonofont{inconsolata}

%%%%%%%%%%%%%%%%% /Workaround %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% lhs2tex %%%%%%%%%%%%%%%%%

\let\Bbbk\undefined    % see https://github.com/kosmikus/lhs2tex/issues/82
%include polycode.fmt
%if style == poly
%format ->. = "⊸"
%format =>. = "\Lolly "
%format .<= = "\RLolly"
%format <== = "\Leftarrow"
%format IOL = "IO_L"
%format . = "."
%format exists = "\exists"
%format forall = "\forall"
%format pack = "\kpack"
%format pack' = "\kpack!"
%
%format a1
%format a_n
%format an = a_n
%endif

%let full = False

%%%%%%%%%%%%%%%%% /lhs2tex %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      \setlength{\marginparwidth}{1.2cm} % A size that matches the new PACMPL format
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\csongor}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=purple,#1]{#2}}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2]{}
      \newcommand{\info}[2]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}
  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Domain-specific macros %%%%%%%%%%%%%%%%%

  \newcommand{\cscheme}[1]{\mathcal{#1}}
  \newcommand{\aand}{\mathop{\&}}
  \DeclareMathOperator*{\bigaand}{\vcenter{\hbox{\Large\&}}}
  \newcommand{\lollycirc}{\raisebox{-0.2ex}{\scalebox{1.4}{$\circ$}}}
  \newcommand{\Lolly}{\mathop{=\!\!\!{\lollycirc}}}
  \newcommand{\RLolly}{\mathop{\lollycirc\!\!\!=}}
  \newcommand{\rlolly}{\mathop{\reflectbox{$\multimap$}}}
  \newcommand{\subst}[2]{[#1]#2}
  \newcommand{\sby}[2]{#1 ↦ #2}
  \newcommand{\vdashi}{⊢_{\mathsf{i}}}
  \newcommand{\vdashs}{⊢_{\mathsf{s}}}
  \newcommand{\vdashsimp}{⊢_{\mathsf{s}}^{\mathsf{simp}}}

  % language keywords
  \newcommand{\keyword}[1]{\mathbf{#1}}
  \newcommand{\klet}{\keyword{let}}
  \newcommand{\kcase}{\keyword{case}}
  \newcommand{\kwith}{\keyword{with}}
  \newcommand{\kpack}{\keyword{pack}}
  \newcommand{\kunpack}{\keyword{unpack}}
  \newcommand{\kin}{\keyword{in}}
  \newcommand{\kof}{\keyword{of}}

  % defining grammars
  \newcommand{\bnfeq}{\mathrel{{:}{:}{=}}}
  \newcommand{\bnfor}{\mathrel{\mid}}

%%%%%%%%%%%%%%%%% /Domain-specific macros %%%%%%%%%%%%%%%%%

\acmConference[ICFP'21]{ICFP}{2021}{} 
\acmYear{2021}
\copyrightyear{2021}
\setcopyright{none}

\begin{document}

\title{Linear Constraints}

\author{Jean-Philippe Bernardy}
\affiliation{
  \institution{University of Gothenburg}
  \city{Gothenburg}
  \country{Sweden}
}
\email{jean-philippe.bernardy@@gu.se}
\author{Richard Eisenberg}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{richard.eisenberg@@tweag.io}
\author{Csongor Kiss}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{csongor.kiss14@@imperial.ac.uk}
\author{Arnaud Spiwack}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{arnaud.spiwack@@tweag.io}
\author{Nicolas Wu}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{n.wu@@imperial.ac.uk}

\keywords{GHC, Haskell, laziness, linear logic, linear types,
  constraints, inference}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024</concept_id>
       <concept_desc>Software and its engineering~Language features</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011039</concept_id>
       <concept_desc>Software and its engineering~Formal language definitions</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Language features}
\ccsdesc[300]{Software and its engineering~Functional languages}
\ccsdesc[300]{Software and its engineering~Formal language definitions}

\begin{abstract}
This paper presents \emph{linear constraints}, a language feature that improves
the ergonomics of using linear types by freeing programmers from having to
manually pass around linear resource tokens. The resulting code retains the
safety of the linear version while also keeping the simplicity of the
traditional version. We present a qualified type system with linear constraints,
and a typechecking algorithm. We prove the soundness of the algorithm with
respect to the type system, and show how our changes can be integrated into
|OutsideIn|, \textsc{ghc}'s existing constraint solver algorithm.
\end{abstract}

\maketitle

\renewcommand{\shortauthors}{Bernardy, Eisenberg, Kiss, Spiwack, Wu}

\section{Introduction}
\label{sec:introduction}
\info{There  is an Appendix section with unorganised thoughts and
  examples.}

Linear type systems have seen somewhat of a renaissance in recent years in
various mainstream programming communities. Rust's ownership system guarantees
memory safety for systems programmers, Haskell's linear types give functional
programmers safe \textsc{api}s for low-level mutable data structures, and even
dependently typed programmers can now use linear types with Idris 2.

\csongor{Say something about ergonomics to set up the next part}

The essence of linear types is careful tracking of the usage of resources. To
get a sense of their power, consider the following example from
the Linear Haskell article~\cite{LinearHaskell}\footnote{|IOL| is the linear IO
monad}:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp =   do  {  h <- openFile fp
                     ;  (h, Ur xs) <- readLine h
                     ;  closeFile h
                     ;  return xs }
\end{code}

This simple function opens a file, reads its first line, then closes
it. Linearity ensures that the file handle |h| is consumed at the end.
Forgetting to call |closeFile h| would result in a type error since |h| would
remain unused at the end of the function. Notice that |readLine| consumes the
file handle, and returns a fresh |h| that shadows the previous version, to be
used in further interactions with the file. The line's content is a string
|xs| that is returned in an |Ur| wrapper (pronounced ``unrestricted'') to
signify that it can be used arbitrary many times.

% We see that linear types introduce some noise on the |readLine| line:
% we need to destruct an extra pair, and an extra |Ur| (called
% |Unrestricted| in~\cite{LinearHaskell}), compared to the
% traditional (albeit less safe)\unsure{It only gets worse in larger
%   program. This makes the extra safety afforded by linear types too
%   rarely worth it.}

Compare this function with one would write in a non-linear language:
\begin{code}
firstLine :: FilePath -> IO String
firstLine fp = do  {  h <- openFile fp
                   ;  xs <- readLine h
                   ;  closeFile h
                   ;  return xs }
\end{code}

This style is less safe, because the type system does not keep track of the
file handle, and the programmer risks forgetting doing it. But it is also
simpler: apparently, using a linear language means trading clarity for safety. When
the file handle is an expendable resource, the type system must know at all
times where it is being consumed, so the file handle must be passed around
manually, resulting in extra noise. 
% Worse, the larger the program gets, the more additional bookkeeping this requires. % this is vaccuous, perhaps something else is meant?
But when reading the non-linear version, it is clear where the
handle is used, and ultimately, consumed. Couldn't the compiler
figure this out without extra help?

In this paper, we answer this question affirmatively, and introduce
\emph{linear constraints}. Like class constraints, linear constraints
are passed around implicity by the compiler, and like linear arguments,
they can be used to track resources. This way, resource consumption is tracked
without explicitly having to thread the tokens through the program. We show how
linear constraints can be implemented as an extension of Haskell's type class
mechanism.

With this extension, the final version of |firstLine| is almost the
same as the traditional version above, with a few minor modifications:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp = do  {  pack' h <- openFile fp
                   ;  pack' xs <- readLine h
                   ;  closeFile h
                   ;  return xs }
\end{code}
The only changes from the unsafe version are that this version runs in the linear IO monad,
and explicit |pack'| annotations are used to indicate the variables that require
special treatment. Crucially, the resource representing the open file no longer
needs to be passed around manually.

Our contributions are as follows:
\begin{itemize}
\item A system of qualified types that allows a constraint assumption
  to be given a multiplicity. Linear assumptions are used precisely
  once in the body of a definition
  (\Fref{sec:qualified-type-system}).
\item This system supports examples that have motivated the design of
  several resource-aware systems, such as ownership à la Rust (\Fref{sec:memory-ownership}), or
  capabilities in the style of Mezzo~\cite{mezzo-permissions}
  or \textsc{ats}~\cite{AtsLinearViews}; accordingly, our system
  may serve as a way to unify these lines of research.
\item An inference algorithm, that respects the multiplicity of
  assumptions. We prove that this algorithm is sound with respect to
  our type system~(\Fref{sec:type-inference}).
\item Expressions in our qualified
  type system desugar into a core language (directly adapted
  from Linear Haskell~\cite{LinearHaskell}) that supports linear functions. We
  prove that the output of desugaring is well-typed in the core
  language~(\Fref{sec:desugaring}).
\end{itemize}


\section{Motivation}
\label{sec:motivation}

The goal of this paper is to introduce linear constraints as a mechanism for
ergonomically using linear types in Haskell.  Of course, in order to appreciate
this work, it helps to understand how linear types work.\jp{condescending}  In this section, we
sketch out some of the basics of how linear types work in Haskell, and then
give a number of examples that motivate our extension into linear constraints.

\subsection{Linear Types}
\label{sec:linear-types}

\begin{figure}%
  \centering
  \begin{subfigure}{.3\linewidth}%
    \noindent%
\begin{code}
openFile   :: FilePath -> IOL Handle
readLine   :: Handle ⊸ IOL (Handle, String)
closeFile  :: Handle ⊸ IOL ()
\end{code}
\caption{Linear Types}
\label{fig:linear-interface}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.55\linewidth}
\begin{code}
openFile   :: FilePath -> IOL (exists h. Ur (Handle h) .<= Open h)
readLine   :: Open h =>. Handle h -> IOL (Ur String .<= Open h)
closeFile  :: Open h =>. Handle h -> IOL ()
\end{code}
\caption{Linear Constraints}
\label{fig:constraints-interface}
  \end{subfigure}
\caption{Interfaces for file manipulation}
\end{figure}

Linear Haskell~\cite{LinearHaskell} adds a new type of functions,
dubbed \emph{linear functions}, and written |a ⊸ b|\footnote{The linear function
  type and its notation come from linear
  logic~\cite{girard-linear-logic}, to which the phrase \emph{linear
    types} refers. All the various design of linear typing in the
  literature amount to adding such a linear function type, but details
  can vary wildly. See~\cite[Section 6]{LinearHaskell} for an analysis
  of alternative approaches.}. A linear function consumes its
  argument exactly once, more precisely, Linear Haskell~\cite[Section
  2.1]{LinearHaskell} defines linear functions as:

\begin{quote}
\emph{Meaning of the linear arrow}:
|f :: a ⊸ b| guarantees that if |(f u)| is consumed exactly once,
then the argument |u| is consumed exactly once.
\end{quote}
To make sense of this statement we need to know what ``consumed exactly once'' means.
Our definition is based on the type of the value concerned:
\begin{definition}[Consume exactly once]~ \label{def:consume}
\begin{itemize}
\item To consume a value of atomic base type (like |Int| or |Handle|) exactly once, just evaluate it.
\item To consume a function value exactly once, apply it to one argument, and consume its result exactly once.
\item To consume a pair exactly once, pattern-match on it, and consume each component exactly once.
\item In general, to consume a value of an algebraic datatype exactly once, pattern-match on it,
  and consume all its linear components exactly once.
\end{itemize}
\end{definition}
%
\noindent
Note that a linear arrow specifies \emph{how the function uses its argument}. It does not
restrict \emph{the arguments to which the function can be applied}.
In particular, a linear function cannot assume that it is given the
unique pointer to its argument.  For example, if |f :: a ⊸ b|, then
the following is fine:
\begin{code}
g :: a -> b
g x = f x
\end{code}
The type of |g| makes no particular guarantees about the way in which it uses |x|;
in particular, |g| can pass that argument to |f|.

The example in the previous section makes use of |readLine :: Handle ->. IOL
(Handle, String)|. Since this is a linear function the input handle must be
consumed exactly once and can therefore no longer be used to close the file:
doing so would result in a type error.  To resolve this, a new handle for the
same file is produced that can be used with |closeFile|. In the code, the same
name |h| is given to the new handler, thus shadowing the old |h| that can no
longer be used anyway, and to give the illusion that it is being passed around.

From the perspective of the programmer, this is unwanted boilerplate. Using
shadowing is a trick that hides an underlying problem that should not exist:
it is not the handler that should be consumed by |readLine| at all.  Rather, it
is its implicit state as a handler for an open file  that should be consumed
exactly once. Once this open state is consumed, the handler can no longer be read
from or closed without triggering a compile time error.

In this article we introduce a generic extension to Linear Haskell which lets
the typechecker handle this. There is a bit of boilerplate left, but it doesn't
involve managing names. This considerably lowers the cost of using a linear-type
based abstraction.

\subsection{Working With Linear Constraints}
\label{sec:what-it-looks-like}

Consider the Haskell function |show|:
\begin{code}
show :: Show a => a -> String
\end{code}
In addition to the function arrow |->|, common to all functional
programming languages, the type of this function features a fat arrow |=>|.
Everything to the
left of a fat arrow is called a \emph{constraint}. Here |Show a| is a
type-class constraint, but there are other kinds of constraints such
as equality constraints or implicit parameter constraints.

What is crucial, for our purpose, is that constraints are handled implicitly by
the typechecker. That is, if we want to |show| an integer we would write |show
42|, and the typechecker would handle proving that |Show Int| without
intervention from the programmer.
Thus, constraints are a convenient mechanism that allow the compiler
to automatically fill in implicit arguments.

For our |readLine| example, the implicit argument is the state of a handler |h|.
This can be managed as a constraint called |Open h|, which indicates that the
file associated to the handler is open. The idea is that this constraint
should be a linear value, so that it can only be consumed exactly once.
In order to manage linearity implicitly, this article introduces a
linear fat arrow |=>.|, much like Linear Haskell introduced a linear
function arrow |->.|. We dub constraints to the left of a linear fat
arrow \emph{linear constraints}.
Using the linear constraint |Open h| to represent that a file is, indeed, open.
Using it, we can give the following type to |closeFile|:

\begin{code}
closeFile :: Open h =>. Handle h -> IOL ()
\end{code}

There are a few things to notice:
\unsure{NW: I think this first item can be dropped. Only the second point really needs to be made}
\begin{itemize}
\item First, there is a type variable |h| which did not exist in
  previous representation. In the representation of the Linear Haskell
  paper, for instance, |closeFile| had type |closeFile :: File ->. IOL
  ()|. This |h| is a type-level name for the file handle which we are
  closing.
  Ideally, we'd like the linear constraint to refer directly to the handle value, and have the type |closeFile :: (h :: Handle) -> Open h =>. IOL ()|.
  While giving a name to a function argument is the bread and butter of more
  dependently typed languages such as \textsc{ats}~\cite{ats-lang} or Liquid
  Haskell~\cite{liquid-haskell-abstract-refinement-types}, haskell doesn't have such a
  naming mechanism built in, so we have resort to a type variable to mediate the
  link between handles and linear constraints.
\item Second, if  we have a single, linear, |Open h|
  available, then after |closeFile| there will not be any |Open h|
  left to use, thus preventing the file from being closed
  twice. This is precisely what we were trying to achieve.
\end{itemize}

This deals with closing files and ensuring that a handler cannot be closed more
than once.  However, we still need to explain how a constraint |Open h| can come
into existence. For this, we introduce a type construction |exists a1 ... an. t
.<= Q|, where |Q| is some (linear) constraint that is created.  The type of
|openFile| is thus:
\begin{code}
openFile   :: FilePath -> IOL (exists h. Ur (Handle h) .<= Open h)
\end{code}
The output of this function is a new handle that is unrestricted,
along with a new constraint that indicates that the handle is open.
Since the handle is unrestricted it can be freely passed around. When it is,
the |Open h| constraint implicitly tracks its state.

We must also ensure that |readLine| can both promise to only operate on an open
file, and keep that file open after reading. To do so, its signature 
indicates that given a handle |h|, it consumes and produces the implicit |Open h|
constraint, while also producing an unrestricted |String|.
\begin{code}
readLine   :: Open h =>. Handle h -> IOL (Ur String .<= Open h)
\end{code}

The fact that existential quantification generate new type-level names
is folklore. It's used crucially in the interface of the
|ST| monad~\cite{st-monad} and of type-class
reflection~\cite{type-class-reflection} (in both of these cases, existential
quantification is encoded as a rank-2 universal quantification). We
shall use it exactly this way: |openFile| uses an existential
quantifier to generate the type-level name of the file
handle. Existentially quantified types are paired with a constraint |Q|
which we understand as being returned by functions. We will freely
omit the |exists a1 ... an.| or |.<= Q| parts when they are
empty. This lets us give the following \textsc{api} to files:


Haskell doesn't have existential quantification, however it can
be encoded as a \textsc{gadt}. For instance |exists h. Ur (Handle h) .<= Open h| can
be implemented as
\jp{The following type should be called |OpenFileHandle| ? }
\begin{code}
data PackHandle where
  Pack :: Open h =>. Handle h -> PackHandle
\end{code}
Therefore, the existential types of this article are really a
convenience for the sake of exposition\jp{But if this section is a pedagogical description, the presentation as GADT should come first. The theoretical device can come as an afterthought. One can also argue that the GADT is less pedagogical, then this paragraph should be deleted.}\unsure{Though, see the
  existential type paper}. Correspondingly, existential types are
introduced by a data constructor, which we write as |pack|.

When pattern-matching on a |pack| all the existentially quantified
names are introduced in scope and all the returned constraints are
made available. We also use |pack' x| as a shorthand for
for |pack (Ur x)|. With all these ingredients we can indeed write
the example given in the introduction.

\subsection{Motivating Examples}
\label{sec:examples}

To get a sense of how the features we introduce should behave, let's look at
some simple examples. Using constraints to represent expendable resources allows the typechecker to
reject certain classes of ill-behaved programs. Accordingly, the following examples show
the different reasons a program might be rejected.

In what follows, we will be using a class |C| that is consumed by the |useC|
function.
\begin{code}
class C

useC :: C =>. Int
\end{code}
The type of |useC| indicates that it consumes its linear resource |C| exactly once.

\subsubsection{Dithering}

Now consider the following program, which we reject:
\begin{code}
dithering :: C =>. Bool -> Int
dithering x = if x then useC else 10
\end{code}
The problem with |dithering| is that it does not commit properly to how much
resource it requires: the branch where |x| holds will use the resource in |C|
whereas the other does not.

\subsubsection{Neglecting}

Now consider the type of the linear version of |const|:
\begin{spec}
const :: a ->. b -> a
\end{spec}
This function uses its first argument linearly, and ignores the second.

One way to improperly use the linear |const| is by neglecting a linear variable:
\begin{code}
neglecting :: C =>. Int
neglecting = const 10 useC
\end{code}
The problem with |neglecting| is that although |useC| is mentioned in this program,
it is never evaluated because |const| does not use its second argument.
The constraint |C| is never consumed so this program ought to be rejected.\jp{Rather, it is consumed ω times. Not sure how this fits the exposition. Maybe this example needs moving somewhere else. What follow suggests a funny definition of consumed, where ``consumed ω times'' is a special case of ``not consumed''. That's confusing to me. But I can accept it if it made very explicit. As it stands it seems implicit, or only defined by examples.}

The rule is that a linear constraint is only consumed in a linear context. For
example,
\begin{code}
notNeglecting :: C =>. Int
notNeglecting = const useC 10
\end{code}
is accepted, because the |C| constraint is passed on to |useC| which itself
appears as an argument to a linear function (whose result is consumed linearly).

\subsubsection{Overusing}
\label{sec:overusing}

Finally, consider the following program, which should be rejected:
\begin{code}
overusing :: C =>. (Int, Int)
overusing = (useC, useC)
\end{code}
This is program is rejected because it uses |C| twice. However, the
following version is accepted:
\begin{code}
notOverusing :: (C, C) =>. (Int, Int)
notOverusing = (useC, useC)
\end{code}
That is, it is possible to have multiple copies of a given
constraint. Our system is designed so that the order of resolution is
non-deterministic. This corresponds to the assumption that all
instances of a linear constraint are equivalent. Consequently, if an
\textsc{api} uses linear constraints, it must be designed to ensure that the
runtime behaviour is not dependent on the order of constraint
resolution.  \unsure{Linear constraints have no runtime payload, or
  even if they did, they would not be observable (safely) as
  dictionary methods use the dictionary in an unrestricted way.}

\subsection{Linear IO}
\label{sec:linear-io}

The file handling example discussed in sections~\ref{sec:linear-types}
and~\ref{sec:what-it-looks-like} uses a linear version of the |IO| monad, |IOL|.
There are two main modifications compared to the traditional |IO|. Firstly, the
type of the monadic operations |>>=| and |return| are changed to
use linear arrows. 
% In the case of |>>=|, its argument also must be a linear function: \jp{I could not tell what this was saying which was not already said. Commented out.}
%
\begin{code}
(>>=) :: IOL a ->. (a ->. IOL b) ->. IOL b
return :: a ->. IOL a
\end{code}
%
Bind must be linear because, as explained in the previous
section, a linear constraint can only be consumed in a linear context. For the
following program to typecheck, bind must be linear:
\begin{code}
readTwo ::  Open h =>. Handle h -> IOL (Ur (String, String) .<= Open h)
readTwo h =  readLine h >>= \case pack' xs ->
             readLine h >>= \case pack' ys ->
             return (pack' (xs, ys))
\end{code}
If it were not linear, the first argument |readLine h| of the first bind would
not be able to consume the |Open h| constraint.

\section{Application: memory ownership}
\label{sec:memory-ownership}

In Haskell, memory deallocation is normally the responsibility of a
garbage collector. However, garbage collection comes with runtime
costs which not all applications can afford: one must then resort to
explict memory allocation and deallocation. This task is error
prone: one can easily forget a deallocation (causing a memory leak) or
deallocate several times (corrupting data).

With linear constraints, it is possible to use the type system to
enforce correct deallocation of memory.  The approach is to use a
linear constraint to represent \emph{ownership} of a memory location,
i.e. the responsibility to deallocate it. We use a linear constraint
|Own n|, such that a program which has an |Own n| constraint in
context is responsible to deallocate the memory area(s) associated
with a memory location |n|.  Because of linearity, this constraint
must be discharged exactly once, so it is guaranteed that the memory
is deallocated correctly.

In the above, |n| is a type variable (of a special kind |Location|)
which represents a memory location. Locations mediate
the relationship between references and ownership constraints.

For more granular control, we will be using three linear constraints: one for
ownership, one for reading, and one for writing:


> class Read (n :: Location)
> class Write (n :: Location)
> class Own (n :: Location)


Thanks to this extended set of constraints, programs are able to read or write
to a memory reference without necessarily owning it. To ensure consistency,
writes can only be done when we are sure that no one else is reading. Therefore,
writing also requires owning the read capability.
We will systematically use the |RW| set of constraints, defined below, instead
of |Write|.

> type RW n = (Read n, Write n)

Likewise, a location cannot be deallocated if any part of the program
has a read or write reference to it, so all 3 capabilities are
needed for ownership. So we use |O|, instead of |Own|, defined thus:

> type O n = (Read n, Write n, Own n)

With these components in place, we can provide an \textsc{api} for ownable
references.

> data AtomRef (a :: Type) (n :: Location)

The type |AtomRef| is a type of references to a pure type |a| with
location |n|. Allocation of a reference can be done using the
following function.

> newRef :: (forall n. O n =>. AtomRef a n -> Ur b) ⊸ Ur b

The reference is made available in a function which we call the
\emph{scope} of the reference. The return type of the scope is,
crucially, |Ur b|.  Indeed, if we would allow returning any type |b|
then the |O n| constraint could be embedded in |b|, and therefore
escape from the scope.
This effectively lets |O n| be an unrestricted constraint, and
would no longer guarantee that the reference has a unique owner.

To read a reference, a simple |Read| constraint is demanded, and
immediately returned. Writing is handled similarly.

> readRef :: (Read n) =>. AtomRef a n -> Ur a .<= Read n
> writeRef :: (RW n) =>. AtomRef a n -> a -> () .<= RW n

Note that the above primitives do not need to explicitly declare
effects in terms of a monad or another higher-order effect-tracking
device: because the |RW n| constraint is linear, passing it suffices
to ensure proper sequencing of effects concerning location |n|.
Deallocation consumes all linear constraints associated with |O n|.

> freeRef :: O n =>. AtomRef a n -> ()

As an alternative to freeing the reference, one could transfer control
of the memory location to the garbage collector. This operation is
sometimes called ``freezing'':

> freezeRef :: O n =>. AtomRef a n -> Ur a

The toolkit we set up handles references to base types just fine.
But what about references to references; or,
indeed, arrays of arrays? A common example of plain Linear Haskell is
a pure interface to mutable arrays, but it's terribly
inconvenient to make mutable arrays of mutable arrays, at least if we
want to freeze them in $O(1)$.

The |AtomRef| \textsc{api} above is not quite enough to handle nested
references. Indeed if we have |r :: AtomRef (AtomRef a s) r| and |s ::
AtomRef a s|, we can |writeRef r s|; however we would then be stuck
with the |Own s| constraint, which can only be consumed by
deallocating |s|. What we really need here, is a way to
relinquish the ownership of |s| to |r| so that |r| becomes responsible
for deallocating |s|. This is what the type |PArray| lets us do

> data PArray (a :: Location -> Type) (n :: Location)
> newPArray    :: (forall n. O n =>. PArray a n -> Ur b) ⊸ Ur b

The key is that the kind of |a| is |Location ->
Type|. This way we can easily enforce that each reference in the
array refers to the same location |n|. Both types |AtomRef a| and
|PArray a| have kind |Location -> Type|, and therefore one can allocate, and
manipulate arrays of arrays with this \textsc{api}.

When writing a reference (be it an array or an |AtomRef|) in an array,
ownership of the reference is relinquished to the array.

> writePArray  :: (RW n, O p) =>. PArray a n -> Int -> a p -> () .<= RW n

More precisely, the ownership of the location |p| is absorbed into that
of |n|. Therefore, the associated operational semantics is to move the
reference inside the array (and deallocate any previous reference at that index).

We still want to have read and write access to inner references, of
course, so we use |lendPArrayElt|:

> lendPArrayElt  :: RW n =>. PArray a n -> Int -> (forall p. RW p =>. a p -> r .<= RW p) ⊸ r .<= RW n

The |lendPArrayElt a i k| primitive lends access to the reference at index |i| in
|a|, to a scope function |k| (in Rust terminology, the scope
``borrows'' an element of the array). Here, the return type of the scope, |r|,
is not in |Ur|: since the scope must return the |RW p| constraint, it is
not possible to leak it out by packing it into |r|, so it's not necessary to
wrap the result in |Ur|.
Crucially, with this \textsc{api}, |RW n| and |RW p| are never simultaneously
available. The following function would not be sound:

> extractEltWrong  :: RW n =>. PArray a n -> Int -> exists p. Ur (a p) .<= (RW n, RW p)

Indeed, when called from a context where the program owns |n|,
|n| could immediately be deallocated, even though |RW
p| would remain available, letting us write to free memory. For the same reason,
gaining read access to an element needs to be done using a
scope \textsc{api} as well:

> lendPArrayEltRead    :: Read n =>. PArray a n -> Int
>                      -> (forall p. Read p =>. a p -> r .<= Read p)
>                      ⊸ r .<= Read n

Finally, we can freeze arrays, and arrays of arrays or more, using the
following primitive

> freezePArray :: O n =>. PArray a n -> () <== Read n

After |freezePArray n|, we have unrestricted read access to |n| (and
any element of |n|), as expected.

> readArray :: Read n => PArray a n -> Int -> a n

\section{A qualified type system}
\label{sec:qualified-type-system}

Let us now describe the technical material which supports the
examples of Section~\ref{sec:examples}. The presentation in this
section, as well as Section~\ref{sec:type-inference}, strongly mirrors
the presentation of OutsideIn~\cite{OutsideIn}. OutsideIn is a
foundation of the type inference algorithm of \textsc{ghc}, the most
popular Haskell compiler where Linear Haskell~\cite{LinearHaskell} is being
implemented\footnote{A version of Linear Haskell has been released as
  part of version 9.0.1 of \textsc{ghc}}, as such we choose to frame
this presentation in terms of OutsideIn. By using such a
framing, we intend to highlight the concrete changes required to
\textsc{ghc}, or similar compilers, to support linear constraints.

However, we have chosen, for the sake of clarity of the exposition, to
omit details of OutsideIn which do not interact meaningfully with
linear constraints. We shall point out such simplifications where they
arise.

\subsection{Multiplicities}
\label{sec:multiplicities}

Like in Linear Haskell~\cite{LinearHaskell} we make use of a
system of \emph{multiplicities}, which describe how
many times a function consumes its input. Linear Haskell has a system
of multiplicity which supports multiplicity polymorphism. However, in this article, we will use only the
simplest system of multiplicity: that composed of only $[[1]]$ (representing
linear functions) and $[[omega]]$ (representing
regular Haskell functions).
$$
\begin{array}{lcll}
  [[pi]], [[rho]] & \bnfeq & [[1]] \bnfor [[omega]] & \text{Multiplicities}
\end{array}
$$
\noindent
The idea of multiplicity goes back at least
to~\cite{ghica_bounded_2014}, where they are dubbed a \emph{resource
  semiring}. The power of multiplicities is that they can encode the
structural rules of linear logic with only semiring
operations (that is, addition and multiplication).

\smallskip
\begin{minipage}[c]{0.4\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[pi + rho]] & = & [[omega]]
  \end{array}
\right.
$$
\end{minipage}
\begin{minipage}[c]{0.4\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[1 . pi]] & = & [[pi]] \\
    [[pi . 1]] & = & [[pi]] \\
    [[omega . omega]] & = & [[omega]]
  \end{array}
\right.
$$
\end{minipage}
\smallskip

Polymorphism of multiplicity, in Linear Haskell, is used to avoid
duplicating the definition higher-order functions (such as |map| and
|fold|). First-order functions do not need polymorphism, because
linear functions can be $\eta$-expanded into unrestricted function as
explained in Section~\ref{sec:linear-types}. Functions taking
constraint-qualified arguments are much rarer for non-linear
constraint. So multiplicity-polymorphic constraints don't seem very
useful, and it is not clear how to extend the constraint solver of
Section~\ref{sec:constraint-solver} to multiplicity-polymorphic
constraints.

\subsection{Simple constraints}
\label{sec:constraint-domain}

Following OutsideIn (see~\cite[Section 3.2, in particular Figure
3]{OutsideIn}), we parameterise our entire type system by a constraint
domain\,--\,the $X$ in OutsideIn($X$). Such a domain is characterised by
a set of \emph{atomic constraints} written $[[q]]$ and an entailment
relation $[[Q1 ||- Q2]]$, with properties specified in
Figure~\ref{fig:entailment-relation}. Let us adopt \textsc{ghc}'s
terminology and call these \emph{simple constraints} to distinguish
them from the richer constraints of Section~\ref{sec:wanteds}.

For instance, in \textsc{ghc},
the domain includes type classes, and the entailment relation describes
instance resolution.
In the examples of Section~\ref{sec:examples}, only the simplest kind
constraints are needed, whereby we mean that atomic constraints are treated
as uninterpreted symbols by the entailment relation. Being parameterised by the domain therefore
only serves to support the rest of Haskell, or future extensions.
%
\info{See Fig 3, p14 of OutsideIn\cite{OutsideIn}.}
%
$$
\begin{array}{lcll}
  [[Q]] & \bnfeq & [[pi.q]] \bnfor [[Q1*Q2]] \bnfor [[Empty]] &
                                                                \text{Simple constraints}
\end{array}
$$
%
The multiplicity $\pi$ in front of atomic constraints is the \emph{scaling
  factor}. \jp{Multiplicity and scaling factor essentially evoke the same picture. Annoyingly both 'multiplying' and 'scaling' are used throughout the paper.}
It indicates whether the constraint is to be used linearly
($[[1]]$) or without restriction ($[[omega]]$).

Simple constraints are treated abstractly\jp{Seems wrong, we just defined simple constraints by structural induction; is it only to ignore this structure and treat all of them abstractly? Confusing.} by our system
(just like they are in OutsideIn). For
inference, in Section~\ref{sec:type-inference}, we will need a domain-specific solver, of which
we only require that it adheres to the interface given in
Section~\ref{sec:constraint-solver}. But for the sake of this section,
we only need that the domain be equipped with the entailment relation.

OutsideIn introduces, as part of the constraint domain, a generalised
kind of constraint $\mathcal{Q}$, which include top-level axioms, such
as type-class instance declarations. Such top-level axioms
are never linear\,--\,just like how top-level definitions are never linear in
Linear Haskell~\cite{LinearHaskell}\,--\,as such they don't have
interesting interactions with the rest of the system, and we choose to
omit them for simplicity.

We consider simple constraints to be equal up to associativity and
commutativity of tensor products, as well as idempotence of the
unrestricted constraints. That is
$$
\begin{array}{rcl}
  [[Q1 * Q2]] & = & [[Q2 * Q1]] \\
  [[(Q1*Q2)*Q3]] & = & [[Q1*(Q2*Q3)]] \\
  [[omega.q * omega.q]] & = & [[omega.q]] \\
  [[Q * Empty]] & = & [[Q]]
\end{array}
$$
%
Scaling is extended to all constraints as follows:
%
$$
\left\{
  \begin{array}{lcl}
    [[pi.Empty]] & = & [[Empty]]\\
    [[pi.(Q1 * Q2)]] & = & [[pi.Q1 * pi.Q2]] \\
    [[pi.(rho. Q)]]  & = & [[(pi.rho) . Q]]
  \end{array}
\right.
$$
\unsure{Rendering}

\noindent
The rule that $[[omega.(Q1 * Q2)]] = [[omega.Q1 * omega.Q2]]$ is not a
typical feature of linear logic. Linear Haskell, however, introduces
the corresponding type isomorphism for the sake of polymorphism. While this
article isn't concerned with polymorphism, this equality does make
the overall presentation a bit simpler. Note, in particular, that $[[1.Q]] = [[Q]]$ and
$[[omega.Q * omega.Q]] = [[omega.Q]]$.

We will often omit the scaling factor for linear atomic constraints
and write $[[q]]$ for $[[1.q]]$. This convention does not induce any
ambiguity Indeed the only potential ambiguity is between $[[pi.q]]$
and $[[pi.(1.q)]]$, which are in fact equal.

\begin{definition}[Requirements for the entailment relation]\label{def:entailment-relation}
  The constraint entailment relation must satisfy the properties
  in \autoref{fig:entailment-relation}.
\end{definition}
Apart from linearity, the main difference with OutsideIn is that we
don't require the presence of equality constraints. We come back to
the motivation for this simplification in
Section~\ref{sec:type-inference}.

  
\begin{figure}
  $$
    \begin{array}{l}
      [[Q ||- Q]] \\
      \text{if } [[Q1 ||- Q2]] \text{ and } [[Q * Q2 ||- Q3]] \text{ then } [[Q * Q1 ||- Q3]] \\
      \text{if } [[Q ||- Q1 * Q2]] \text{ then there exists } [[Q']] \text{ and } [[Q'']]
      \text{ such that } [[Q]]=[[Q' * Q'']] \text{, } [[Q' ||- Q1]] \text{ and } [[Q'' ||- Q2]] \\
      \text{if } [[Q ||- Empty]] \text{ then there exists } [[Q']] \text{ such that } [[Q]]=[[omega.Q']] \\
      \text{if } [[Q1 ||- Q1']] \text{ and } [[Q2 ||- Q2']] \text{ then } [[Q1 * Q2 ||- Q1' * Q2']] \\
      \text{if } [[Q ||- rho. q]] \text{ then } [[pi . Q ||- (pi.rho). q]] \\
      \text{if } [[Q ||- (pi.rho) . q]] \text{ then there exists } [[Q']] \text{ such
      that } [[Q]] = [[pi. Q']] \text{ and } [[Q' ||- rho . q]] \\
      \text{if } [[Q1 ||- Q2]] \text{ then } [[omega.Q1 ||- Q2]] \\
      \text{if } [[Q1 ||- Q2]] \text{ then for all } [[Q']], [[omega.Q' * Q1 ||- Q2]]
    \end{array}
  $$
\caption{Requirements for the entailent relation $[[Q ||- Q]]$}
\label{fig:entailment-relation}
\end{figure}

An important feature of simple constraints is that, while scaling
syntactically happens at the level of atomic constraints, these properties
of scaling extend to scaling of arbitrary constraints.

\begin{lemma}[Promotion]
  \label{lem:q:promotion}
  If $[[Q1 ||- Q2]]$, then $[[pi.Q1 ||- pi.Q2]]$.
\end{lemma}

\begin{lemma}[Inversion of promotion]
  \label{lem:q:scaling-inversion}
  If $[[Q1 ||- pi.Q2]]$, then $[[Q1]]=[[pi.Q1']]$ and $[[Q1' ||- Q2]]$ for some $[[Q']]$.
\end{lemma}

\subsection{Typing rules}
\label{sec:typing-rules}

With this material in place, we can present our type system. The
grammar is given in Figure~\ref{fig:declarative:grammar}, and the
typing rules in Figure~\ref{fig:typing-rules}.

\begin{figure}
  \centering\small
  $$
  \begin{array}{lcll}
    [[a]], [[b]] & \bnfeq & \ldots & \text{Type variables} \\
    [[x]], [[y]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[K]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[s]] & \bnfeq & [[forall as. Q =o t]] & \text{Type schemes} \\
    [[t]], [[u]] & \bnfeq & [[a]] \bnfor [[exists as. t o= Q]] \bnfor [[t1 ->_pi t2]]
                            \bnfor [[T ts]] & \text{Types} \\
    [[G]], [[D]] & \bnfeq & [[empty]] \bnfor [[G, x:_pi s]] &
                                                              \text{Contexts} \\
    [[e]] & \bnfeq & [[x]] \bnfor [[K]] \bnfor [[\x. e]] \bnfor [[e1
                     e2]] \bnfor [[pack e]] & \text{Expressions}\\
                 &\bnfor & [[unpack x=e1 in
                     e2]] \bnfor [[case_pi e of { alts }]] &\\
                 &\bnfor & [[let_pi
                     x=e1 in e2]] \bnfor [[let_pi x : s = e1 in e2]] &
  \end{array}
  $$
  \caption{Grammar of the qualified type system}
  \label{fig:declarative:grammar}
\end{figure}

\begin{figure}
  \centering\small
  \drules[In]{$[[x :_1 s \in G]]$}{Context membership}{Var,Weaken}
  \drules[S]{$[[pi . G]]$}{Context scaling}{Empty,Binding}
  \drules[A]{$[[G1 + G2]]$}{Context addition}{Binding,Absent,Empty}
  \drules[E]{$[[Q;G |- e : t]]$}{Expression
    typing}{Var,Abs,App,Pack,Unpack,Let,LetSig,Case,Sub}
  \caption{Qualified type system}
  \label{fig:typing-rules}
\end{figure}

Like in OutsideIn~\cite[Section 4]{OutsideIn}, the type system of Figure~\ref{fig:typing-rules} is
a \emph{qualified type system} in the style first introduced
by~\cite{QualifiedTypes}. Such a qualified type system introduces a
judgement of the form $[[Q;G |- e : t]]$, where $[[G]]$ is a standard
type context, and $[[Q]]$ is a simple constraint as in
Section~\ref{sec:constraint-domain}. $[[Q]]$ behaves
much like $[[G]]$, which will be instrumental for
desugaring in Section~\ref{sec:desugaring}; the main difference is
that $[[G]]$ is addressed explicitly with variables, whereas $[[Q]]$
is used implicitly in \rref{E-Var}.

The type system of Figure~\ref{fig:typing-rules} is purely
declarative: it doesn't have any algorithmic properties. We will see
how to infer constraints in Section~\ref{sec:type-inference}. Yet,
this system is our ground truth: a system with a simple enough
definition that programmers can reason about typing. Another
thing of notice is that we will not attempt to give a dynamic
semantics to this language: instead we will give it meaning via
desugaring to a simpler core language in Section~\ref{sec:desugaring}.\jp{Does this core language have a dynamic (operational?) semantics? if so then we do give an operational semantics, albeit indirectly.}

The main differences with OutsideIn, either for simplification, or for
linear constraints, are: \info{See Fig 10, p25 of OutsideIn\cite{OutsideIn}.}
\begin{itemize}
\item The type system has linear functions (written $[[a ->_1 b]]$ in
  Figure~\ref{fig:typing-rules}). It is not necessarily obvious why:
  after all we seem to be mostly concerned about linearity in
  constraints, maybe it's all we need and we can forgo linear
  functions. But we are not that lucky.\jp{Since we have designed this language, we can hardly speak of luck. We have inflicted this burden upon ourselves.} Actually, just to define the
  bind combinator for the |IOL| monad from
  Section~\ref{sec:introduction}, we need linear arrows: |(>>=) :: IOL
  a ->. (a ->. IOL b) ->. IOL b|.

  So, linear arrows really can't be escaped. What's more, even moving
  away from |IOL|, the linearity of arrows interact in interesting
  ways with linear constraints. Consider $[[f : a ->_omega b]]$ and
  $[[x : q =o a]]$, then calling $[[f x]]$ would actually use $[[q]]$
  many times! That is, it must be impossible to derive
  $[[q ; f :_omega a ->_omega b, x :_omega q =o a |- f x : b]]$,
  otherwise we could make, for instance, the |overusing| function from
  Section~\ref{sec:overusing}. You can check that $[[q ; f :_omega a ->_omega b, x :_omega q =o a |- f x : b]]$ indeed doesn't
  type check, because the scaling of $[[Q2]]$ in \rref{E-App} ensures that
  the constraint would be $[[omega.q]]$ instead. On the other hand,
  it's perfectly fine to have $[[q ; f :_omega a ->_1 b, x :_omega q
  =o a |- f x : b]]$ when $[[f]]$ is a linear function.
\item In the \rref*{E-Let} rule we added support for local
  assumptions. This gives us the possibility to generalise a subset of
  the constraints needed by $[[e1]]$. The inference algorithm of
  Section~\ref{sec:type-inference} will not make use of this
  possibility, but we will revisit it in
  Section~\ref{sec:let-generalisation}.
\item Data types are not \textsc{gadt}s. Instead they are plain data
  types. This serves to considerably simplify the \rref*{E-Case}
  rule. This is another case of simplifying away the bits of OutsideIn
  which don't meaningfully impact the system. It would be
  straightforward, yet tedious, to extend data types to full
  \textsc{gadt}s.
\item On the other hand, we have the type $[[exists as. t o= Q]]$, as
  introduced in Section~\ref{sec:what-it-looks-like}, together with
  the $\kpack$ and $\kunpack$ constructions. It acts as a sort of
  \textsc{gadt} with a single constructor (see rules~\rref*{E-Pack} and
  \rref*{E-Unpack}).
\item A more minor difference with OutsideIn is that we have an
  explicit \rref*{E-Sub} rule, while OutsideIn uses simple constraint
  entailment directly in the relevant rules. In OutsideIn, only the
  \rref*{E-Var} rule needed subsumption; we would also need it for the
  \rref*{E-Pack} rule as well. So we preferred having one shared
  dedicated rule.
\end{itemize}
%
\info{No substitution on $[[Q1]]$ in the $\kunpack$ rule, because there is
  only existential quantification.}


\section{Constraint inference}
\label{sec:type-inference}

Of course, the entire point of linear constraints is the promise that
the compiler can handle these constraints for the
programmer. Otherwise linear constraints are just plain Linear
Haskell~\cite{LinearHaskell} with an extra set of symbols.

In this section, we are still following OutsideIn~\cite[Section
5]{OutsideIn}. We make, however, an important simplification: we infer
constraints, but not types. That is the types in the syntax-directed
constraint generation algorithm of
Section~\ref{sec:constraint-generation}, typing rules are still declarative
rather than algorithmic. You can think of types as being inferred by
an external oracle.

In OutsideIn\,--\,and in \textsc{ghc}\,--\, on the other hand, in order to support the wide range of
features that modern Haskell boasts, the constraint solver is responsible not only
for type class constraint resolution, but also for type inference in the more
traditional sense. That is, the constraint solver deals with equality
constraints and does unification. Doing so allows seamless integration of \emph{e.g.}
\textsc{gadt}s with their local equality assumptions and type families with their
top-level equality axioms.

Not dealing with type inference greatly simplifies the presentation,
as will be evident in the rest of the section. We can make this
simplification for two reasons:
\begin{itemize}
\item Our system doesn't have equality constraints, and even when
  integrated into \textsc{ghc}, we do not allow equality constraints to be
  linear. This is because traditional unification algorithms are
  unsound for linear equalities as they will gladly reuse the same
  equality many times (or none at all). Linear equalities make sense
  (see \emph{e.g.}~\cite{shulman2018linear}, which puts linear
  equalities to great use), but they don't seem to lend themselves to
  automation.
\item We do not support, or intend to support, multiplicity
  polymorphism in constraint arrows. That is, the multiplicity of a
  constraint is always syntactically known to be either linear or
  unrestricted. This way, no equality constraints can interfere with
  constraint resolution.
\end{itemize}

\noindent
Our current focus is more narrow than a general typechecking algorithm for
all of \textsc{ghc}'s features.
Secondly,
As a consequence of these two restrictions (no linear equalities and no
multiplicity polymorphic constraints), type inference and (linear) class
constraint resolution are completely orthogonal. Therefore, the syntax-directed
constraint generation system presented in this section can legitimately assume
that type inference is solved elsewhere, greatly simplifying the presentation.


\subsection{Wanted constraints}
\label{sec:wanteds}

Like OutsideIn~\cite[Figure~11, page~37]{OutsideIn}, the constraint
generated in Section~\ref{sec:constraint-generation} have a richer
logical structure than the simple constraints. Let us follow
\textsc{ghc}'s terminology and call these \emph{wanted constraints}:
they are constraints which we \emph{want} to hold.
%
$$
\begin{array}{lcll}
  [[C]] & \bnfeq & [[Q]] \bnfor [[C1*C2]] \bnfor [[C1&C2]] \bnfor [[pi.(Q=>C)]]&
                                                                \text{Wanted constraints}
\end{array}
$$
%
The wanteds for linear constraints differ a little from the wanteds of
OutsideIn.
The first, superficial, difference is that our wanteds don't need existential
quantification, this is one of the benefits that not doing type inference
affords. Existential quantification, in OutsideIn, constrains which variables
can be unified with which; but we are not doing any unification.

The more interesting difference is the presence of this extra
$[[C1 & C2]]$ (read $[[C1]]$ \emph{with} $[[C2]]$). This is a
connective from linear logic: a remarkable feature of linear logic is
that every connective from classical or intuitionistic logic tends to
appear in two flavours in linear logic. Where $[[C1*C2]]$ is the
\emph{multiplicative} conjunction, $[[C1&C2]]$ is the \emph{additive}
conjunction. Both connectives are conjunctions, but they differ rather
dramatically in meaning: $[[C1*C2]]$ consumes both the (linear)
assumptions consumed by $[[C1]]$ and that consumed by $[[C2]]$. On the
other hand in $[[C1&C2]]$, both $[[C1]]$ and $[[C2]]$ \emph{must}
consume the same assumptions, and $[[C1 & C2]]$ will consume the same
assumptions once. The intuition, here, is that in $[[C1 & C2]]$, only
one of $[[C1]]$ or $[[C2]]$ will be eventually used. Much like if
$[[C1]]$ and $[[C2]]$ were branches in a $\kcase$-expression. Not
coincidentally, $[[C1&C2]]$ will be used to generate constraints for
$\kcase$-expressions.

Finally, implication constraints differ in two ways. First they are
linear implications (an implication which must consume its
assumption exactly once), where in OutsideIn they are intuitionistic implications.
Additionally, there is a scaling factor $[[pi]]$ on implication. This scaling prescribes how
many times the implication must be consumed,
and is needed when
extending scaling to all wanted constraints below. This scaling factor
does not, however, change linear implication into intuitionistic
implication. Concretely, $[[1.(omega.Q => C)]]$ is an intuitionistic
implication which must be consumed exactly once, while $[[omega.(1.Q =>
C)]]$ is a linear implication which can be consumed many times.
%
$$
\left\{
  \begin{array}{lcl}
    [[pi.(C1 * C2)]] & = & [[pi.C1 * pi.C2]] \\
    [[omega.(C1 & C2)]] & = & [[omega.C1 * omega.C2]] \\
    [[1.(C1 & C2)]] & = & [[C1 & C2]] \\
    [[pi.(rho.(Q => C))]] & = & [[(pi.rho).(Q => C)]]
  \end{array}
\right.
$$

\noindent
Like in Section~\ref{sec:constraint-domain}, we will typically drop
the scaling factor for implication when it is $[[1]]$ and write $[[Q
=> C]]$ for $[[1.(Q=>C)]]$.

The meaning of wanted constraints is given by the following,
inductive, entailment relation. This is a technical difference with
OutsideIn where the meaning of wanteds is given directly by the
constraint solver, whereas our solver
(Section~\ref{sec:constraint-solver}) has to be proved correct with
respect to this entailment relation. We found that, in the case of
linear logic, having this extra entailment relation made proofs much
simpler.

\drules[C]{$[[Q |- C]]$}
  {Wanted-constraint entailment}
  {Dom,Tensor,With,Impl}

\noindent
Note that it's always a simple constraint entails a wanted: we
generate wanted constraints, but programs only mention simple
constraints, and the left-hand side does come from the program. It is
possible to relax the syntax of the left-hand some,
though~\cite{quantified-constraints}, which current \textsc{ghc} does.

Before we move on to constraint generation proper, let us prove a few
technical, yet essential, lemmas about the wanted-constraint
entailment relation.

\begin{lemma}[Inversion]
  \label{lem:inversion}
  The inference rules of $[[Q |- C]]$ can be read bottom-up as well
  as top-down, as is required of $[[Q1 |- Q2]]$ in
  Definition~\ref{def:entailment-relation}. That is
  \begin{itemize}
  \item If $[[Q |- C1*C2]]$, then there exists $[[Q1]]$ and
    $[[Q2]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[Q]] = [[Q1 * Q2]]$
    \end{itemize}
  \item If $[[Q |- C1 & C2]]$, then $[[Q |- C1]]$ and $[[Q |- C2]]$.
  \item If $[[Q |- pi.(Q2 => C)]]$, then there exists $[[Q1]]$ such
    that
    \begin{itemize}
    \item $[[Q1 * Q2 |- C]]$
    \item $[[Q]] = [[pi.Q1]]$
    \end{itemize}
  \end{itemize}
\end{lemma}
\begin{proof}
  The cases $[[Q |- C1 & C2]]$ and $[[Q |- pi.(Q2 => C)]]$ are
  immediate, since there is only one rule (\rref*{C-With} and
  \rref*{C-Impl} respectively) which can have them as their
  conclusion.

  For $[[Q |- C1 * C2]]$ we have two cases:
  \begin{itemize}
  \item either it is the conclusion of a \rref*{C-Tensor} rule, and
    the result is immediate.
  \item or it is the result of a \rref*{C-Dom} rule, in which case we
    have $[[C1]]=[[Q1]]$, $[[C2]]=[[Q2]]$, and the result follows from
    the definition of the entailment relation.
  \end{itemize}

  This proof may look very fragile. After all in a system with
  quantified constraints~\cite{quantified-constraints}, such as the
  current implementation of \textsc{ghc}, there are rules with
  non-atomic conclusions which do not introduce a connective.

  Proofs where each non-atomic goal is the conclusion of a
  corresponding introduction rule has been called \emph{uniform}
  in~\cite{hh-ll}. They prove for a fragment of linear
  logic\,--\,which includes quantified constraints and linear
  generalisations thereof\,--\,that all provable sequents can be proved
  by a uniform proof. So this lemma, is, in fact, quite robust.
\end{proof}

\begin{lemma}[Promotion]
  \label{lem:wanted:promote}
  If $[[Q |- C]]$, then $[[pi.Q |- pi.C]]$
\end{lemma}

\begin{lemma}[Inversion of promotion]
  \label{lem:wanted:demote}
  If $[[Q |- pi.C]]$ then $[[Q' |- C]]$ and $[[Q]] = [[pi.Q']]$ for some $[[Q']]$
\end{lemma}

\subsection{Constraint generation}
\label{sec:constraint-generation}

The process of inferring constraints is split into two parts: generating
constraints, which we do in this section, then solving them in
Section~\ref{sec:constraint-solver}. To generate constraints we
introduce a judgement $[[G |-> e : t ~> C]]$ (defined in
Figure~\ref{fig:constraint-generation}). It is intended that $[[C]]$
is understood as an output of this judgement. The definition $[[G |->
e : t ~> C]]$ is syntax directed, so it can directly be read as an
algorithm, taking as input a term $[[e]]$ (together with some type
inference oracle, as discussed above) and returning a wanted
constraint $[[C]]$.

\info{See Fig.13, p39 of OutsideIn~\cite{OutsideIn}}

\info{Not caring about inferences simplifies $\kpack$ quite a bit, we
  are using the pseudo-inferred type to generate constraint. In a real
  system, we would need $\kpack$ to know its type (\emph{e.g.} using
  bidirectional type checking).}
\begin{figure}
  \centering\small
  \drules[G]{$[[G |-> e : t ~> C]]$}{Constraint generation}{Var, Abs,
    App, Case, Unpack, Pack, Let, LetSig}

  \caption{Constraint generation}
  \label{fig:constraint-generation}
\end{figure}

The rules of Figure~\ref{fig:constraint-generation} are a mostly
unsurprising translation of the rules of Figure~\ref{fig:typing-rules}
in the style of OutsideIn~\cite[Section 5.4]{OutsideIn}, the operation
on typing contexts, in particular are the same as in
Figure~\ref{fig:typing-rules}, and are not repeated here. Still, a few
remarks are in order
\begin{itemize}
\item The most conspicuous difference is that, as we explained in
  Section~\ref{sec:wanteds}, where OutsideIn uses a single kind of
  conjunctions, the \rref*{E-Case} rule needs two. OutsideIn
  accumulates constraints across branches, whereas we need to make
  sure that each branch of a $\kcase$-expression consumes the same
  constraints.

  This is easily understood in terms of the file example of
  Section~\ref{sec:introduction}:
  if a file is closed in one branch of a $\kcase$, it had better be closed in
the other branches too.
  Otherwise, after the case its state will be unknown to the type system.

  The reason this distinction is not made in an intuitionistic
  system like the original OutsideIn is somewhat elucidated by the
  fact that $[[omega.(C1&C2)]] = [[omega.C1 * omega.C2]]$. In other
  words, in a purely unrestricted context, linear constraints don't
  need two kinds of conjunctions either.
\item Just like OutsideIn, the introduction of constraints local to a
  premise in the rules of Figure~\ref{fig:typing-rules} corresponds to
  emitting an implicational constraint in
  Figure~\ref{fig:constraint-generation}.
\item However, the~\rref*{G-Let} rule doesn't have an implicational
  constraint corresponding to the local constraint of \rref{E-Let}:
  like in OutsideIn~\cite[Section 4.2]{OutsideIn}, a $\klet$ without a
  signature is not generalised. That being said, while not
  generalising is the easiest choice, here, it is not clear that it is
  the best in the presence of linear constraint, as we discuss in
  Section~\ref{sec:let-generalisation}
\item As we've observed throughout this section, not including type
  inference in the constraint generation algorithm significantly
  simplifies the presentation. There is no particular difficulty
  involved, however, in combining type inference and linear
  constraint; keeping in mind that equality constraints should never
  be linear.
\end{itemize}

% I [aspiwack] removed a section which I felt wasn't relevant. Leaving
% the label to avoid dead links.
\label{sec:constraint-generation-soundness}

The key property of the constraint-generation algorithm is that,
if the generated constraint is solvable, then we can indeed type the
term in the qualified type system of
Section~\ref{sec:qualified-type-system}. That is, we are not defining
a new type system but indeed an inference algorithm for the qualified
type system.

\begin{lemma}[Soundness of constraint generation]\label{lem:generation-soundness}
  For all $[[Q_g]]$ if $[[G |-> e : t ~> C]]$ and $[[Q_g |- C]]$ then
  $[[Q_g; G |- e : t]]$
\end{lemma}
\begin{proof}
Soundness is proved by induction on $[[G |-> e : t ~> C]]$, we only
present the two most interesting cases, the full proof can be found
in the supplementary material
  \begin{description}
  \item[\rref*{E-App}] We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q1]]$, $[[Q2]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[Q_g]] = [[Q1 * pi.Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[\rref*{E-Unpack}] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * Q' => C2]]$
    \item $[[Q_g |- C1 * Q' => C2]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q_1]]$, $[[Q_2]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 * Q' |- C2]]$
    \item $[[Q_g]] = [[Q1 * Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \end{description}
\end{proof}

\subsection{Constraint solving}
\label{sec:constraint-solver}

In this section,
let us build a \emph{constraint solver} whose purpose it is to prove
that $[[Q_g |- C]]$ holds, as required by Lemma~\ref{lem:generation-soundness}.

The constraint solver is represented by the following judgement
%
$$
[[ UCtx ; LCtx_i |-s C ~> LCtx_o]]
$$

The judgement
takes in two contexts: $[[UCtx]]$, which holds all the unrestricted
atomic constraints
(scaled by $[[omega]]$) and $[[LCtx_i]]$, which holds all the linear
atomic constraints. Due to the associativity and commutativity properties of $[[*]]$ we
can always factor any simple constraint into such a pair of contexts. The
unrestricted context $[[UCtx]]$ is an unordered set of constraints, while the
multiplicative contexts $[[LCtx_i]]$ and $[[LCtx_o]]$ are ordered lists of
constraints.

Linearity requires treating constraints as consumable resources. This
is what $[[LCtx_o]]$ is for: it is the list of the hypotheses of
$[[LCtx_i]]$ which haven't been consumed to prove $[[Q_w]]$. As
suggested by the notation, it is an output of the algorithm.

There are three simplifications compared to OutsideIn's solver:

\begin{itemize}
  \item As explained in Section~\ref{sec:constraint-generation}, our solver does
not need to do unification, so we do not return a substitution in the solver
judgement.
\item As discussed in Section~\ref{sec:constraint-domain}, we omit
  top-level constraints, for the sake of simplification.
  \item In case the wanteds can not be solved completely, OutsideIn also returns
a set of \emph{residual} constraints: these are the remaining subproblems to
solve. This set of residual constraints is simply quantified over in the type of
top-level function definitions that don't have signatures. The residuals are
essentially all of the wanteds that arise from the program that couldn't be
solved from the global givens. In our solver we omit these residual constraints,
because we never infer a type scheme for a let without a signature.

It is important to reiterate this
distinction: OutsideIn's residual constraints are constraints from the
goal that remain to be
solved, while our output constraints $[[LCtx_o]]$ are constraints from
the hypotheses that have yet to be used. OutsideIn's residuals are a
tool for type inference, while our output constraints are an internal
device for the solver.
\end{itemize}

Like OutsideIn~\cite[Section 5.5]{OutsideIn}, the main solver infrastructure
deals with solving the wanted constraints, which themselves are parameterised by
the simple-constraint domain.
To handle simple constraints, we will need  a domain-specific
\emph{simple-constraint solver} to be the algorithmic counterpart of the
abstract entailment relation of Section~\ref{sec:constraint-domain}. The
main solver will appeal to this simple-constraint solver when solving atomic
constraints.  The simple-constraint solver is represented by the following
judgement
%
$$
[[ UCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]
$$

It has a similar structure to the main solver, but only deals with atomic
constraints. Though the solver is parameterised in the particular choice of this
simple-constraint solver, we will give an instantiation in
Section~\ref{sec:simple-constr-solv}.

\begin{definition}[Simple constraint solver]
\label{def:solver-soundness} The simple constraint solver must verify
that whenever $[[UCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]$, we have
$[[LCtx_o]]\subseteq [[LCtx_i]]$ and $[[ctx(UCtx) * ctx(LCtx_i) ||-
pi.q * ctx(LCtx_o)]]$ (where the $[[UCtx]]$ and the $[[LCtx]]$ are
interpreted as simple constraints by taking the tensor of their atomic
constraints).
\end{definition}

\subsubsection{The solver}

To extend the solver to all wanted constraints, we use a linear proof
search algorithm based on the recipe given
by~\textcite{resource-management-for-ll-proof-search}. Figure~\ref{fig:constraint-solver}
presents the rules of the constraint solver.

\begin{figure}
  \centering\small
  \drules[S]{$[[UCtx ; LCtx_i |-s C_w ~> LCtx_o ]]$}{Constraint solving}{Simple, Mult, Add, ImplMany, ImplOne}
  \caption{Constraint solver}
  \label{fig:constraint-solver}
\end{figure}

\begin{itemize}
  \item The~\rref*{S-Mult} rule first proceeds by solving one side of a
conjunction first, then passing the output constraint to the other side. Note
that the unrestricted context is shared between both sides.
  \item The~\rref*{S-Add} rule handles additive conjunction. Here, the linear
constraints are also shared between the branches (since additive conjunction is
generated from case expressions, only one of them is actually going to be
executed). Note that both branches must consume exactly the same amount of
resources.
  \item Implications are handled by~\rref*{S-ImplOne} and~\rref*{S-ImplMany},
for solving linear and unrestricted wanted implications respectively. In
both cases, the assumption of the implication is split to the unrestricted
$[[omega . Q1]]$ and the linear $[[Q2]]$ parts (this can be done deterministically).
When solving a linear implication, we add the assumptions to their respective context, and
proceed with solving the conclusion. Importantly (see
Section~\ref{sec:simple-constr-solv} below), the linear assumptions are
added to the front of the list. The side condition requires that the output
context is a subset of the input context: this is to ensure that the implication
actually consumes its assumption and doesn't leak it to the ambient context.
That the correct one will be consumed is ensured by the~\rref*{AtomOneL} rule.
Solving unrestricted implications only allows the conclusion of the implication
to be solved using its own linear assumption, but none of the other linear
constraints. This is because unrestricted implications only use their own
assumption linearly, but use everything from the ambient context $[[omega]]$
times.

\end{itemize}

\subsubsection{A simple-constraint solver}
\label{sec:simple-constr-solv}

In the entire article, so far, the simple-constraint domain has been
an abstract parameter. In this section, though, let us give a concrete
domain which can support our examples.

For the sake of our examples, we really need very little: linear
constraints are simply names as far as the type checker is
concerned. So it is enough for the entailment relation
(Figure~\ref{fig:simpl-entailment}) to prove $[[q]]$ if and only if it
is already on the left hand side, with some restriction for linearity
of course.

\begin{figure}\centering\small
  \begin{subfigure}{\linewidth}
    \drules[Q]{$[[Q1 ||- Q2]]$}{Entailment relation}{Hyp,Prod,Empty}
    \caption{Entailment relation}
    \label{fig:simpl-entailment}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \drules[Simp]{$[[UCtx ; LCtx |-simp pi.q ~>
      LCtx_o]]$}{Simple-constraint solver}{AtomMany,AtomOneL,AtomOneU}
    \caption{Simple-constraint solver}
    \label{fig:simpl-solver}
  \end{subfigure}
  \caption{A stripped-down constraint domain}
  \label{fig:predicate-domain}
\end{figure}

The corresponding simple-constraint solver
(Figure~\ref{fig:simpl-solver}) is more interesting. The first thing
to notice is that the rules are deterministic: in any circumstances,
only one of the three rules can apply. This is a central tenet of
OutsideIn~\cite[Section 6.4]{OutsideIn}: it never makes guesses.

Figure~\ref{fig:simpl-solver} is also where the fact that the
$[[LCtx]]$ are lists comes into play. Indeed, \rref{Simp-AtomOneL}
takes great care to use the most recent occurrence of $[[q]]$
(remember that \rref{S-ImplOne} adds the new hypotheses on the top of
the list). To understand why, consider the following example:
\begin{code}
f :: FilePath -> IOL ()
f fp =   do  {  pack h <- openFile fp
             ;  let  {  cl :: Open h =>. IOL ()
                     ;  cl = closeFile h }
             ;  cl }
\end{code}
\noindent
In this example it is quite clear, that the programmer meant for
|closeFile| to use |Open h| introduced locally in |cl|. In fact this
is the only possible way to attribute constraints.

Another interesting feature of Figure~\ref{fig:simpl-solver} is that
no rule apply to solve a linear constraint if it appears both in the
unrestricted and the linear context.
Consider the following (contrived) \textsc{api}:

\begin{code}
class C

giveC :: (C => Int) -> Int
useC :: C =>. Int
\end{code}
\noindent
|giveC| gives an unrestricted copy of |C| to some continuation, while |useC|
uses |C| linearly. Now consider a consumer of this \textsc{api}:

\begin{code}
bad :: C =>. (Int, Int)
bad = (giveC useC, useC)
\end{code}

It is possible to give a type derivation to |bad| in the qualified type system
of Section~\ref{sec:qualified-type-system}. In fact, the constraint assignment
is unambiguous: the left-most |useC| must use the unrestricted |C|, while the
right-most must use the linear |C|. This assignment, however, would require of
the constraint solver to make a guess when solving the left-most |useC|. So in
the spirit of OutsideIn, |bad| is rejected.

\section{Desugaring}
\label{sec:desugaring}

\info{
  On unicity of tokens:

  A constraint like ``Open'' (no argument), does not make much sense
  to use as a linear constraint. Because if we can make it once, then
  we can make it omega times, and it's useless (it's similar to linear
  constants in linear haskell. They were not supported.)

  So, we're using something like ``Open f'' instead. Together with a
  an existential variable f. Then the \textsc{api} can ensure that there is a
  single token for this constraint at any given point. The \textsc{api} can
  however create several copies --- but then it's up to the \textsc{api} to
  make sure that the order of picking the constraints is
  computationally irrelevant. (Label constraints as computationally
  relevant? Then \textsc{ghc} could issue errors if there is computational
  relevance and it ends up making a choice.)

  [arnaud] maybe reject all programs which force the constraint solver
  to make an ordering choice
  
  [jp] still worried about the order of treating arguments: if we have
  writeFile f x >> writeFile f y then the arguments of could be
  handled in any order (for constraints!), and it's a shame to reject
  this.

  [arnaud] I don't think so, because in this case, writeFile f y is
  forced to use the most nested evidence, which is that given by
  writeFile f x
  
}

The semantics of our language is given by desugaring it into
a simpler core language: a mild variant of the $λ^q$
calculus from the Linear Haskell article~\cite{LinearHaskell}.

\subsection{The core calculus}
\label{sec:core-calculus}

% Our Core calculus is described in Figure~\ref{fig:core-grammar}
% (grammar) and Figure~\ref{fig:core-typing-rules} (typing rules).

\begin{figure}
  \centering\small
%if full
  $$
  \begin{array}{lcll}
    [[a]], [[b]] & \bnfeq & \ldots & \text{Type variables} \\
    [[x]], [[y]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[K]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[s]] & \bnfeq & [[forall as. t]] & \text{Type schemes} \\
    [[t]], [[u]] & \bnfeq & [[a]] \bnfor [[exists as. t o- u]] \bnfor [[t1 ->_pi t2]]
                            \bnfor [[T ts]] & \text{Types} \\
    [[G]], [[D]] & \bnfeq & [[empty]] \bnfor [[G, x:_pi s]] &
                                                              \text{Contexts} \\
    [[e]] & \bnfeq & [[x]] \bnfor [[K]] \bnfor [[\x. e]] \bnfor [[e1
                     e2]] \bnfor [[pack (e1, e2)]] & \text{Expressions}\\
                 &\bnfor & [[unpack (y,x)=e1 in
                           e2]] \bnfor [[case_pi e of { alts }]] &\\
                 &\bnfor & [[let_pi
                           x=e1 in e2]] \bnfor [[let_pi x : s = e1 in e2]] &
  \end{array}
  $$
  \caption{Grammar of the core calculus}
%else
  $$
  \begin{array}{lcll}
    [[s]] & \bnfeq & [[forall as. t]] & \text{Type schemes} \\
    [[t]], [[u]] & \bnfeq & ... \bnfor [[exists as. t o- u]] & \text{Types} \\
    [[e]] & \bnfeq & ... \bnfor [[pack (e1, e2)]] \bnfor [[unpack (y,x)=e1 in e2]] & \text{Expressions}
  \end{array}
  $$
  \caption{Grammar of the core calculus (subset)}
%endif
  \label{fig:core-grammar}
\end{figure}

\begin{figure}
  \centering\small
%if full
  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Var,Abs,App,Pack,Unpack,Let,Case}
  \caption{Core calculus type system}
%else
  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Pack,Unpack}
  \caption{Core calculus type system (subset)}
%endif
  \label{fig:core-typing-rules}
\end{figure}

The core calculus is a variant of the type system defined in
Section~\ref{sec:qualified-type-system} without implicit arguments.
Figure~\ref{fig:core-grammar} (grammar) and Figure~\ref{fig:core-typing-rules}
(typing rules) highlight the differences from the qualified system:
\begin{itemize}
  \item Type schemes $[[s]]$ do not support qualified types.
  \item Exisentially quantified types $[[exists as. t2 o- t1]]$ now represent a
(linear) pair of values. Accordingly, $\kpack$ and $\kunpack$ operate on pairs.
\end{itemize}

\noindent
The differences between our core calculus and $λ^q$ from Linear Haskell~\cite{LinearHaskell} are as follows
\begin{itemize}
\item We don't have multiplicity polymorphism.
\item We need, on the other hand, type polymorphism.
\item Polymorphism is implicit rather than explicit. This is not an
  essential difference but it simplifies the presentation.
\item We have existential types. These can be realised in regular Haskell as a
  family of \textsc{gadt}s.
\end{itemize}

\noindent
In addition, we shall assume the existence of data types
\begin{itemize}
\item $[[t1 * t2]]$ with sole constructor
  $[[ (,) : forall a b. a ->_1 b ->_1 a * b ]]$. We will write $[[(e1,
  e2)]]$ for $[[(,) e1 e2]]$.
\item $[[unit]]$ with sole constructor $[[() : unit]]$.
\item $[[Ur t]]$ with sole constructor $[[ Ur : forall a. a ->_omega
  Ur a]]$
\end{itemize}
\unsure{As I'm writing this I realise that there is no mention that
  data constructors are treated as variables. It should probably be
  somewhere in English, as well as in the definition of
  $[[x :_1 forall as. u \in G]]$}

\noindent
These are regular data types and constructors of the language, they
are consumed with $\kcase$.

\noindent
For the sake of concision, we will allow ourselves to write nested patterns in
case expressions. Desugaring nested patterns into atomic case expression is
routine.

\subsection{Inferred constraints}
\label{sec:ds:inferred-constraints}

Using Lemma~\ref{sec:constraint-generation-soundness} together with
Definition~\ref{def:solver-soundness} we know that if
$[[G |-> e : t ~> C]]$ and $[[UCtx ; LCtx |-s C~> empty ]]$, then
$[[ctx(UCtx) * ctx(LCtx)  ; G |- e : t]]$.

It only remains to desugar derivations of $[[Q;G|-e : t]]$ into the
Core Calculus. Let us dedicate the rest of this section to describe
this last step.

\subsection{From qualified to core}
\label{sec:ds:from-qualified-core}

\subsubsection{Evidence}
In order to desugar derivations of the qualified system to the core
calculus, we will use the classic technique known as evidence-passing
style\footnote{This technique is also often called dictionary-passing
  style because, in the case of type classes, evidences are
  dictionaries, and because type classes were the original form of
  constraints in Haskell.}.

To do so, we shall require some more material from
constraints. Namely, we will assume a type $[[Ev(q)]]$ for each atomic
constraint. It is extended to all simple constraints
%
$$
\left\{
  \begin{array}{lcl}
    [[Ev(1.q)]] & = & [[Ev(q)]] \\
    [[Ev(omega.q)]] & = & [[Ur (Ev(q))]] \\
    [[Ev(Empty)]] & = & [[unit]] \\
    [[Ev(Q1 * Q2)]] & = & [[Ev(Q1) * Ev(Q2)]]
  \end{array}
\right.
$$

It ought to be noted that $[[Ev(Q)]]$ is not technically well defined, as the
rules in Section~\ref{sec:constraint-domain} define the syntax as being
quotiented by associativity and commutativity of the tensor product, and
idempotence of unrestricted constraints. But core language data types (or
Haskell's for that matter) are not so quotiented. It's not actually hard to fix
the imprecision: give a name to each atomic constraint, and operate on canonical
representatives of the equivalence classes.  This is actually essentially how
\textsc{ghc} deals with constraints today. It is also the mechanism that our
prototype implementation (see Section~\ref{sec:implementation}) uses. However,
we have preferred keeping this section a little imprecise, in order to save the
rest of the article from the non-trivial extra tedium that the more precise
presentation entails.

Furthermore we shall require that for every $[[Q1]]$ and $[[Q2]]$
such that $[[Q1 ||- Q2]]$, there is a (linear) function
$[[Ev(Q1 ||- Q2) : Ev(Q1) ->_1 Ev(Q2)]]$.

We will need to define one more device. Namely, we'll need a way to turn every
$[[Ev(omega.Q)]]$ into an $[[Ur(Ev(Q))]]$. For any
$[[e : Ev(omega.Q)]]$, we shall write $[[urify(Q;e) :
Ur(Ev(omega.Q))]]$. As a shorthand, particularly useful in nested
patterns, we will write $[[case_pi e of {urified(Q;x) -> e'}]]$ for
$[[case_pi urify(Q;e) of {Ur x -> e'}]]$.
%
$$
\left\{
  \begin{array}{lcl}
    [[urify(Empty;e)]]& = & [[case_1 e of {() -> Ur ()}]] \\
    [[urify(1.q;e)]] & = & [[e]] \\
    [[urify(omega.q;e)]] & = & [[case_1 e of {Ur x -> Ur (Ur x)}]] \\
    [[urify(Q1*Q2;e)]] & = & [[case_1 e of {(urified(Q1;x), urified(Q2;y)) -> Ur (x,y)}]]
  \end{array}
\right.
$$

\noindent
We will often omit the $[[Q]]$ in $[[urify(Q;e)]]$, and write
$[[urify(e)]]$ when it can be easily inferred from the context.

Let us now define a family of functions $[[Ds(Hole)]]$ to translate
the type schemes, types, and typing derivations of the qualified system into the
types, type schemes, and terms of the Core Calculus.

\subsubsection{Translating types}
Type schemes $[[s]]$ are translated by turning the implicit argument $[[Q]]$
into a function that takes the evidence of $[[Q]]$.
%
$$
\left\{
  \begin{array}{lcl}
    [[Ds(forall as. Q =o t)]] & = & [[forall as. Ev(Q) ->_1 Ds(t)]] \\
  \end{array}
\right.
$$

\noindent
Translating types $[[t]]$ proceeds as expected.
%
$$
\left\{
  \begin{array}{lcl}
    [[Ds(t1 ->_pi t2)]] & = & [[Ds(t1) ->_pi Ds(t2)]] \\
    [[Ds(exists as. t o= Q)]] & = & [[exists as. Ds(t) o- Ev(Q)]]
  \end{array}
\right.
$$

\subsubsection{Translating terms}
Let us finally build, given a derivation $[[Q;G |- e : t]]$, an expression
$[[Ds(z;Q;G |- e : t)]]$, such that
$[[G, z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ (for some variable
$[[z]]$). Even though we abbreviate the derivation as its
conclusion, the translation is defined recursively on the
whole typing derivation, in particular we have access to typing rule
premises in the body of the definition.
%
%if full
$$
\left\{
  \begin{array}{lcl}
    [[ Ds(z;Q;G |- x : u[ts/as]) ]] & = & [[ x z ]] \\
    [[ Ds(z;Q;G |- \x.e : t1 ->_pi t2) ]] & = & [[ \x. Ds(z;Q;G,x:_pi t1
                                              |- e : t2) ]] \\
    [[ Ds(z;Q1*Q2; G1+G2 |- e1 e2 : t) ]]
        & = & [[ case_1 z of { (z1, z2) -> Ds(z1;Q1;G1 |- e1 : t1 ->_1 t)
              Ds(z2;Q2;G2 |- e2 : t1) } ]] \\
    [[ Ds(z;Q1*omega.Q2; G1+omega.G2 |- e1 e2 : t) ]]
        & = & [[ case_1 z of { (z1, urified(z2))
              -> Ds(z1;Q1;G1 |- e1 : t1 ->_omega t)
              Ds(z2;Q2;G2 |- e2 : t1) } ]] \\
    [[ Ds(z;Q * Q1[us/as];G |- pack e : exists as. t o=
    Q1)]]
        & = & [[ case_1 z of { (z', z'')
              -> pack (z'', Ds(z'; Q ; G |- e : t[us/as]))} ]] \\
    [[ Ds(z;Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t) ]]
        & = & [[ case_1 z of { (z1, z2) -> unpack (z',x) =
              Ds(z';Q1;G1 |- e1 : exists as. t1 o= Q) in let_1 z2' =
              (z2,z') in Ds(z2';Q2 * Q;G2,x:_1 t1 |- e2 : t)
              } ]] \\
    [[ Ds(z;Q1 * Q2 ;G1+G2 |- let_1 x = e1 in e2 : t) ]]
        & = & [[ case_1 z of { (z1, z2) -> let_1 x : Ev(Q) ->_1 t1 = Ds(z1;Q1*Q;G1 |- e1 :
              t1) in Ds(z2;Q2;G2,x:_1 t1 |- e2 : t)} ]] \\
    [[ Ds(z;omega.Q1 * Q2 ;omega.G1+G2 |- let_omega x = e1 in e2 : t) ]]
        & = & [[ case_1 z of { (urified(z1), z2) -> let_omega x : Ev(Q) ->_1 t1 = Ds(z1;Q1*Q;G1 |- e1 :
              t1) in Ds(z2;Q2;G2,x:_omega t1 |- e2 : t)} ]] \\
    [[ Ds(z;Q1 * Q2 ;G1+G2 |- let_1 x : forall as. Q =o t1 = e1 in e2 : t) ]]
        & = & [[ case_1 z of { (z1, z2) -> let_1 x : forall as. Ev(Q) ->_1 t1 = Ds(z1;Q1*Q;G1 |- e1 :
              t1) in Ds(z2;Q2;G2,x:_1 forall as. Q =o t1 |- e2 : t)} ]] \\
    [[ Ds(z;omega.Q1 * Q2 ;omega.G1+G2 |- let_omega x : forall as. Q =o t1 = e1 in e2 : t) ]]
        & = & [[ case_1 z of { (urified(z1), z2) -> let_omega x : forall as. Ev(Q) ->_1 t1 = Ds(z1;Q1*Q;G1 |- e1 :
              t1) in Ds(z2;Q2;G2,x:_omega t1 |- e2 : t)} ]] \\
    [[ Ds(z;omega.Q1*Q2;omega.G1+G2 |- case_1 e of { alts } : t) ]]
        & = & [[ case_1 z of { (urified(z1), z2) -> case_1 Ds(z1;Q1;G1 |-
              e : T ts) of { < K xsi -> Ds(z2; Q2; G2, < xi :_(pi.pii)
              ui[ts/as] > |- ei : t)>}} ]] \\
    [[ Ds(z;Q1*Q2;G1+G2 |- case_omega e of { alts } : t) ]]
        & = & [[ case_1 z of { (z1, z2) -> case_omega Ds(z1;Q1;G1 |-
              e : T ts) of { < K xsi -> Ds(z2; Q2; G2, < xi :_(pi.pii)
              ui[ts/as] > |- ei : t)>}} ]] \\
    [[ Ds(z;Q;G |- e : t) ]] & = & [[ let_1 z' = Ev(Q ||- Q1) z in
                                   Ds(z';Q1;G |- e : t) ]]
  \end{array}
\right.
$$
%else
\noindent
We present some of the interesting cases.

$$
\left\{
\begin{array}{lcl}
  [[ Ds(z;Q;G |- x : u[ts/as]) ]] & = & [[ x z ]] \\
  [[ Ds(z;Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t) ]]
      & = & [[ case_1 z of { (z1, z2) -> unpack (z',x) =
            Ds(z';Q1;G1 |- e1 : exists as. t1 o= Q) in let_1 z2' =
            (z2,z') in Ds(z2';Q2 * Q;G2,x:_1 t1 |- e2 : t)
            } ]] \\
  [[ Ds(z;Q;G |- e : t) ]] & = & [[ let_1 z' = Ev(Q ||- Q1) z in
                                 Ds(z';Q1;G |- e : t) ]] \\
  ...

\end{array}
\right.
$$

\noindent
The cases correspond to the~\rref*{E-Var},~\rref*{E-Unpack}, and~\rref*{E-Sub} rules, respectively.
Variables are stored with qualified types in the environment, so they get
translated to functions that take the evidence as argument. Accordingly, the evidence
is inserted  passing $[[z]]$ as an argument.
Handling $\kunpack$ requires splitting the context into two: $[[e1]]$ is desugared as a pair, and the evidence
it contains is passed to $[[e2]]$. Finally, subsumption summons the function corresponding to the entailment relation $[[Q ||- Q1]]$
and applies it to $[[z]]$ : $[[Ev(Q)]]$ then proceeds to desugar $[[e]]$ with the resulting evidence for $[[Q1]]$.
Crucially, since $[[Ds(Hole)]]$ is defined on \emph{derivations}, we can access the premises used in the rule.
Namely, $[[Q1]]$ is available in this last case from the~\rref*{E-Sub} rule's premise.
%endif

It is straightforward by induction, to verify that, indeed,
$[[G, z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ as
announced.\unsure{a few more closing words would be welcome.}


\section{Implementation}
\label{sec:implementation}

We have written a prototype implementation of linear constraints on top of \textsc{ghc} 9.1, a version that already
ships with the LinearTypes extension. Function arrows (|->|) and context arrows
(|=>|) share the same internal representation in the typechecker, differentiated
only by a boolean flag. Thus, the LinearTypes implementation effort has already
laid down the bureaucratic ground work of annotating these arrows with
multiplicity information.

The key changes affect constraint generation and constraint solving. Constraints
are now annotated with a multiplicity, scaled according to the usage environment
from which they arise. With LinearTypes, \textsc{ghc} already does scaling for the usage
of term variables, we simply needed to modify the scaling function to capture all
the generated constraints and re-emit a scaled version of them -- a fairly local
change.

The constraint solver maintains a set of givens, called the \emph{inert} set,
which corresponds to the $[[UCtx]]$ and $[[LCtx]]$ contexts in our solver
judgements in Section~\ref{sec:constraint-solver}. A property of the inert set
is that no pairwise interactions between the constraints contained in it can
happen. These interactions are dictated by the constraint domain. For example,
equality constraints interact with other constraints by applying a substitution.

The treatment of implication constraints is of particular interest.
Implications, by their nature, introduce assumptions which do not necessarily
hold in the outer context, therefore recording these assumptions in the inert
set is a destructive operation. To ensure proper scoping, \textsc{ghc} creates a fresh
copy of the inert set for each nested implication it solves, so these
destructive operations do not leak out. We modify the inert set so that for each
constraint stored in it, the level (or depth) of the implication is recorded
alongside it. Each interaction with nested assumptions (which might give rise to
additional derived givens) is recorded at the appropriate level and
multiplicity (decomposing a constraint tuple into its constituent parts is done
by the simplifier, which must now record the multiplicities of the components).

Atomic wanteds are then solved by finding a matching given in the inert set (or
a top-level given, which is not relevant to linear constraints). When a linear
given is used to solve a linear wanted, our prototype removes the given from the
inert set so it can not be used again. Before, the inert set only held a single
copy of each given, but now it must hold multiple copies of linear givens,
together with their implication level. When solving an atomic wanted, the
matching given with the largest implication level (i.e. the innermost given) is
selected, as per the \rref*{Simp-AtomOneL} rule.

When an implication is finally solved, we must check that every linear
constraint introduced in this implication was consumed, which is done by
checking that the level of every linear constraint in the inert set is less than
the implication.

When a linear equality constraint is encountered, it is automatically promoted
to an unrestricted one and handled accordingly. This may happen in many
different scenarios, as the entailment relation of \textsc{ghc}'s constraint domain does
produce many equalities, thus we need to ensure they are turned into
unrestricted before interacting with the inert set. \textsc{ghc} core already represents
the equality constraint as a boxed type, so we can simply modify it to store an
unrestricted payload.

As constraint solving proceeds, the compiler pipeline constructs a term in the
intrinsically typed core language System FC~\cite{system-fc}, also
known as \textsc{ghc} Core.
In Core, type class constraints are turned into
explicit evidences (see Section~\ref{sec:desugaring}). Thanks to being fully
annotated, Core has decidable typechecking which is useful in sanity
checking modifications to the compiler. Thus, the soundness of our
implementation is verified by the Core typechecker, which already supports
linearity.

\section{Extensions}
\label{sec:design-decisions}

\csongor{intro}

\subsection{let generalisation}
\label{sec:let-generalisation}

As discussed in Section~\ref{sec:constraint-generation}, the~\rref*{G-Let} rule
of our constraint generator does not generalise the type of let bindings.
Not doing let generalisation is in line with \textsc{ghc}'s existing
behaviour~\cite[Section 4.2]{OutsideIn}. There, this behaviour was guided
by concerns around inferring type variables, which become harder in the
presence of local equality assumptions (that is, \textsc{gadt} pattern
matching).

In this section, however, we argue that generalising over linear constraints
may, in fact, improve user experience.
Let us revisit the |firstLine| example from Section~\ref{sec:introduction}, but
this time, instead of executing |closeFile| directly, we assign it to a variable
in a let binding:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp =   do  {  pack' h <- openFile fp
                     ;  let closer = closeFile h
                     ;  pack' xs <- readLine h
                     ;  closer
                     ;  return xs }
\end{code}
This program looks reasonable; however, it is rejected. The type of
|closer| is |IOL ()|, which means that the definition of |closer|
consumes the linear constraint |Open h|. So, by the time we try to
|readLine h|, the constraint is no longer available.

What the programmer really meant, here, was for |closer| to have type
|Open h =o IOL ()|. After all a |let| definition is not part of the
sequence of instructions, it is just a definition for later; it is not
supposed to consume the current state of the file. With no let
generalisation, the only way to give |closer| the type |Open h =o IOL
()| is to give |closer| a type signature. In current \textsc{ghc}, we
can't write that signature down, since the type variable |h| is not
bound in the program text, and there is no syntax for binding it
(though see~\cite{variables-in-patterns} for a proposed syntax). But
even ignoring this, it would be rather unfortunate that the default
behaviour of |let|, in presence of linear constraints, almost never be
what the programmer wants.

To handle $\klet$-generalisation, let us consider the following rule
%
$$
\drule{G-LetGen}
$$

\noindent
This rule is non-deterministic, as it requires finding $[[Q_r]]$ and
$[[Q]]$. We can modify the constraint solver of
Section~\ref{sec:constraint-solver} to find $[[Q_r*Q]]$: it's the
residual constraint, from OutsideIn, that we have omitted. But we
still have to determine how to split the residual into $[[Q_r]]$ and
$[[Q]]$. What we would like to say is ``$[[Q]]$ is the set of linear
constraints''. But it's not clear how to make it formal.

Any predictable strategy would do: as long as it's an instance of the
\rref*{G-LetGen} rule, constraint generation will be sound. Experience
will tell whether we can find a better suited strategy than the current
one, which never generalise any constraint.

\subsection{Empty cases}
\label{sec:empty-cases}

Throughout the article we have assumed that $\kcase$-expressions
always have a non-empty list of alternatives. This is, incidentally,
also how Haskell originally behaved; though \textsc{ghc} now has an
|EmptyCase| extension to allow empty lists of alternatives.

Not allowing empty lists of alternatives is, therefore not terrible in
principle. Though it makes empty types more awkward than they need to
be, and, of course, to support the entirety of \textsc{ghc}, we will
need to support empty lists of alternatives.

The reason why it has been omitted from the rest of the article is
that generating constraints for an empty requires an $0$-ary version
of $[[C1&C2]]$, usually written $[[Top]]$ in Linear Logic. The
corresponding entailment rule would be
$$
\drule{C-Top}
$$

\noindent
That is $[[Top]]$ is unconditionally true, and can consume any number
of linear given constraints. The \rref*{C-Top} rule thus induces a
considerable amount of non-determinism. Eliminating the
non-determinism induced by $[[Top]]$ is ultimately
what~\cite{resource-management-for-ll-proof-search} builds up
to. Their methods can be adapted to the constraint solver of
Section~\ref{sec:constraint-solver} without any technical
difficulty. We chose, however, to keep empty cases out the
presentation because has a very high overhead and would distract from
the point. Instead, we refer readers
to~\cite[Section 4]{resource-management-for-ll-proof-search} for a
careful treatment of $[[Top]]$.

\section{Related work}
\label{sec:related-work}

\paragraph{Rust}

The memory ownership example of Section~\ref{sec:memory-ownership} is
strongly inspired by Rust. There seems to be something inescapable
about Rust's model (itself inspired by C++'s move semantic) of owned
and borrowed reference as the \textsc{api} in
Section~\ref{sec:memory-ownership} follows naturally from the
requirements (namely a pure interface for a mutable array, and the
ability to freeze arrays into an immutable but shareable). It is not
clear that any other \textsc{api} could be essentially different.

Rust is built around ownership and borrows for memory
management. As a consequence, they have a much more convenient syntax
than Linear Haskell with linear constraint can propose. Though Rust's
convenient syntax comes at the price that it is almost impossible to
write tail-recursive functions in Rust, which is surprising from the
perspective of functional programming.

On the other hand, the focus of Linear Haskell as well as linear
constraints is to provide programmers with tools create safe
interfaces in libraries. The language itself is agnostic about what
linear constraints mean. If linear constraints don't have anywhere
near the convenience of Rust's syntax, we expect that they will
support a greater variety of abstraction. It is worth noting, though,
that Rust programmers have come up with varied abstraction leveraging
borrowing which go beyond memory management (for instance, safe file
handling).

Yet, if Section~\ref{sec:memory-ownership} shows how to make a
Rust-like \textsc{dsl} in Haskell with linear constraints. It is not
clear that it is possible at all to make a Linear-Haskell-like
\textsc{dsl} in Rust. So, presumably, linear constraints are indeed
more general.

\paragraph{Languages with capabilities}

Both Mezzo~\cite{mezzo-permissions} and
\textsc{ats}~\cite{AtsLinearViews} were big inspiration for the design
of linear constraints. Of the two, Mezzo is more specialised, being
entirely built around its system of capabilities, and \textsc{ats} is
the closest to our system with an explicit appeal to linear logic, and
the capabilities (known as \emph{stateful views}) being a part of a
bigger language. However, \textsc{ats} doesn't have full inference of
capabilities, which the programmer may have to fill in.

Other than that, both systems have a lot of similarities. They have a
finer grain capability system than is expressible in Rust (or our
Section~\ref{sec:memory-ownership}) which makes it possible to change
the type of a reference cell on write. They also eschew scoped
borrowing in favour of more traditional read and write access. In
exchange, neither Mezzo nor \textsc{ats} support $O(1)$ freezing like
in Section~\ref{sec:memory-ownership} and Rust.

Mezzo, being geared towards functional programming does support
freezing, but freezing a nested data structure requires traversing it.
As far as we know, \textsc{ats} doesn't support
freezing. \textsc{Ats} is more oriented towards system programming,
and, interestingly, supports pointer arithmetic.

We chose an example in the style of rust for
Section~\ref{sec:memory-ownership} because freezing arrays of arrays
in $O(1)$ was one of the initial motivations of this article. However,
a Mezzo or \textsc{ats} style of capabilities could certainly be
encoded within linear constraints.

Linear constraints are more general than either Mezzo or \textsc{ats},
but at the same time our inference algorithm is considerably simpler
than either, and at the same time supporting a richer set of
constraint (for instance, being based on OutsideIn makes linear
constraints compatible with \textsc{gadt}s for almost free). This
simplicity is a benefit of abstracting over the simple-constraint
domain. In fact, it should be possible to see Mezzo or \textsc{ats} as
particular instantiations of the simple-constraint domain, with linear
constraints providing the general inference mechanism.

Though both Mezzo and \textsc{ats} have an advantage that we don't:
they assume that their instructions are properly sequenced, whereas
basing linear constraints on Haskell, a lazy language, we are forced
to make sequencing explicit in \textsc{api}s.

\paragraph{Logic programming}

There are a lot of commonalities between \textsc{ghc}'s constraint and
logic programs. Traditional type classes can be seen as Horn Clauses
programs, much like Prolog's programs. \textsc{Ghc} put restriction on
the solutions of this program in order to avoid backtracking for speed
and predictability.

The recent addition of quantified
constraints~\cite{quantified-constraints} extends type class
resolution to Hereditary Harrop programs. A generalisation of the
Hereditary Harrop fragment to linear logic, described in~\cite{hh-ll},
is the foundation of the Lolli language. Together with this fragment,
the authors coin the notion of \emph{uniform} proof. A fragment where
uniform proofs are complete supports goal-oriented proof search, like
Prolog does.

Uniformity is used implicitly in the proof of
Lemma~\ref{lem:inversion}, which, in turn is used in the proof of the
soundness lemma~\ref{lem:generation-soundness}. This seems to indicate
that goal-oriented search is baked in the definition of OutsideIn. An
immediate consequence of this observation, however, is that the fact
that that the fragment of linear logic described in~\cite{hh-ll} (and
for which~\cite{resource-management-for-ll-proof-search} provides a
refined search strategy) contains the Hereditary Harrop fragment of
intuitionistic logic guarantees that quantified constraints don't
break our proofs.

\section{Conclusion}
\label{sec:conclusion}

This article shows how a simple linear type system like that of Linear
Haskell can be extended with an inference mechanism which lets the
compiler manage some of the additional complexity of linear types
instead of the programmer. Linear constraints narrow the gap between a
general purpose linearly typed language and language with dedicated
linear-like typing disciplines such as Rust's, Mezzo's, or
\textsc{ats}'s.

In some cases, like the file example of Section~\ref{sec:introduction},
linear constraints are a mere convenience that reduce line noise and
make code more idiomatic. But the memory management \textsc{api} of
Section~\ref{sec:memory-ownership} is not easily feasible without
linear constraints. Surely, ownership proofs could be managed manually, but
it is hard to imagine a circumstance where this tedious task would be worth
the cost.

And, this, really, is the philosophy of linear constraints: lowering
the cost of linear types so that more theoretical application become
practical.

\printbibliography

\appendix

\section{Proofs of the lemmas}
\label{sec:appendix:proofs-lemmas}

\setcounter{subsection}{3}
\subsection{Lemmas on the qualified type system}
\label{sec:appendix:qual-type-syst}

\begin{proof}[Proof of Lemma~\ref{lem:q:promotion}]
  Let us prove this lemma by induction on the syntax of $[[Q2]]$:
  \begin{itemize}
  \item If $[[Q2]] = [[rho.q]]$, then $[[pi.Q1 ||- (pi.rho).q]]$ holds by
    Definition~\ref{def:entailment-relation}.
  \item If $[[Q2]] = [[Q2' * Q2'']]$, then, by Definition~\ref{def:entailment-relation}, we know that
    $[[Q1]] = [[Q1' * Q1'']]$ for some $[[Q1']]$ and $[[Q1'']]$, and
    that
    $[[Q1' ||- Q2']]$ and $[[Q1'' ||- Q2'']]$. By induction hypothesis,
    we have
    $[[pi.Q1' ||- pi.Q2']]$ and $[[pi.Q1'' ||- pi.Q2'']]$. From which it
    follows that
    $[[pi.Q1 ||- pi.Q2]]$.
  \item If $[[Q2]]=[[Empty]]$, then, by
    Definition~\ref{def:entailment-relation}, we know that
    $[[Q1]]=[[omega.Q1']]$, for some $[[Q1']]$
    and the result follows from Lemma~\ref{lem:simples:monoid-action},
    since $[[pi.Q1]]=[[pi.(omega.Q1')]]=[[omega.Q1']]$.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:q:scaling-inversion}]
  Let us prove this lemma by induction on the syntax of $[[Q2]]$
  \begin{itemize}
  \item If $[[Q2]] = [[rho.q]]$, then, by Definition~\ref{def:entailment-relation}, there exists
    $[[Q1']]$ such that $[[Q1]]=[[pi.Q1']]$ and $[[Q1' ||- rho.q]]$.
  \item If $[[Q2]] = [[Q2' * Q2'']]$, then, by Definition~\ref{def:entailment-relation}, we know
    that
    $[[Q1]] = [[Q1' * Q1'']]$ for some $[[Q1']]$ and $[[Q1'']]$, and
    that
    $[[Q1' ||- pi.Q2']]$ and $[[Q1'' ||- pi.Q2'']]$ (remember that, by
    definition, $[[pi.Q2]] = [[pi.Q2' * pi.Q2'']]$). By induction hypothesis,
    we have
    constraints $[[Q']]$ and $[[Q'']]$, such that $[[Q1']] =
    [[pi.Q']]$ and $[[Q1'']] = [[pi.Q'']]$, and $[[Q' ||- Q2']]$ and
    $[[Q'' ||- Q2'']]$.
    It follows that $[[Q1]] = [[pi.(Q' * Q'')]]$ and
    $[[Q' * Q'' ||- Q2]]$.
  \item If $[[Q2]]=[[Empty]]$, then, since $[[pi.Empty]]=[[Empty]]$, by
    Definition~\ref{def:entailment-relation}, $[[Q1]]=[[Empty]]$ and
    the result is immediate.
  \end{itemize}
\end{proof}

\begin{lemma}\label{lem:simples:monoid-action}
  The following equality holds $[[pi.(rho.Q)]]=[[(pi.rho).Q]]$
\end{lemma}
\begin{proof}
  This result can be proved by a straightforward induction on the
  structure of $[[Q]]$.
\end{proof}

\setcounter{subsection}{4}
\subsection{Lemmas on constraint inference}
\label{sec:appendix:constraint-inference}

\begin{proof}[Proof of Lemma~\ref{lem:wanted:promote}]
  By induction on the syntax of $[[C]]$
  \begin{itemize}
  \item If $[[C]]=[[Q']]$, then the result follows
    from Lemma~\ref{lem:q:promotion}
  \item If $[[C]]=[[C1*C2]]$, then we can prove the result like we
    proved the corresponding case in Lemma~\ref{lem:q:promotion},
    using Lemma~\ref{lem:inversion}.
  \item If $[[C]]=[[C1&C2]]$, then we the case where $[[pi]]=[[1]]$ is
    immediate, so we can assume without loss of generality that
    $[[pi]]=[[omega]]$, and, therefore, that $[[pi.C]] = [[pi.C1 *
    pi.C2]]$.
    By Lemma~\ref{lem:inversion}, we have that $[[Q|-C1]]$ and
    $[[Q|-C2]]$; hence, by induction, $[[omega.Q |- omega.C1]]$ and
    $[[omega.Q |- omega.C1]]$.
    Then, by definition of the entailment relation, we have $[[omega.Q
    * omega.Q |- omega.C1 * omega.C2]]$, which concludes,
    since $[[omega.Q]] = [[omega.Q * omega.Q]]$.
  \item If $[[C]]=[[rho.(Q1 => C')]]$, then by
    Lemma~\ref{lem:inversion}, there is a $[[Q']]$ such that
    $[[Q]]=[[pi.Q']]$ and $[[Q'*Q1 |- C']]$. Applying
    rule~\rref*{C-Impl} with $[[pi.rho]]$, we get
    $[[(pi.rho).Q' |- (pi.rho).(Q1 => C')]]$.

    In other words: $[[pi.Q |- pi.(rho.(Q=>C))]]$ as expected.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:wanted:demote}]
  By induction on the syntax of $[[C]]$
  \begin{itemize}
  \item If $[[C]]=[[Q']]$, then the result follows from
    Lemma~\ref{lem:q:scaling-inversion}
  \item If $[[C]]=[[C1*C2]]$, then we can prove the result like we
    proved the corresponding case in
    Lemma~\ref{lem:q:scaling-inversion} using
    Lemma~\ref{lem:inversion}.
  \item If $[[C]]=[[C1&C2]]$, then we the case where $[[pi]]=[[1]]$ is
    immediate, so we can assume without loss of generality that
    $[[pi]]=[[omega]]$, and, therefore, that
    $[[pi.C]] = [[pi.C1 * pi.C2]]$. By Lemma~\ref{lem:inversion},
    there exist $[[Q1]]$ and $[[Q2]]$ such that $[[Q1|- omega.C1]]$,
    $[[Q2|- omega.C2]]$ and $[[Q]]=[[Q1 * Q2]]$. By induction
    hypothesis, we get $[[Q1]] = [[omega.Q1']]$ and $[[Q2]] = [[omega.Q2']]$
    such that $[[Q1' |- C1]]$ and $[[Q2' |- C2]]$. From which it
    follows that $[[omega.Q1'*omega.Q2' |- C1]]$ and
    $[[omega.Q1'*omega.Q2' |- C1]]$ (by
    Lemma~\ref{lem:wanteds:weakening}) and, finally,
    $[[Q]]=[[omega.Q]]$ (by Lemma~\ref{lem:wanteds:module-action})
    and $[[Q |- C1 & C2]]$.
  \item If $[[C]]=[[rho.(Q1 => C')]]$, then
    $[[pi.C]] = [[(pi.rho). (Q1 => C')]]$. The result follows
    immediately by Lemma~\ref{lem:inversion}.
  \end{itemize}
\end{proof}

\begin{proof}[Full proof of Lemma~\ref{lem:generation-soundness}]
  By induction on $[[G |-> e : t ~> C]]$
  \begin{description}
  \item[\rref*{E-Var}] We have
    \begin{itemize}
    \item $[[x :_1 forall as. Q =o u \in G]]$
    \item $[[G |-> x : u[ts/as] ~> Q[ts/as] ]]$
    \item $[[Q_g |- Q[ts/as] ]]$
    \end{itemize}
    Therefore, by rules~\rref*{E-Var} and~\rref*{E-Sub}, it follows
    immediately that $[[Q_g ; G |- x : u[ts/as] ]]$
  \item[\rref*{E-Abs}] We have
    \begin{itemize}
    \item $[[G |-> \x. e : t0 ->_pi t ~> C]]$
    \item $[[Q_g |- C]]$
    \item $[[G, x:_pi t0 |-> e : t ~> C]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_g; G, x:_pi t0 |- e : t]]$
    \end{itemize}
    From which follows that $[[Q_g; G |- \x. e : t0 ->_pi t]]$.
  \item[\rref*{E-Let}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x = e1 in e2 : t ~> pi.C1 * C2]]$
    \item $[[Q_g |- pi.C1 * C2]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q1]]$ and $[[Q_2]]$ such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 |- C2]]$
    \item $[[Q_g]] = [[pi.Q_1 * Q_2]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_1 ; G1 |- e1 : t1]]$
    \item $[[Q_2; G2, x:_pi  t1 |- e1 : t1]]$
    \end{itemize}
    From which follows that $[[Q_g; pi.G1+G2 |- let_pi x = e1 in e2 :
    t]]$.
  \item[\rref*{E-LetSig}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x : forall as. Q =o t1 = e1 in e2 : t ~>
      C2 * pi.(Q => C1)]]$
    \item $[[Q_g |- C2 * pi.(Q => C1)]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \item $[[G2, x:_pi forall as. Q =o t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q1]]$, $[[Q2]]$ such
    that
    \begin{itemize}
    \item $[[Q2 |- C2]]$
    \item $[[Q1*Q |- C]]$
    \item $[[Q_g]] = [[pi.Q1*Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1*Q;G1 |- e1 : t1]]$
    \item $[[Q2; G2, x:_pi forall as. Q =o t1 |- e2 : t]]$
    \end{itemize}
    Hence $[[Q_g; pi.G1+G2 |- let_pi x : forall as. Q =o t1 = e1 in e2 : t]]$
  \item[\rref*{E-App}] \info{Most of the linearity problems are in the App
      rule. Unpack is also relevant.}
    We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q1]]$, $[[Q2]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[Q_g]] = [[Q1 * pi.Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[\rref*{E-Pack}] We have
    \begin{itemize}
    \item $[[omega.G |-> pack e : exists as. t o= Q ~> omega.C * Q[ts/as] ]]$
    \item $[[Q_g |- C * Q[us/as] ]]$
    \item $[[G |-> e : t[us/as] ~> C]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q_1]]$, $[[Q_2]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C]]$
    \item $[[Q_2 |- Q[us/as] ]]$
    \item $[[Q_g]] = [[omega.Q_1*Q_2]]$
    \end{itemize}
    Bu induction hypothesis
    \begin{itemize}
    \item $[[Q_1 ; G |- e : t[us/as] ]]$
    \end{itemize}
    So we have $[[Q_1 * Q[us/as] ; omega.G |- pack e : exists as. t o=
    Q]]$. By rule~\rref*{E-Sub}, we conclude
    $[[Q_g ; omega.G |- pack e : exists as. t o= Q]]$.
  \item[\rref*{E-Unpack}] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * Q' => C2]]$
    \item $[[Q_g |- C1 * Q' => C2]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q_1]]$, $[[Q_2]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 * Q' |- C2]]$
    \item $[[Q_g]] = [[Q1 * Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \item[\rref*{E-Case}] We have
    \begin{itemize}
    \item $[[pi.G + D |-> case_pi e of {alts} : t ~> pi.C * && Ci]]$
    \item $[[Q_g |- pi.C * && Ci]]$
    \item $[[G |-> e : T ss ~> C]]$
    \item For each $i$, $[[D, <xi:_(pi.pii) ui[ss/as]> |-> ei : t ~> Ci]]$
    \end{itemize}
    By repeated uses of Lemma~\ref{lem:inversion}, there exist
    $[[Q]]$, $[[Q']]$ such that
    \begin{itemize}
    \item $[[Q |- C]]$
    \item For each $i$, $[[Q' |- Ci]]$
    \item $[[Q_g]] = [[pi.Q * Q']]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q; G |- e : T ss]]$
    \item For each $i$, $[[Q';D, <xi:_(pi.pii) ui[ss/as]> |- ei : t]]$
    \end{itemize}
    Therefore $[[Q_g ; pi.G + D |- case_pi e of {alts} : t]]$.
  \end{description}
\end{proof}

\begin{lemma}[Weakening of wanteds]\label{lem:wanteds:weakening}
  If $[[Q |- C]]$, then $[[omega.Q'*Q |- C]]$
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the derivation of
  $[[Q |- C]]$, using the corresponding property on the
  simple-constraint entailment relation from
  Definition~\ref{def:entailment-relation}, for the \rref*{C-Dom} case.
\end{proof}

\begin{lemma}\label{lem:wanteds:module-action}
  The following equality holds: $[[pi.(rho.C)]]=[[(pi.rho).C]]$.
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the structure of
  $[[C]]$, using Lemma~\ref{lem:simples:monoid-action} for the case
  $[[C]]=[[Q]]$.
\end{proof}
\end{document}

% LocalWords:  sequent typechecker idempotence polymorphism desugar
% LocalWords:  desugaring ghc OutsideIn quotiented gadt typeable
% LocalWords:  combinator sigils equalities wanteds intuitionistic
% LocalWords:  sequents implicational deallocate deallocating
% LocalWords:  deallocated instantiations desugars
