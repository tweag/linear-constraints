% -*- latex -*-

%if style == newcode
module LinearConstraints where

\begin{code}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE ConstraintKinds #-}
{-# LANGUAGE RankNTypes #-}
{-# LANGUAGE TypeOperators #-}
{-# LANGUAGE KindSignatures #-}
{-# LANGUAGE MultiParamTypeClasses #-}

import Data.Kind (Constraint)
--import GHC.IO.Unsafe
import GHC.Base
\end{code}
%endif

\documentclass[acmsmall,review,natbib=false]{acmart}

\usepackage[backend=biber,citestyle=authoryear,style=alphabetic]{biblatex}
\bibliography{bibliography}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
  }
\usepackage[plain]{fancyref}
\usepackage{mathpartir}
\usepackage{newunicodechar}
\input{newunicodedefs}

%%%%%%%%%%%%%%%%% ott %%%%%%%%%%%%%%%%%

\usepackage[supertabular,implicitLineBreakHack]{ottalt}
\inputott{ott.tex}

%%%%%%%%%%%%%%%%% /ott %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% Workaround %%%%%%%%%%%%%%%%%

% This should be handled by the acmclass article, there are a couple
% of issues about
% this. https://github.com/borisveytsman/acmart/issues/271,
% https://github.com/borisveytsman/acmart/issues/327 . Both have been
% merged long ago, and the version of acmart in the shell.nix is from
% 2020.

%% \usepackage{fontspec}
%% \setmainfont{Linux Libertine O}
%% \setsansfont{Linux Biolinum O}
%% \setmonofont{inconsolata}

%%%%%%%%%%%%%%%%% /Workaround %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% lhs2tex %%%%%%%%%%%%%%%%%

\let\Bbbk\undefined    % see https://github.com/kosmikus/lhs2tex/issues/82
%include polycode.fmt
%if style == poly
%format ->. = "⊸"
%format =>. = "\Lolly"
%format .<= = "\RLolly"
%format IOL = "IO_L"
%format . = "."
%format exists = "\exists"
%format forall = "\forall"
%format pack = "\kpack"
%endif

%%%%%%%%%%%%%%%%% /lhs2tex %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      \setlength{\marginparwidth}{1.2cm} % A size that matches the new PACMPL format
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\csongor}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=purple,#1]{#2}}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2]{}
      \newcommand{\info}[2]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}
  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Domain-specific macros %%%%%%%%%%%%%%%%%

  \newcommand{\cscheme}[1]{\mathcal{#1}}
  \newcommand{\aand}{\mathop{\&}}
  \DeclareMathOperator*{\bigaand}{\vcenter{\hbox{\Large\&}}}
  \newcommand{\lollycirc}{\raisebox{-0.2ex}{\scalebox{1.4}{$\circ$}}}
  \newcommand{\Lolly}{\mathop{=\!\!\!{\lollycirc}}}
  \newcommand{\RLolly}{\mathop{\lollycirc\!\!\!=}}
  \newcommand{\rlolly}{\mathop{\reflectbox{$\multimap$}}}
  \newcommand{\subst}[2]{[#1]#2}
  \newcommand{\sby}[2]{#1 ↦ #2}
  \newcommand{\vdashi}{⊢_{\mathsf{i}}}
  \newcommand{\vdashs}{⊢_{\mathsf{s}}}

  % language keywords
  \newcommand{\keyword}[1]{\mathbf{#1}}
  \newcommand{\klet}{\keyword{let}}
  \newcommand{\kcase}{\keyword{case}}
  \newcommand{\kwith}{\keyword{with}}
  \newcommand{\kpack}{\keyword{pack}}
  \newcommand{\kunpack}{\keyword{unpack}}
  \newcommand{\kin}{\keyword{in}}
  \newcommand{\kof}{\keyword{of}}

%%%%%%%%%%%%%%%%% /Domain-specific macros %%%%%%%%%%%%%%%%%
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA} 
\acmYear{1997}
\copyrightyear{2016}

\acmPrice{15.00}

\begin{document}

\title{Linear Constraints}

\author{Jean-Philippe Bernardy}
\affiliation{
  \institution{University of Gothenburg}
  \city{Gothenburg}
  \country{Sweden}
}
\email{jean-philippe.bernardy@@gu.se}
\author{Richard Eisenberg}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{richard.eisenberg@@tweag.io}
\author{Csongor Kiss}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{csongor.kiss14@@imperial.ac.uk}
\author{Arnaud Spiwack}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{arnaud.spiwack@@tweag.io}

\begin{abstract}
This paper presents \emph{linear constraints}, a language feature that improves
the ergonomics of using linear types by freeing programmers from having to
manually pass around linear resource tokens. The resulting code retains the
safety of the linear version while also keeping the simplicity of the
traditional version. We present a qualified type system with linear constraints,
and a typechecking algorithm. We prove the soundness of the algorithm with
respect to the type system, and show how our changes can be integrated into
|OutsideIn|, GHC's existing constraint solver algorithm.
\end{abstract}

\maketitle

\renewcommand{\shortauthors}{Bernardy, Eisenberg, Kiss, and Spiwack}

\section{Introduction}
\info{There  is an Appendix section with unorganised thoughts and
  examples.}

Linear type systems have seen somewhat of a renaissance in recent years in
various mainstream programming communities. Rust's ownership system guarantees
memory safety for systems programmers, Haskell's linear types give functional
programmers safe APIs for low-level mutable data structures, and even
dependently typed programmers can now use linear types with Idris 2.

\csongor{Say something about ergonomics to set up the next part}

To get a sense of the power of linear types, consider the following example from
the Linear Haskell article~\cite{LinearHaskell}\footnote{|IOL| is the linear IO monad}:
\begin{code}
firstLine :: FilePath -> IOL Bytestring
firstLine fp =
  do  {  f <- openFile fp
      ;  (f, Ur bs) <- readLine f
      ;  closeFile f
      ;  return bs }
\end{code} This simple function opens a file, reads its first line, then closes
it. Linearity ensures that the file handle |f| is consumed at the end.
Forgetting to call |closeFile f| would result in a type error since |f| would
remain unused at the end of the function. Notice that |readLine| consumes the
file handle, and returns a fresh |f| that shadows the previous version, to be
used in further interactions with the file. The line's content is returned in an
|Ur| wrapper (pronounced ``unrestricted'') to signify that it can be used
arbitrary many times.

% We see that linear types introduce some noise on the |readLine| line:
% we need to destruct an extra pair, and an extra |Ur| (called
% |Unrestricted| in~\cite{LinearHaskell}), compared to the
% traditional (albeit less safe)\unsure{It only gets worse in larger
%   program. This makes the extra safety afforded by linear types too
%   rarely worth it.}

Compare this function with the traditional non-linear version:
\begin{code}
firstLine :: FilePath -> IO Bytestring
firstLine fp =
  do  {  f <- openFile fp
      ;  bs <- readLine f
      ;  closeFile f
      ;  return bs }
\end{code}

This version is less safe, because the type system does not keep track of the
file handle, so the programmer must be careful to do this. But it is also
simpler: apparently, in the linear version, we traded clarity for safety. Since
the file handle is now an expendable resource, the type system must know at all
times where it is being consumed, so the file handle must be passed around
manually, culminating in extra noise. Worse, the larger the program gets, the
more additional bookkeeping this requires.

But reading the non-linear version, it is perfectly clear where the
handle is used, and ultimately, consumed. Could the compiler not figure this out
without extra help?

In this paper, we answer this question affirmatively, and introduce \emph{linear
constraints}, an extension of Haskell's type class mechanism to be aware of
linearity. This way, resource consumption is tracked without explicitly having
to thread the tokens through the program. With this extension, the final version
of |firstLine| is exactly the same as the traditional version above,
except that the type |IOL| is used instead of |IO| indicating that this
is working within the linear IO monad.
%if False
\begin{code}
firstLine :: FilePath -> IOL Bytestring
firstLine fp =
  do  {  f <- openFile fp
      ;  bs <- readLine f
      ;  closeFile f
      ;  return bs }
\end{code}

Note the only change from the unsafe version is that this version runs in the
linear IO monad.
%endif

\paragraph{Our contributions are as follows}

\begin{itemize}
\item A system of qualified types that allows a constraint assumption
  to be given a multiplicity. Linear assumptions are used precisely
  once in the body of a definition.
\item This system supports examples that have motivated the design of
  several resource-aware systems, such as ownership à la Rust, or
  capabilities in the style of Mezzo~\cite{mezzo-permissions}
  or \textsc{ats}~\cite{AtsLinearViews}; accordingly, our system
  may serve as a way to unify these lines of research.\unsure{Speak of
  the Pony language? On the one hand Pony is a pretty cool piece of
  tech, on the other hand, I don't know enough to say something
  smart about it.}\unsure{Should we speak of typestate here?}
\item A typechecking algorithm, based on a combination
  of~\cite{OutsideIn}
  and~\cite{resource-management-for-ll-proof-search}, that respects
  the multiplicity of assumptions. We prove that this algorithm is
  sound and complete with respect to our type system.\unsure{Can we
    have a concrete completeness result?}
\item Our algorithm additionally desugars expressions in our qualified
  type system into a core language (directly inspired
  by Linear Haskell~\cite{LinearHaskell}) that supports linear functions. We
  prove that the output of desugaring is well-typed in the core
  language.
\end{itemize}

\newpage

\section{Motivation}


\csongor{I moved the more detailed stuff from the introduction here for now. We
have more space here to elaborate the problem statement}
% This extra noise is the consequence of the fact that, as far as a
% linear type system is concerned, |f| is expended after |readLine
% f|. But, of course, we typically want to do more with a file that
% reading just one line of it, so the linear |readLine| is given the type.

\begin{code}
  readLine :: File ⊸ IOL (File, Unrestricted ByteString)
\end{code}
\unsure{Probably we want the entire \textsc{api} for this
  example. Also, we will probably start with the \textsc{api}, as part
  of the general linear types introduction only to
  demonstrate its limitation here.}

It appears to return a new file, but really, it returns a new name for
the same file. It really is these names which can only be used once,
the file handle itself continues to exist until |closeFile| is called.

This is a bit of a bother, though. Why do I have to manage a bunch of
names to help a compiler count? Surely this can be handled by the
typechecker automatically. And, indeed, compilers with \emph{ad hoc}
specialised logic let me write essentially the traditional program but
with the guarantee that if I forget to |closeFile|, I'll get a type
error. This is most notably the case of the
\textsc{ats}~\cite{AtsLinearViews}, and Mezzo~\cite{mezzo-permissions}
languages.\unsure{Both \textsc{ats} and Mezzo
  are specialised in handling pointers-and-mutations, rather than
  file. So the sentence above is a little bit of a lie. Either
  rephrase to say that we can write programs of \emph{this type}, or
  use an array example instead of a file example. Though I think that
  the file example is a better introduction.}

\unsure{Maybe we want to hammer in the fact that, in Linear Haskell,
  there is no use case natively understood by the compiler. Every
  abstraction is built user-side. \textsc{Ats}'s views are closest to
  what we are doing, though seems to only be geared towards pointers.}
In this article we introduce a generic extension to Linear Haskell,
which lets the typechecker handle the counting. With this extension
|firstLine| would be written as:
\unsure{probably display |pack| in bold in Haskell code}
\begin{code}
firstLine :: FilePath -> IOL Bytestring
firstLine fp =
  do  { pack f <- openFile fp
      ; pack bs <- readLine f
      ; closeFile f
      ; return bs }
\end{code}

There is a bit of boilerplate left, but it doesn't involve managing
names to please the compiler. This considerably lowers the cost of
using a linear-type based abstraction.\unsure{I'm trying to get at the
  cost/benefit analysis thing and how it matters a lot, but I haven't
  found a convincing way to do so without ranting for a page or so,
  I'll get back to this when the rest of the paper is written and I
  revisit the introduction.}


\section{What it looks like}
\label{sec:what-it-looks-like}

Consider the Haskell function |show|

\begin{code}
show :: Show a => a -> String
\end{code}

In addition to the function arrow |->|, common to all functional
programming language, it features a fat arrow |=>|. Everything to the
left of a fat arrow is called a \emph{constraint}. Here |Show a| is a
type-class constraint, but there are other kinds of constraints such
as equality constraint or implicit parameter constraints.

What is crucial, for our purpose, is that constraints are handled
implicitly by the typechecker. That is, if we want to |show| an
integer, we'd write |show 42|, the typechecker would handle proving
that |Show Int| without intervention from the programmer.

In order to manage linearity implicitly, this article introduces a
linear fat arrow |=>.|, much like Linear Haskell introduced a linear
function arrow |->.|. We dub constraints to the left of a linear fat
arrow \emph{linear constraints}.  \unsure{I'm jumping to
  conclusions a little fast. I need to explain what it entails for a
  constraint to be linear. Then I need to re-read (and
  presumably re-write) the paragraphs below, they will probably flow a
  little bit better once the right principles are firmly established.}

In the introduction we wanted to use linearity to make sure that a
file was closed and not used after. That is, we need to track in types
whether the file open or not. We can use a linear constraint |Open f|
to represent that a file is, indeed, open. We can write the type of
|closeFile|:

\begin{code}
closeFile :: Open f =>. File f -> IOL ()
\end{code}

There are a few things to notice
\begin{itemize}
\item First, there is this type variable |f| which didn't exist in
  previous representation. In the representation of the Linear Haskell
  paper, for instance, |closeFile| had type |closeFile :: File ->. IOL
  ()|. This |f| is a type-level name for the file which we are
  closing.
  Giving a name to function argument is the bread and butter of more
  dependently typed languages such as \textsc{ats}~\cite{ats-lang} or Liquid
  Haskell~\cite{liquid-haskell-abstract-refinement-types}. But Haskell doesn't have such a
  naming mechanism built in, so we have to make argument names
  explicit in types.
\item Second, assuming that we have a single, linear, |Open f|
  available, then after |closeFile| there will not be any |Open f|
  left to use, therefore we won't be able to close the file
  twice. Which is precisely what we were trying to achieve.
\end{itemize}

This still leaves questions open: where does |f| come from? where does
|Open f| come from? what are the types of |openFile| and |readLine|?
The answers to these three question rely on the same device: we
introduce a type construction |exists a1 ... an. t .<= Q|\unsure{Todo:
  render the indices as actual indices}, where |Q| is some (linear)
constraint.

The fact that existential quantification generate new type-level names
is a folklore observation. It's used crucially in the interface of the
|ST| monad~\cite{st-monad} and of type-class
reflection~\cite{type-class-reflection} (in both of these cases, existential
quantification is encoded as a rank-2 universal quantificaton). We
shall use it exactly this way: |openFile| uses an existential
quantifier to generate the type-level name of the file
handle. Existentially quantified types are paired with constraint |Q|
which we understand as being returned by functions. We will freely
omit the |exists a1 ... an.| or |.<= Q| parts when they are
empty. This lets us give the following \textsc{api} to files:

\begin{code}
openFile :: FilePath -> IOL (exists f. File f .<= Open f)
readLine :: Open f =>. File f -> IOL (() .<= Open f)
closeFile :: Open f =>. File f -> IOL ()
\end{code}

Haskell doesn't have such existential quantification, however each
instance of such existential quantification can be encoded as a
\textsc{gadt}. For instance |exists f. File f .<= Open f| can be
implemented as

\begin{code}
data NewFile where
  Pack :: Open f =>. File f -> NewFile
\end{code}

Therefore, the existential types of this article are really a
convenience for the sake of exposition\unsure{Though, see the
  existential type paper}. Correspondingly, existential types are
introduced by a data constructor, which we write |pack|.

When pattern-matching on a |pack| all the existentially quantified
names are introduced in scope and all the returned constraints are
made available. With all these ingredients, we can indeed write, as
promised in the introduction
\begin{code}
firstLine :: FilePath -> IOL Bytestring
firstLine fp =
  do  { pack f <- openFile fp
      ; pack bs <- readLine f
      ; closeFile f
      ; return bs }
\end{code}
\unsure{Deduplicate with the version in the introduction to avoid desync?}

\unsure {Explain that existentially quantified type always have an
  implicit unrestricted at their core.}

\unsure{Link to a typestate paper + mention how much closer this is
  to the idea of typestate than the pedestrian encoding in regular
  linear types.}

\newpage

\section{Examples}
\label{sec:examples}


\newpage

\section{A qualified type system}

\unsure{Some transition from example into the technique needed here.}

The description of the type system in this section, as well as the
constraint inference algorithm of Section~\ref{sec:type-inference}, are
strongly inspired by the presentation of
OutsideIn~\cite{OutsideIn}. OutsideIn is a foundation of the type
inference algorithm of \textsc{ghc}, the most popular Haskell
compiler, as such we decided to frame this presentation as an
extension of OutsideIn. We have chosen, for the sake of clarity of the
exposition, to omit details of OutsideIn which do not interact with
linear constraints. We shall point out such simplifications where they
arise.

\subsection{Multiplicities}
\label{sec:multiplicities}

Like in Linear Haskell~\cite{LinearHaskell} we shall make use of a
system of \emph{multiplicities}, which describe how linear functions
can be. Linear Haskell is parametric in the system of
multiplicities. For the sake of this article, however, we will use
only the simplest system of multiplicity: that composed of only
$[[1]]$ (representing linear functions) and $[[omega]]$ (representing
regular Haskell functions).

We will need to add and multiply constraints. Here are the definitions
of these operations.

$$
\left\{
  \begin{array}{lcl}
    [[pi + rho]] & = & [[omega]]
  \end{array}
\right.
$$

$$
\left\{
  \begin{array}{lcl}
    [[1 . pi]] & = & [[pi]] \\
    [[pi . 1]] & = & [[pi]] \\
    [[omega . omega]] & = & [[omega]]
  \end{array}
\right.
$$

\subsection{Simple constraints}
\label{sec:constraint-domain}

\info{See Fig 3, p14 of OutsideIn\cite{OutsideIn}.}

Like OutsideIn (see~\cite[Section 3.2, in particular Figure
3]{OutsideIn}), we parameterise the entire type system by a constraint
domain\,--\,the $X$ in OutsideIn($X$). This domain is characterised by
a set of \emph{atomic constraints} written $[[q]]$ and an entailment
relation $[[Q1 ||- Q2]]$. These constraints are called \emph{simple
  constraints} to distinguish them from the richer constraints of
Section~\ref{sec:wanteds}. This is also the terminology used in
\textsc{ghc}.

Simple constraints are treated completely abstractly by the system; for
inference it will have to come with a domain-specific solver, of which
we only require that it adheres to the interface given in
Section~\ref{sec:constraint-solver}. For instance, in \textsc{ghc},
the domain includes type classes the entailment relation describes
instance resolution.

For the sake of the examples of this article simple constraints need
only be the simplest possible, whereby atomic constraints are treated
as uninterpreted symbols. Being parameterised by the domain therefore
only serves to support the rest of Haskell, or any future extension.

OutsideIn introduces, as part of the constraint domain, a generalised
kind of constraint $\mathcal{Q}$, which include toplevel axioms, such
as type-class instance declarations. Such toplevel axioms
are never linear\,--\,just like toplevel definition are never linear in
Linear Haskell~\cite{LinearHaskell}\,--\,as such they don't have
interesting interaction with the rest of the system, and we choose to
omit them for simplicity.

\unsure{Missing: detailed syntax}

We consider simple constraints equal up to associativity and commutativity of
tensor products, as well as idempotence of the unrestricted
constraints. That is
$$
\begin{array}{l}
  [[Q1 * Q2]] = [[Q2 * Q1]] \\
  [[(Q1*Q2)*Q3]] = [[Q1*(Q2*Q3)]] \\
  [[omega.q * omega.q]] = [[omega.q]]
\end{array}
$$

Scaling is extended to all constraints:

$$
\left\{
  \begin{array}{lcl}
    [[pi.(Q1 * Q2)]] & = & [[pi.Q1 * pi.Q2]] \\
    [[pi.(rho. Q)]]  & = & [[(pi.rho) . Q]]
  \end{array}
\right.
$$
\unsure{Rendering}
\unsure{Explain that the commutation of scaling and tensor product is
  an exotic feature of Linear Haskell. It does simplify the
  presentation a bit here.}

Note that $[[1.Q]] = [[Q]]$ and that
$[[omega.Q * omega.Q]] = [[omega.Q]]$.

We will often omit the scaling factor for linear atomic constraints
and write $[[q]]$ for $[[1.q]]$. Note that there is no ambiguity as,
if $[[pi.q]]$ is read as $[[pi.(1.q)]]$, then it does, indeed, equal
$[[pi.q]]$.

The constraint entailment relation must satisfy the following
properties:

\begin{displaymath}
  \begin{array}{l}
    [[Q ||- Q]] \\
    [[Q1 ||- Q2]] \text{ and } [[Q * Q2 ||- Q3]] \text{ then } [[Q * Q1 ||- Q3]] \\
    [[Q ||- Q1 * Q2]] \text{ then there exists } [[Q']] \text{ and } [[Q'']]
    \text{ such that } [[Q]]=[[Q' * Q'']] \text{, } [[Q' ||- Q1]] \text{ and } [[Q'' ||- Q2]] \\
    [[Q1 ||- Q1]] \text{ and } [[Q2 ||- Q2]] \text{ then } [[Q1 * Q2 ||- Q1 * Q2]] \\
    [[Q ||- rho. q]] \text{ then } [[pi . Q ||- (pi.rho). q]] \\
    [[Q ||- (pi.rho) . q]] \text{ then there exists } [[Q']] \text{ such
    that } [[Q]] = [[pi. Q']] \text{ and } [[Q' ||- rho . q]]
  \end{array}
\end{displaymath}

Another difference with OutsideIn is that we don't require the
presence of equality constraints. We come back to the motivation for
this simplification in Section~\ref{sec:constraint-generation}.

\begin{lemma}
  \label{lem:q:promotion}
  If $[[Q1 ||- Q2]]$, then $[[pi.Q1 ||- pi.Q2]]$.
\end{lemma}
\begin{proof}
  By induction on the syntax of $[[Q2]]$:
  \begin{itemize}
  \item If $[[Q2]] = [[rho.q]]$, then $[[pi.Q1 ||- (pi.rho).q]]$ holds by
    hypothesis.
  \item If $[[Q2]] = [[Q2' * Q2'']]$, then, by hypothesis, we know that
    $[[Q1]] = [[Q1' * Q1'']]$ for some $[[Q1']]$ and $[[Q1'']]$, and
    that
    $[[Q1' ||- Q2']]$ and $[[Q1'' ||- Q2'']]$. By induction hypothesis,
    we have
    $[[pi.Q1' ||- pi.Q2']]$ and $[[pi.Q1'' ||- pi.Q2'']]$. From which it
    follows that
    $[[pi.Q1 ||- pi.Q2]]$.
  \end{itemize}
\end{proof}

\begin{lemma}
  \label{lem:q:scaling-inversion}
  If $[[Q1 ||- pi.Q2]]$, then there exists $[[Q1']]$ such that
  $[[Q1]]=[[pi.Q1']]$ and $[[Q1' ||- Q2]]$
\end{lemma}
\begin{proof}
  By induction on the syntax of $[[Q2]]$
  \begin{itemize}
  \item If $[[Q2]] = [[rho.q]]$, then, by hypothesis, there exists
    $[[Q1']]$ such that $[[Q1]]=[[pi.Q1']]$ and $[[Q1' ||- rho.q]]$.
  \item If $[[Q2]] = [[Q2' * Q2'']]$, then, by hypothesis, we know
    that
    $[[Q1]] = [[Q1' * Q1'']]$ for some $[[Q1']]$ and $[[Q1'']]$, and
    that
    $[[Q1' ||- pi.Q2']]$ and $[[Q1'' ||- pi.Q2'']]$ (remember that, by
    definition, $[[pi.Q2]] = [[pi.Q2' * pi.Q2'']]$). By induction hypothesis,
    we have
    constraints $[[Q']]$ and $[[Q'']]$, such that $[[Q1']] =
    [[pi.Q']]$ and $[[Q1'']] = [[pi.Q'']]$, and $[[Q' ||- Q2']]$ and
    $[[Q'' ||- Q2'']]$.
    It follows that $[[Q1]] = [[pi.(Q' * Q'')]]$ and
    $[[Q' * Q'' ||- Q2]]$.
  \end{itemize}
\end{proof}

\newpage

\subsection{Typing rules}
\label{sec:typing-rules}

Like in OutsideIn~\cite[Section 4]{OutsideIn}, the type system is
presented as a \emph{qualified type system} in the style first introduced
by~\cite{QualifiedTypes}. Such a qualified type system introduces a
judgement of the form $[[Q;G |- e : t]]$, where $[[G]]$ is a standard
type context, and $[[Q]]$ is a constraint from the domain of
Section~\ref{sec:constraint-domain}. For the most part $[[Q]]$ behaves
much like with $[[G]]$, which will be instrumental for
desugaring in Section~\ref{sec:desugaring}; the main difference is
that $[[G]]$ is referred to explicitly with variables, whereas $[[Q]]$
is used implicitly in \rref{E-Var}.

\unsure{We probably want the grammar somewhere}
\unsure{Todo: explain what $[[x :_1 forall as. Q =o u \in G]]$ means}
%
\jp{It's very important to explain in detail that linear constraints
  (as linear values) can never escape to omega contexts.
}

%
\info{See Fig 10, p25 of OutsideIn\cite{OutsideIn}.}

See Figure~\ref{fig:typing-rules}.\unsure{I [Arnaud] think that either
  the subsumption should be removed from the Var rule (using the Sub
  rule explicitly when needed), or, I should remove the Sub rule and
  use subsumption wherever relevant (in particular the Pack
  rule). Let's revisit this when the proofs are done.}

Main differences:
\begin{itemize}
\item Linearity as in the Linear Haskell paper
\item $\kcase$ doesn't have \textsc{gadt}s
\item Existential packs are our only \textsc{gadt}. They have a single
  constructor, pattern-matched over by $\kunpack$.

  (Consequently, we encode linear constraints inside data types as
  existential pack type.)\unsure{We may need a comment on the fact
    that $\kunpack$ doesn't have a $\pi$ index. There would be nothing
    wrong with it, I imagine, it's just a little bit useless.}
\item Explicit subsumption rule \rref*{E-Sub}
\end{itemize}
\improvement{We also want let with a signature. For the sake of completeness}

\begin{figure}
  \centering
  \drules[E]{$[[Q;G |- e : t]]$}{Expression
    typing}{Var,Abs,App,Pack,Unpack,Let,Case,Sub}
  \caption{Qualified type system}
  \label{fig:typing-rules}
\end{figure}

\info{No substitution on $[[Q1]]$ in the $\kunpack$ rule, because there is
  only existential quantification.}


\newpage

\section{Constraint inference}
\label{sec:type-inference}

\unsure{Todo: transition}

\subsection{Wanted constraints}
\label{sec:wanteds}

The constraint generated in Section~\ref{sec:constraint-generation}
have a richer logical structure than the simple constraints. Let us
follow \textsc{ghc}'s terminology and call these \emph{wanted
constraints}: these are constraints which we \emph{want} to hold.

\drules[C]{$[[Q |- C]]$}
  {Generalised constraint entailment}
  {Dom,Tensor,With,Impl}
\info{[Arnaud]: It's really more of a side remark, but there seems
  to be a connection with focusing here: if a combinator is
  ``asynchronous'' then it need not appear in $[[Q]]$ constraints
  whereas if the combinator is ``synchronous'', then it does.}

Scaling is extended to all wanted constraints:

$$
\left\{
  \begin{array}{lcl}
    [[pi.(C1 * C2)]] & = & [[pi.C1 * pi.C2]] \\
    [[omega.(C1 & C2)]] & = & [[omega.C1 * omega.C2]] \\
    [[1.(C1 & C2)]] & = & [[C1 & C2]] \\
    [[pi.(rho.(Q => C))]] & = & [[(pi.rho).(Q => C)]]
  \end{array}
\right.
$$

Like in Section~\ref{sec:constraint-domain}, we will typically drop
the scaling factor for implication when it is $[[1]]$ and write $[[Q
=> C]]$ for $[[1.(Q=>C)]]$.

\begin{lemma}
  \label{lem:inversion}
  \begin{itemize}
  \item If $[[Q |- C1*C2]]$, then there exists $[[Q1]]$ and
    $[[Q2]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[Q]] = [[Q1 * Q2]]$
    \end{itemize}
  \item If $[[Q |- C1 & C2]]$, then $[[Q |- C1]]$ and $[[Q |- C2]]$.
  \item If $[[Q |- pi.(Q2 => C)]]$, then there exists $[[Q1]]$ such
    that
    \begin{itemize}
    \item $[[Q1 * Q2 |- C]]$
    \item $[[Q]] = [[pi.Q1]]$
    \end{itemize}
  \end{itemize}
\end{lemma}
\begin{proof}
  The cases $[[Q |- C1 & C2]]$ and $[[Q |- pi.(Q2 => C)]]$ are
  immediate, since there is only one rule (\rref*{C-With} and
  \rref*{C-Impl} respectively) which can have them as their
  conclusion.

  For $[[Q |- C1 * C2]]$ we have two cases:
  \begin{itemize}
  \item either it is the conclusion of a \rref*{C-Tensor} rule, and
    the result is immediate.
  \item or it is the result of a \rref*{C-Dom} rule, in which case we
    have $[[C1]]=[[Q1]]$, $[[C2]]=[[Q2]]$, and the result follows from
    the definition of the entailment relation.
  \end{itemize}

  This proof may look very fragile. After all in a system with
  quantified constraints~\cite{quantified-constraints}, such as the
  current implementation of \textsc{ghc}, there are rules with
  non-atomic conclusions which do not introduce a connective.

  Proofs where each non-atomic goal is the conclusion of a
  corresponding introduction rule has been called \emph{uniform}
  in~\cite{hh-ll}. They prove for a fragment of linear logic which
  includes quantified constraints and linear generalisations thereof,
  that all provable sequent can be proved by a uniform proof. So this
  lemma, is, in fact, quite robust.
\end{proof}

\begin{lemma}
  \label{lem:wanted:promote}
  If $[[Q |- C]]$, then $[[pi.Q |- pi.C]]$
\end{lemma}
\begin{proof}
  By induction on the syntax of $[[C]]$
  \begin{itemize}
  \item If $[[C]]=[[Q]]$, then the result follows
    from Lemma~\ref{lem:q:promotion}
  \item If $[[C]]=[[C1*C2]]$, then we can prove the result like we
    proved the corresponding case in Lemma~\ref{lem:q:promotion},
    using Lemma~\ref{lem:inversion}.
  \item If $[[C]]=[[C1&C2]]$, then we the case where $[[pi]]=[[1]]$ is
    immediate, so we can assume without loss of generality that
    $[[pi]]=[[omega]]$, and, therefore, that $[[pi.C]] = [[pi.C1 *
    pi.C2]]$.
    By Lemma~\ref{lem:inversion}, we have that $[[Q|-C1]]$ and
    $[[Q|-C2]]$; hence, by induction, $[[omega.Q |- omega.C1]]$ and
    $[[omega.Q |- omega.C1]]$.
    Then, by definition of the entailment relation, we have $[[omega.Q
    * omega.Q |- omega.C1 * omega.C2]]$, which concludes,
    since $[[omega.Q]] = [[omega.Q * omega.Q]]$.
  \item If $[[C]]=[[rho.(Q1 => C)]]$, then by
    Lemma~\ref{lem:inversion}, there is a $[[Q']]$ such that
    $[[Q]]=[[pi.Q']]$ and $[[Q'*Q1 |- C]]$. Applying
    rule~\rref*{C-Impl} with $[[pi.rho]]$, we get
    $[[(pi.rho).Q' |- (pi.rho).(Q1 => C)]]$.

    In other words: $[[pi.Q |- pi.(rho.(Q=>C))]]$ as expected.
  \end{itemize}
\end{proof}

\begin{lemma}
  \label{lem:wanted:demote}
  If $[[Q |- pi.C]]$ then there exists $[[Q']]$ such that
  \begin{itemize}
  \item $[[Q' |- C]]$
  \item $[[Q]] = [[pi.Q']]$
  \end{itemize}
\end{lemma}

\newpage


\subsection{Constraint generation}
\label{sec:constraint-generation}

See Fig.13, p39 of OutsideIn~\cite{OutsideIn} \unsure{Todo: syntax to
  select the $\pi$ in case and let rules}

In a full blown Haskell with linear constraints, there wouldn't be
linear equality constraints.\jp{Seems to suggest that we have linear
  equality constraints here? [Arnaud]: my poor phrasing, I meant to
  imply that we didn't have any equality constraints at all.}
That is, $a \sim b$\unsure{this notation
  hasn't been introduced, so if it makes the cut explain where it
  comes from} wouldn't appear to the left of a linear fat arrow. It's
not that linear equalities don't make sense, see for
instance~\cite{shulman2018linear} for a system which takes linear
equality seriously. However, the usual unification algorithms are
unsound for linear equalities, because they will gladly use the same
equality many times (or none-at-all). Haskell could, by some arbitrary
mean, reject equality constraints to the left of a linear fat arrow,
or it could simply refuse to do any solving with such equalities.

While it is possible that a future version of Haskell includes linear
equality constraints, automatic resolution of linear equality
constraints is beyond the scope of this article. Nor is it needed,
or even useful, for our use cases.
%
Thus, in general, no linear constraint can be used in, or influence in
any way, the unification of type meta variables to types.  As a
consequence, linear
%
constraints are fully orthogonal to type inference. Therefore, the
syntax-directed constraint generation system presented in this section
can legitimately assume that type inference is solved elsewhere;
contrary to~\cite{OutsideIn}, where type inference is mixed with
constraint generation. This separation of concern simplifies the
presentation significantly.\unsure{This paragraph is more wordy than
  it is clear, so let's not take it as an actual proposal for the
  explanation, I [Arnaud] merely wanted to record my thoughts}

\unsure{Todo: the rule for a constraint-generalising signatureless
  let}
\improvement{We also want let with a signature. There are two rules in
  OutsideIn: when the signature is monomorphic, and when it's
  polymorphic. Maybe we don't care about this distinction all that
  much.}
\unsure{The case rule, for an empty case, implies the existence of the
  typically annoying $⊤$. We will have to confront this.}
\unsure{We probably want the freshness condition on the $\kunpack$
  rule, though these variables are universal variables, not
  existentials.}
\info{Not caring about inferences simplifies $\kpack$ quite a bit, we
  are using the pseudo-inferred type to generate constraint. In a real
  system, we would need $\kpack$ to know its type (\emph{e.g.} using
  bidirectional type checking).}
\drules[G]{$[[G |-> e : t ~> C]]$}{Constraint generation}{Var, Abs,
  App, Case, Unpack, Pack}

\newpage

\subsubsection{Soundness of constraint generation}
\label{sec:constraint-generation-soundness}

Let us now prove that every term whose generated wanted constraints
are solvable can indeed be typed in the declarative type system.

\begin{lemma}
  For all $[[Q_g]]$ if
  \begin{itemize}
  \item $[[G |-> e : t ~> C]]$
  \item $[[Q_g |- C]]$
  \end{itemize}
  then
  $[[Q_g; G |- e : t]]$
\end{lemma}
\begin{proof}
  By induction on $[[G |-> e : t ~> C]]$
  \begin{description}
  \item[Var] We have
    \begin{itemize}
    \item $[[x :_1 forall as. Q =o u \in G]]$
    \item $[[G |-> x : u[ts/as] ~> Q[ts/as] ]]$
    \item $[[Q_g |- Q[ts/as] ]]$
    \end{itemize}
    Therefore, by rule~\rref*{E-Var}\unsure{and E-Sub if we
      remove subsumption from the Var rule}, it follows
    immediately that $[[Q_g ; G |- x : u[ts/as] ]]$
  \item[Abs] We have
    \begin{itemize}
    \item $[[G |-> \x. e : t0 ->_pi t ~> C]]$
    \item $[[Q_g |- C]]$
    \item $[[G, x:_pi t0 |-> e : t ~> C]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_g; G, x:_pi t0 |- e : t]]$
    \end{itemize}
    From which follows that $[[Q_g; G |- \x. e : t0 ->_pi t]]$.
  \item[App] \info{Most of the linearity problems are in the App
      rule. Unpack is also relevant.}
    We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q1]]$, $[[Q2]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[Q_g]] = [[Q1 * pi.Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[Pack] We have
    \begin{itemize}
    \item $[[omega.G |-> pack e : exists as. t o= Q ~> omega.C * Q[ts/as] ]]$
    \item $[[Q_g |- C * Q[us/as] ]]$
    \item $[[G |-> e : t[us/as] ~> C]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q_1]]$, $[[Q_2]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C]]$
    \item $[[Q_2 |- Q[us/as] ]]$
    \item $[[Q_g]] = [[omega.Q_1*Q_2]]$
    \end{itemize}
    Bu induction hypothesis
    \begin{itemize}
    \item $[[Q_1 ; G |- e : t[us/as] ]]$
    \end{itemize}
    So we have $[[Q_1 * Q[us/as] ; omega.G |- pack e : exists as. t o=
    Q]]$. By rule~\rref*{E-Sub}, we conclude
    $[[Q_g ; omega.G |- pack e : exists as. t o= Q]]$.
  \item[Unpack] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * Q' => C2]]$
    \item $[[Q_g |- C1 * Q' => C2]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[Q_1]]$, $[[Q_2]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 * Q' |- C2]]$
    \item $[[Q_g]] = [[Q1 * Q2]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \item[Case] We have
    \begin{itemize}
    \item $[[pi.G + D |-> case e of {alts} : t ~> pi.C * && Ci]]$
    \item $[[Q_g |- pi.C * && Ci]]$
    \item $[[G |-> e : T ss ~> C]]$
    \item For each $i$, $[[D, <xi:_(pi.pii) ui[ss/as]> |-> ei : t ~> Ci]]$
    \end{itemize}
    By repeated uses of Lemma~\ref{lem:inversion}, there exist
    $[[Q]]$, $[[Q']]$ such that
    \begin{itemize}
    \item $[[Q |- C]]$
    \item For each $i$, $[[Q' |- Ci]]$
    \item $[[Q_g]] = [[pi.Q * Q']]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q; G |- e : T ss]]$
    \item For each $i$, $[[Q';D, <xi:_(pi.pii) ui[ss/as]> |- ei : t]]$
    \end{itemize}
    Therefore $[[Q_g ; pi.G + D |- case e of {alts} : t]]$.
  \end{description}
\end{proof}

\newpage

\subsection{Constraint solver}
\label{sec:constraint-solver}
$[[Q_g |-s C ~> Q_o;Q_r]]$

Correctness property: $[[Q_g * Q_r |- C * Q_o]]$.\unsure{We would
  typically have $[[Q_o]]⊆[[Q_g]]$, but does it matter? Actually, I'm
  [Arnaud] tempted to posit that this wouldn't always hold for
  GHC. Indeed, GHC rewrites givens, therefore, $[[Q_o]]$ may have
  extra formula or rewritten formulas which are not actually in
  $[[Q_g]]$.}

Various recipes are given
by~\textcite{resource-management-for-ll-proof-search}.
\info{The first presentation of returning output context that I could
  find is~\cite{hh-ll}, but I
  find~\cite{resource-management-for-ll-proof-search} more
  informative.}
These recipes are phrased in a way which implies goal oriented
proof search, but they can be adapted \emph{mutatis mutandis} to GHC's
backtrackingless rewrite-based search.

The key points are
\begin{itemize}
\item Each rule return the remaining, unused (linear) hypotheses: the
  \emph{leftovers}.
\item To deal with $⊤$, remember that you've encountered a $⊤$ in a
  different branch. And let you spend any leftover back into a
  previous $⊤$
\item In~\cite{resource-management-for-ll-proof-search}, there is a
  third input context ($Ξ$), which represents a context which need to
  be used, and cannot be returned as leftovers. However, this is used
  to fail earlier in backtracking branches which are doomed to
  fail. It doesn't apply to our backtrackingless
  setting\unsure{[Arnaud]: I think.}
\end{itemize}

We can do pretty much like in OutsideIn: split the constraint between
simple constraints and non-simple wanted constraints if we wish (but, this time,
non-simple constraint include $\aand$! so there will be significantly
more of them). Apply synchronous rules on non-simple constraints to retrieve simple
constraints, call the simplifier on them.

To be honest, we can even do one atomic constraint at a time, given
that we have no equality, hence our wanted can't interact (I [Arnaud]
think) but the details don't matter terribly.

\newpage

\section{Desugaring}
\label{sec:desugaring}

\info{The plan is to give the operational semantics in the form
  of a desugaring to the core calculus of the Linear Haskell paper.}


\info{
  On unicity of tokens:

  A constraint like ``Open'' (no argument), does not make much sense
  to use as a linear constraint. Because if we can make it once, then
  we can make it omega times, and it's useless (it's similar to linear
  constants in linear haskell. They were not supported.)

  So, we're using something like ``Open f'' instead. Together with a
  an existential variable f. Then the API can ensure that there is a
  single token for this constraint at any given point. The API can
  however create several copies --- but then it's up to the API to
  make sure that the order of picking the constraints is
  computationally irrelevant. (Label constraints as computationally
  relevant? Then GHC could issue errors if there is computational
  relevance and it ends up making a choice.)

  [arnaud] maybe reject all programs which force the constraint solver
  to make an ordering choice
  
  [jp] still worried about the order of treating arguments: if we have
  writeFile f x >> writeFile f y then the arguments of could be
  handled in any order (for constraints!), and it's a shame to reject
  this.

  [arnaud] I don't think so, because in this case, writeFile f y is
  forced to use the most nested evidence, which is that given by
  writeFile f x
  
}

The semantics of our language is given by desugaring it into
a simpler core language: a mild variant of the $λ^q$
calculus from the Linear Haskell article~\cite{LinearHaskell}.

\subsection{The core calculus}
\label{sec:core-calculus}

Our Core calculus is described in Figure~\ref{fig:core-typing-rules}.

\begin{figure}
  \centering
  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Var,Abs,App,Pack,Unpack,Let,Case}
  \caption{Qualified type system}
  \label{fig:core-typing-rules}
\end{figure}

The differences between Figure~\ref{fig:core-typing-rules} and $λ^q$
from the Linear Haskell article~\cite{LinearHaskell}
are as follows
\begin{itemize}
\item We don't have multiplicity polymorphism.
\item We need, on the other hand, type polymorphism.
\item Polymorphism is implicit rather than explicit. This is not an
  essential difference but it simplifies the presentation.
\item We a $\kpack$ and $\kunpack$ pair of constructions, which
  introduce existentially quantified types. Specifically type of the
  form $[[exists as. t2 o- t1]]$: a pair of an unrestricted $[[t2]]$
  and a linear $[[t1]]$. As the $\kpack$ and $\kunpack$ of
  Section~\ref{sec:typing-rules}, these can be realised in regular
  Haskell as a family of \textsc{gadt}s.
\end{itemize}

In addition, we shall assume the existence of data types
\begin{itemize}
\item $[[t1 * t2]]$ with sole constructor
  $[[ (,) : forall a b. a ->_1 b ->_1 a * b ]]$. We will write $[[(e1,
  e2)]]$ for $[[(,) e1 e2]]$.
\item $[[unit]]$ with sole constructor $[[() : unit]]$.
\item $[[Ur t]]$ with sole constructor $[[ Ur : forall a. a ->_omega
  Ur a]]$
\end{itemize}
\unsure{As I'm writing this I realise that there is no mention that
  data constructors are treated as variables. It should probably be
  somewhere in English, as well as in the definition of
  $[[x :_1 forall as. u \in G]]$}

These are regular data types and constructors of the language, they
are consumed with $\kcase$.

In addition, for the sake of concision, when writing expressions in
the core calculus, we will allow ourselves to write nested
patterns in case expressions. Desugaring nested patterns into atomic
case expression is routine.

\subsection{Inferred constraints}
\label{sec:ds:inferred-constraints}

Using Lemma~\ref{sec:constraint-generation-soundness} together with
the correctness criterion\unsure{maybe tug this in a definition so
  that referring to it is easier} of the solver from
Section~\ref{sec:constraint-solver} we know that if
$[[G |-> e : t ~> C]]$ and $[[Q_g |-s C_w ~> Empty ; Q_r]]$, then
$[[Q_g * Q_r ; G |- e : t]]$.

It only remains to desugar derivations of $[[Q;G|-e : t]]$ into the
Core Calculus. Let us dedicate the rest of this section to describe
this last step.

\subsection{From qualified to core}
\label{sec:ds:from-qualified-core}

In order to desugar derivations of the qualified system to the core
calculus, we will use the classic technique known as evidence-passing
style\footnote{This technique is also often called dictionary-passing
  style because, in the case of type classes, evidences are
  dictionaries, and because type classes were the original form of
  constraints in Haskell.}

To do so, we shall require some more material from
constraints. Namely, we will assume a type $[[Ev(q)]]$ for each atomic
constraint. It is extended to all simple constraints

$$
\left\{
  \begin{array}{lcl}
    [[Ev(1.q)]] & = & [[Ev(q)]] \\
    [[Ev(omega.q)]] & = & [[Ur (Ev(q))]] \\
    [[Ev(Empty)]] & = & [[unit]] \\
    [[Ev(Q1 * Q2)]] & = & [[Ev(Q1) * Ev(Q2)]]
  \end{array}
\right.
$$

It ought to be noted that $[[Ev(Q)]]$ is not technically well
defined. Indeed Section~\ref{sec:constraint-domain} defines the syntax
as being quotiented by associativity and commutativity of the tensor
product, and idempotence of unrestricted constraints. But core
language data types (or Haskell's for that matter) are not so
quotiented. So for the sake of defining $[[Ev(Q)]]$, we have to assume
that a particular representative of the equivalence classes has been
fixed.

This is a bit imprecise. It's not actually hard to fix the
imprecision: give a name to each atomic constraint, when tensoring two
simple constraints together merge constraints with the same name. Much
like we do for type context. This is actually essentially how
\textsc{gch} deals with constraints today. It is also the mechanism
that our prototype implementation (see
Section~\ref{sec:implementation}) uses. However, we have preferred
keeping this section a little imprecise, in order to save the rest of
the article from the non-trivial extra tedium that the more precise
presentation entails.

Furthermore we shall require that for every $[[Q1]]$ and $[[Q2]]$
such that $[[Q1 ||- Q2]]$, there is a (linear) function
$[[Ev(Q1 ||- Q2) : Ev(Q1) ->_1 Ev(Q2)]]$.

We will need one more device. Namely, we'll need a way to turn every
$[[Ev(omega.Q)]]$ into an $[[Ur(Ev(Q))]]$. For any
$[[e : Ev(omega.Q)]]$, we shall write $[[urify(Q;e) :
Ur(Ev(omega.Q))]]$. As a shorthand, particularly useful in nested
patterns, we will write $[[case e of {urified(Q;x) -> e'}]]$ for
$[[case urify(Q;e) of {Ur x -> e'}]]$.
Let's define $[[e : Ev(omega.Q)]]$:

$$
\left\{
  \begin{array}{lcl}
    [[urify(Empty;e)]]& = & [[case e of {() -> Ur ()}]] \\
    [[urify(1.q;e)]] & = & [[e]] \\
    [[urify(omega.q;e)]] & = & [[case e of {Ur x -> Ur (Ur x)}]] \\
    [[urify(Q1*Q2;e)]] & = & [[case e of {(urified(Q1;x), urified(Q2;y)) -> Ur (x,y)}]]
  \end{array}
\right.
$$

We will often omit the $[[Q]]$ in $[[urify(Q;e)]]$, and write
$[[urify(e)]]$ when it can be easily inferred from the context.

With this we can desugar a type $[[t]]$ of the qualified system into
a type $[[Ds(t)]]$ of the core calculus.\unsure{confusion type vs type
  scheme aka polytype. Probably not a problem, but it should be
  explained somewhere.}

$$
\left\{
  \begin{array}{lcl}
    [[Ds(forall as. Q =o t)]] & = & [[forall as. Ev(Q) ->_1 Ds(t)]] \\
    [[Ds(t1 ->_pi t2)]] & = & [[Ds(t1) ->_pi Ds(t2)]] \\
    [[Ds(exists as. t o= Q)]] & = & [[exists as. Ds(t) o- Ev(Q)]]
  \end{array}
\right.
$$

Let us finally build, given a derivation $[[Q;G |- e : t]]$, an
expression $[[Ds(z;Q;G |- e : t)]]$, such that
$[[G, z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ (for a distinguished
variable $[[z]]$).

\unsure{Explain how to read the recursive definition, because it's not
  really obvious}
$$
\left\{
  \begin{array}{lcl}
    [[ Ds(z;Q;G |- x : u[ts/as]) ]] & = & [[ x (Ev(Q ||- Q1[ts/as]) z) ]] \\
    [[ Ds(z;Q;G |- \x.e : t1 ->_pi t2) ]] & = & [[ \x. Ds(z;Q;G,x:_pi t1
                                              |- e : t2) ]] \\
    [[ Ds(z;Q1*Q2; G1+G2 |- e1 e2 : t) ]]
        & = & [[ case z of { (z1, z2) -> Ds(z1;Q1;G1 |- e1 : t1 ->_1 t)
              Ds(z2;Q2;G2 |- e2 : t1) } ]] \\
    [[ Ds(z;Q1*omega.Q2; G1+omega.G2 |- e1 e2 : t) ]]
        & = & [[ case z of { (z1, urified(z2))
              -> Ds(z1;Q1;G1 |- e1 : t1 ->_omega t)
              Ds(z2;Q2;G2 |- e2 : t1) } ]] \\
    [[ Ds(z;omega.Q * Q1[us/as];omega.G |- pack e : exists as. t o=
    Q1)]]
        & = & [[ case z of { (urified(z'), z'')
              -> pack (z'', Ds(z'; Q ; G |- e : t[us/as]))} ]] \\
    [[ Ds(z;Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t) ]]
        & = & [[ case z of { (z1, z2) -> unpack (z',x) =
              Ds(z';Q1;G1 |- e1 : exists as. t1 o= Q) in let z2' =
              (z2,z') in Ds(z2';Q2 * Q;G2,x:_omega t1 |- e2 : t)
              } ]] \\
    [[ Ds(z;Q1 * Q2 ;G1+G2 |- let x = e1 in e2 : t) ]]
        & = & [[ case z of { (z1, z2) -> let x = Ds(z1;Q1;G1 |- e1 :
              t1) in Ds(z2;Q2;G2,x:_1 t1 |- e2 : t)} ]] \\
    [[ Ds(z;omega.Q1 * Q2 ;omega.G1+G2 |- let x = e1 in e2 : t) ]]
        & = & [[ case z of { (urified(z1), z2) -> let x = Ds(z1;Q1;G1 |- e1 :
              t1) in Ds(z2;Q2;G2,x:_omega t1 |- e2 : t)} ]] \\
    [[ Ds(z;omega.Q1*Q2;omega.G1+G2 |- case e of { alts } : t) ]]
        & = & [[ case z of { (urified(z1), z2) -> case Ds(z1;Q1;G1 |-
              e : T ts) of { < K xsi -> Ds(z2; Q2; G2, < xi :_(pi.pii)
              ui[ts/as] > |- ei : t)>}} ]] \\
    [[ Ds(z;Q1*Q2;G1+G2 |- case e of { alts } : t) ]]
        & = & [[ case z of { (z1, z2) -> case Ds(z1;Q1;G1 |-
              e : T ts) of { < K xsi -> Ds(z2; Q2; G2, < xi :_(pi.pii)
              ui[ts/as] > |- ei : t)>}} ]] \\
    [[ Ds(z;Q;G |- e : t) ]] & = & [[ let z' = Ev(Q1 ||- Q) z in
                                   Ds(z';Q1;G |- e : t) ]]
  \end{array}
\right.
$$

It is straightforward by induction, to verify that, indeed,
$[[G, z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ as
announced.\unsure{a few more closing words would be welcome.}

\newpage

\section{Implementation}
\label{sec:implementation}

We have implemented linear constraints on top of GHC 9.1, a version that already
ships with the LinearTypes extension. Function arrows (|->|) and context arrows
(|=>|) share the same internal representation in the typechecker, differentiated
only by a boolean flag. Thus, the LinearTypes implementation effort has already
laid down the bureaucratic ground work of annotating these arrows with
multiplicity information.

The key changes affect constraint generation and constraint solving. Constraints
are now annotated with a multiplicity, scaled according to the usage environment
from which they arise.
To illustrate these changes, let us consider the following API:

\begin{code}
class C

useC :: C =>. Int
\end{code}

Now consider the following program, which we hope to reject:

\begin{code}
bad :: C =>. Bool -> Int
bad x = if x then useC else 10
\end{code}

Traditionally, GHC would generate \emph{wanted} constraints from both branches
of the if expression (here |C| from the ``true'' branch, and the trivial
constraint from the ``false'' branch), then union these constraints and attempt
to solve using the \emph{given} constraints (|C|). Then this program would be
accepted, since |C| was used exactly once. However, as discussed in
\ref{sec:constraint-generation}, the constraints of the individual branches of
|if| (and more generally |case|) are combined using $[[&]]$. Intuitively, this
means that for the whole function to use |C| once, both branches must use |C|
exactly once. The shape of generated constraints thus need to follow the shape
of the source program, and instead of representing constraints as sets, we need
a tree of constraints. OutsideIn already includes implication constraints to
handle GADT pattern matches, and we merely borrow these implication constraints
to represent the tree structure.

\unsure{I've been having some ott issues, so for now this is mainly written down
as text} Implication constraints have a bag of \emph{given} constraints, and a
bag of \emph{wanted} constraints, which themselves can be further implications.
For |bad|, our implementation generates an implication with one copy of |C| as
the given and a set of two implications |() => C| and |() => ()| as the wanteds,
for the respective branches. Then we must be able to solve both branches by
using up all linear constraints. To do this, we push the given |C| into the
nested implications and proceed to attempt to solve |C => C| and |C => ()|.  The
second implication's wanted is solvable without consuming the given, and thus an
error is generated.

Now consider the following program:

\begin{code}
bad2 :: C =>. Int
bad2 = const 10 useC
\end{code}

where |const :: a ->. b -> a|. The constant function uses its first argument
linearly, but it does not use its second argument, so the above program needs to
be rejected. Once again, the LinearTypes effort has paved the way for us: in a
function application, the argument's usage is scaled by the multiplicity of the
function's arrow. Similarly, we just scale all wanted constraints that arise
from an argument position. In fact, we only needed to modify the scaling
function to also scale the emitted wanted constraints. The above program then
gets rejected, because |C| is used $[[omega]]$ times.

Finally, consider the following program

\begin{code}
bad3 :: C =>. (Int, Int)
bad3 = (useC, useC)
\end{code}

Once again, this program is rejected, but here we slightly deviate from the
strategy used in LinearTypes. The analogous program using LinearTypes is as follows:

\begin{code}
bad3' :: Int ->. (Int, Int)
bad3' x = (x, x)
\end{code}

In this case, GHC counts the usage of |x| to be $[[omega]]$ and rejects the
program. Instead of counting the usage up front, we modified the constraints
solver to delete a linear given from the set of givens (what GHC calls the
\emph{inert set}) when used to solve a linear wanted (of course linear givens
can't be used to solve nonlinear wanteds). The rationale for this decision is to
support the following program:

\begin{code}
good3 :: (C, C) =>. (Int, Int)
good3 = (useC, useC)
\end{code}

That is, it is possible to have multiple copies of a given constraint. The order
of resolution is non-deterministic, and it is up to the creator of the API to
ensure that the runtime behaviour is confluent irrespective of the order of
constraint resolution.
\unsure{Linear constraints have no runtime payload, or even if they did, they
would not be observable (safely) as dictionary methods use the dictionary in an
unrestricted way.}

\newpage

\section{Extensions}
\begin{itemize}
\item Like there are implicational and universally-quantified
  constraints in the left-hand side of fat arrows, we may want to have
  $\aand$ constraints on the left hand side of (linear) fat
  arrows. This falls in the Linear Hereditary Harrop fragment
  described, for instance, in~\cite{hh-ll}
  and~\cite{resource-management-for-ll-proof-search}. Hereditary
  Harrop is a natural extension of Horn clauses in proof search
  algorithms.
\end{itemize}

\appendix

\section{Preamble from Csongor}

\begin{spec}
data a .<= c where
  Pack :: c =>. a -> a .<= c
data IOL c a = IOL {runIOL :: RealWorld -> (RealWorld, a .<= c)}
\end{spec}

If we bake the constraint into |IOL|, then we need to change \emph{both} |c| and |a|:
\begin{spec}
(>>=) :: IOL c a -> (c =>. a -> IOL d b) -> IOL d b
io_a >>= f = IOL $ \rw -> case runIOL io_a rw of
                            (rw', Pack a) -> runIOL (f a) rw'
\end{spec}

% $ emacs

\section{Arnaud's motivating examples}

\subsection{Linear IO without the hassle (file handles)}

In Linear Haskell~\cite{LinearHaskell}

\begin{spec}
readTwoLines path = do
  h0 <- openFile path
  (h1, line1) <- readLine h0
  (h2, line2) <- readLine h1
  close h2
\end{spec}
API:
\begin{spec}
openFile  :: FilePath -> IOL Handle
close     :: Handle ⊸ IOL ()
readLine  :: Handle ⊸ IOL (Handle, String)
\end{spec}
With linear constraints, API:
\begin{spec}
openFile  :: FilePath -> IOL (Handle h .<= Open h)
close     :: Open h =>. Handle h -> IOL ()
readLine  :: Open h =>. Handle h -> IOL (String .<= Open h)
\end{spec}
The example become (remark: not the do-notation for the |IO| monad):
\begin{spec}
readTwoLines path = do
  h <- openFile path
  line1 <- readLine h
  line2 <- readLine h
  close h
\end{spec}

It looks exactly the same as without linear types. But you still get
an error for double-free and use-after-free usages.

\subsection{Quantum IO Monad}

To showcase another area where deploying linear constraints seems promising, let
us review the quantum computational model. At the fundamental level, the
evolution of a quantum state is \emph{unitary}, which, in essence, means that
information cannot be lost at any point in a quantum circuit. Quantum circuit
gates can thus only express \emph{reversible} operations, and algorithms
operating on quantum machines therefore need to be carefully constructed to only
use reversible operations.

This criterion also poses a challenge to language designers who wish to create
high-level abstractions that can be compiled down to a realisable quantum
circuit, as many common operators, such as the logical OR, are not expressible
in a quantum machine due to their loss of information.

In \cite{altenkirch2010quantum} the authors describe an encoding of quantum
computation in the Quantum IO (QIO) monad, which can effectively simulate
probabilistic quantum operations by sampling a probability distribution. The
framework supplies a DSL, |U|, for reversible computations. This DSL includes,
for example |unot :: Qbit -> U|, which reverses negates a quantum bit (qbit), and |ifQ
:: Qbit -> U -> U| which runs a reversible computation when the supplied quantum
bit is in the true state. Qbits can be initialised using the |mkQbit ::
Bool -> QIO Qbit| operation, and reversible computations can be executed on the
state of the machine using |applyU :: U -> QIO ()|.

These seemingly simple combinators already expose a shortcoming of the API: it
is possible to express irreversible computations: given some |q :: Qbit|, |ifQ q
(unot q)| sets |q|'s value to false. Indeed, the QIO monad enforces a semantic
side condition that the conditional variable must not be modified in the body.
This condition is enforced by the simulator by throwing a runtime error when
such updates are attempted.

A better solution would be to catch the error before even attempting to execute
the program! Using linear constraints, we can provide a safe API for expressing
reversible computations.

The strategy is to modify the QIO API to keep track of having write access to a
qbit. Initialising a qbit provides write access to the qbit.

\begin{code}
mkQbit :: Bool -> exists u. QIO (Qbit q) .<= Write q
\end{code}

Then, we modify |unot| to require write access to the qbit.

\begin{code}
unot :: Write q =>. Qbit q -> U .<= Write q
\end{code}

Finally, |ifQ| consumes write access.

\begin{code}
ifQ :: Write q =>. Qbit q -> U -> U .<= Write q
\end{code}

This way, writing |ifQ q (unot q)| will no longer typecheck, as the call to
|ifQ| has consumed the write access to |q|, thus the body will not be able to
modify it. Of course, it is still possible to write any \emph{other} qbit |r| in the
body, creating an \emph{entagled} pair of qbits: |ifQ q (unot r)|.

\subsection{Ownership and so on…}

From the linear types paper:

\begin{spec}
  newMArray    :: (MArray a ⊸ Unrestricted b) ⊸ Unrestricted b
  writeMArray  :: MArray a ⊸ Int -> a -> MArray a
  freeze       :: MArray a ⊸ Unrestricted (Array a)
  readArray    :: Array a -> Int -> a
\end{spec}

In |writeArray|: we insert an unrestricted element |a ->|. Otherwise
we could do a linear type taboo:

\begin{spec}
unrestrict :: a ⊸ Unrestricted a
unrestrict x = case unrestrictArray x of
  Unrestricted arr -> readArray 0

unrestrictArray :: a ⊸ Unrestricted (Array a)
unrestrictArray x = newArray $ \marr ->
  freeze $
  writeMArray 0 x marr
\end{spec} % emacs <- syntax highlighting bug

This is not ok if I want to make a multidimensional array (say an
|MArray (MArray a)|), which I would later freeze.

How could freeze look like for that use-case?

Something like

\begin{spec}
  freeze :: MArray a ⊸ (a ⊸ Unrestricted b) -> Unrestricted (Array b)
\end{spec}

But this is no longer $O(1)$ unless the compiler has special support
(like |Coercible|~\cite{safe-coercions}).

The crux of the issue is that mutable arrays and immutable arrays have
distinct types, which me must convert.

Contrast with Rust, where there is a single type \verb+Vector+, and
freezing is simply
\verb+fn (vect : Vector<a>) : Rc<Vector<a>> -> { Rc<vect> }+ (check
syntax).

With linear constraints:

\begin{spec}
-- Each reference has 3 linear capabilities associated with it. References can be freely copied, but the capabilities are controlled linearly.

-- The relation between references and capabilities is mediated with an existential type variable (of kind Token for legibility here, but we can let Token = Type with no loss of expressivity.)

kind Token
class Read (n :: Token)    -- read capability
class Write (n :: Token)   -- write capability
class Own (n :: Token)     -- move, free,

type O n = (Read n, Write n, Own n) -- but we cannot move unless no one has kept a reference, so really all 3 capabilities are needed. We never use Own alone, always O.

type RW n = (Read n, Write n) -- likewise, one cannot be writing unless we also have read access (so that a reader does not see changes happening while it is reading)

type Reference = Token -> Type -- kind of types which are associated to capabilities via tokens (so ``references'')

AtomRef a :: Reference

data AtomRef (a :: Type) (n :: Token)

writeRef :: (RW n) =>. AtomRef a n -> a -> () .<= RW n
readRef :: (Read n) =>. AtomRef a n -> a .<= Read n
dealloc :: O n =>. AtomRef a n -> ()
newRef :: (forall n. O n =>. AtomRef a n -> Unrestricted b) ⊸ Unrestricted b
aliasRef :: forall n. (RW n, O p) =>. AtomRef a n -> AtomRef a p -> () .<= RW n

-- Polymorphic aliasing
aliasRef' :: forall n. (RW n, O p, KnownRef a) =>. a n -> a p -> () .<= RW n


data PArray (a :: Reference) (n :: Token)
  -- This an array of References: is an array of boxed stuff. Are all
  -- references inside it are associated with the same token.

PArray a :: Reference

-- type Array a = Frozen (PArray a)
-- data Frozen a where
--   Freezed :: Read n => a n -> Frozen a
-- data Reading a where
--   Read :: Read n =>. a n -> Reading a
-- data Borrowed a where
--   Borrow :: RW n =>. a n -> Borrowed a
-- data Owned (a :: * -> *) where
--   Move :: O n =>. a n -> Owned a

newPArray    :: (forall n. O n =>. PArray a n -> Unrestricted b) ⊸ Unrestricted b
-- Create a new array with the 3 capabilities available (|O n|).

-- Wrong:
-- writePArray  :: (RW n, O p) =>. PArray a n -> Int -> a p -> Owned a .<= RW n
  -- Relinquishes the ownership of the |a p| argument. Returns
  -- ownership of the old value.
  -- iiuc: The reference p is moved inside the array (if we'd make a copy we don't need the O p capability). So we make a new token for it (existentially quantified by Owned).
  -- ? With this interface it seems that the owner will be able to freeze the array, but the reference will survive. So it seems that there is a bug here.

writePArrayAlt  :: (RW n, O p) =>. PArray a n -> Int -> a p -> () .<= (RW n)
-- here the ownership of |a p| is absorbed by n.
-- Operational semantics: move the p *reference* inside n.

readPArray   :: Read n =>. PArray a n -> Int -> a n .<= Read n
-- OK

borrowPArrayElement     :: RW n =>. Parray a n -> Int -> (Borrowed a             ⊸ Unrestricted b         ) ⊸ Unrestricted b .<= RW n
borrowPArrayElementAlt  :: RW n =>. Parray a n -> Int -> (forall p. RW p =>. a p ⊸ Unrestricted b .<= RW p) ⊸ Unrestricted b .<= RW n
borrowPArrayElementAlt' :: RW n =>. Parray a n -> Int -> (forall p. RW p =>. a p ⊸ k              .<= RW p) ⊸              k .<= RW n -- continuation won't be called from an unrestricted context because the initial RW n constraint is unique and linear.

borrowPArrayElement is the only way to access an element in read-write

-- freeze       :: O n =>. PArray a n -> Array a
freeze       :: O n =>. PArray a n -> Array a
-- OK


readArray :: Array a -> Int -> Frozen a
readArray (Freezed arr) i = Freezed (readPArray arr i)
\end{spec}

\section{$\klet$ should be generalised}

\unsure{In the current version of the system $\klet$ doesn't
  generalise. We may want to repurpose this argumentation as a
  discussion on possible refinement of the type system for
  convenience.}

The let rule infers a qualified type for the bound variable, by generalising
over all the linear constraints appearing in the bound expression. This is in
stark contrast with OutsideIn's strategy of inferring fully monomorphic types
for let expressions. So why not follow the established tradition and also infer
monomorphic types when linear constraints are involved? Since the let binder is
unrestricted, the variable $x$ may be used multiple times (or none at all). This
means that the let \emph{must not consume any linear constraints}. \change{which
means that if we also add linear lets to the language, then those can consume
linear constraints. But I (Csongor) don't think linear lets will be necessary?}

To illustrate the practical necessity of the let generalisation strategy,
consider the following file handling API:

\begin{code}
newFile :: IO (exists f. File f .<= Open f)
writeFile :: Open f =>. File f -> String -> IO (() .<= Open f)
closeFile :: Open f =>. File f -> IO ()
\end{code}

The |newFile| function creates a file and returns a file handle |File f|,
together with a linear constraint witnessing that the file |f| is open (note
that |f| is existentially quantified). |writeFile| writes a string to an open
file and keeps it open. Finally, |closeFile| closes the file and consumes the
|Open f| constraint.

Now consider the following program:

\begin{code}
readBad :: IO ()
readBad = do
  file <- newFile
  writeFile file "hello"
  let x = closeFile file
  return ()
\end{code}

This program creates a new file, writes the string |"hello"| to it, then
returns. Even though the |closeFile file| action is assigned to a variable, the
action itself is never invoked, and the file remains open. The fixed version
follows:

\begin{code}
readGood :: IO ()
readGood = do
  file <- newFile
  writeFile file "hello"
  let x = closeFile file
  x
\end{code}

Here, |x| is actually executed, thus the file is closed before the function
returns. The type of |x| in both cases is |Open f0 =>. IO ()| (with |f0| the
existential variable created by |newFile|). Happily, |readBad| gets rejected
because the |Open f0| constraint doesn't get consumed before the function returns.

Unlike traditional let-generalisation, this behaviour can not be overridden with
a signature, so writing |x :: IO ()| is rejected. \change{We don't have a rule
for let with a signature yet, but it will have to be this way.}

\subsection{Comparison with OutsideIn}

So far we have argued that linear constraints should be quantified over in let
bindings. But how does this fit into the OutsideIn constraint solver, the type
inference framework employed by GHC?  In~\cite{OutsideIn}, the authors carefully
consider various different generalisation strategies, each with different
tradeoffs, before reaching the conclusion that no generalisation is the most
ergonomic option.

Here is a summary of the different criteria:

\begin{description}
  \item[Equalities]
        OutsideIn never generalises over equality constraints. Doing
        so would result in very large constraints, resulting in ergonomic and
        performance penalties. In our system, equality constraints are always
        unrestricted, so the issues around consumption explained above do not
        apply to them. Thus, there is no need to generalise over equality
        constraints in our system.
  \item[Class constraints]
        OutsideIn never generalises over class constraints. A downside of
        generalising is that type errors are delayed to call sites when a
        constraint can not be solved. In the case of linear constraints, this is
        the desired behaviour, since whether the constraint can be solved depends
        on whether it is available at the call site, which might differ from
        whether it is available at the definition site.
  \item[Type variables]
        OutsideIn makes the observation that if a type variable is generalised,
        then so must be all the constraints that mention that variable (otherwise
        principal types are lost). Because constraints are not generalised, the
        algorithm opts to also not generalise type variables. A possibility not
        considered in~\cite{OutsideIn} is generalising only the constraint, but
        not the type variables mentioned in it. This is the path we take: type
        variables are not quantified over, but (linear) constraints are. This is
        a sensible option in our setting because it still allows deferring
        constraint solving to use sites, without deviating too much from GHC's
        existing strategy.
\end{description}

To summarise, the generalisation strategy in let bindings is to always
generalise over linear constraints, but keep type variables monomorphic and never
quantify over nonlinear constraints (which includes all equality constraints).
This is a conservative extension of OutsideIn.

\subsection{Maybe we need to be more careful?}

As I (Csongor) wrote the above example, I realised that the example API might
not be sufficient. For example,

\begin{code}
readBad2 = do
  file <- newFile
  writeFile file "hello"
  const (return ()) (closeFile file)
\end{code}

here the file handle is not closed, but according to the App rule, the |Open f0|
constrant is consumed by the application to |const|. The issue here is that we
want to actually ensure that |closeFile| gets \emph{executed}, so maybe a better
interface would be

\begin{code}
newFile :: IO (exists f. File f .<= Open f)
writeFile :: File f -> String -> IO (Open f =>. () .<= Open f)
closeFile :: File f -> IO (Open f =>. ())
\end{code}

is there any other way to fix it? Maybe a linear constraint is only consumed in
an application to a linear function?\info{Arnaud: yes! consuming a
  linear constraints in a non-linear operation should be a type error,
  otherwise linear constraints would be unsound.}

\newpage


\printbibliography

\end{document}

% LocalWords:  sequent typechecker idempotence polymorphism desugar
% LocalWords:  desugaring ghc OutsideIn toplevel quotiented
