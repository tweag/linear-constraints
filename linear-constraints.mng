% -*- latex -*-

%if style == newcode
module LinearConstraints where

\begin{code}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE ConstraintKinds #-}
{-# LANGUAGE RankNTypes #-}
{-# LANGUAGE TypeOperators #-}
{-# LANGUAGE KindSignatures #-}
{-# LANGUAGE MultiParamTypeClasses #-}

import Data.Kind (Constraint)
--import GHC.IO.Unsafe
import GHC.Base
\end{code}
%endif

\documentclass[acmsmall,usenames,dvipsnames]{acmart}

% \usepackage[backend=biber,citestyle=authoryear,style=alphabetic]{biblatex}


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
  }
\usepackage[plain]{fancyref}
\usepackage{mathpartir}
\usepackage{newunicodechar}
\input{newunicodedefs}

%%%%%%%%%%%%%%%%% ott %%%%%%%%%%%%%%%%%

\usepackage[supertabular,implicitLineBreakHack]{ottalt}
\inputott{ott.tex}

%%%%%%%%%%%%%%%%% /ott %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% Workaround %%%%%%%%%%%%%%%%%

% This should be handled by the acmclass article, there are a couple
% of issues about
% this. https://github.com/borisveytsman/acmart/issues/271,
% https://github.com/borisveytsman/acmart/issues/327 . Both have been
% merged long ago, and the version of acmart in the shell.nix is from
% 2020.

%% \usepackage{fontspec}
%% \setmainfont{Linux Libertine O}
%% \setsansfont{Linux Biolinum O}
%% \setmonofont{inconsolata}

%%%%%%%%%%%%%%%%% /Workaround %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% lhs2tex %%%%%%%%%%%%%%%%%

\let\Bbbk\undefined    % see https://github.com/kosmikus/lhs2tex/issues/82
%include polycode.fmt
%if style == poly
%format ->. = "⊸"
%format =>. = "\Lolly "
%format poly_arrow = "\mathop{\to_{m}}"
%format .<= = "\RLolly"
%format <== = "\Leftarrow"
%format IOL = "IO_L"
%format . = "."
%format exists = "\exists"
%format forall = "\forall"
%format pack = "\kpack"
%format pack' = "\kpack!"
%format constraint (c) = "\constraintfont{" c "}"
%
%format a1
%format a_n
%format an = a_n
%format ^^ = "\,"
%endif

%let full = False

%%%%%%%%%%%%%%%%% /lhs2tex %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  % TOGGLE ME to turn off all the commentary:
  \InputIfFileExists{no-editing-marks}{
    \def\noeditingmarks{}
  }

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      \setlength{\marginparwidth}{1.2cm} % A size that matches the new PACMPL format
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\csongor}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\rae}[2][1=]{\todo[linecolor=magenta,backgroundcolor=magenta!25,bordercolor=magenta,#1]{RAE: #2}}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2][1=]{}
      \newcommand{\info}[2][1=]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}

      \newcommand{\csongor}[2][1=]{}
      \newcommand{\jp}[2][1=]{}
      \newcommandx{\rae}[2][1=]{}
  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Domain-specific macros %%%%%%%%%%%%%%%%%

  % Fonts and colours
  \newcommand{\constraintcolour}{\color{RoyalBlue}}
  \newcommand{\constraintfont}[1]{{\constraintcolour#1}}
  \newcommand{\multiplicitycolour}{\color{Black}}
  \newcommand{\multiplicityfont}[1]{{\multiplicitycolour#1}}

  % Utilities
  \newcommand{\revop}[1]{\mathop{\reflectbox{\ensuremath{#1}}}}

  % Notations
  \newcommand{\cscheme}[1]{\mathcal{#1}}
  \newcommand{\rlolly}{\mathop{\revop{\multimap}}}
  \newcommand{\subst}[2]{[#1]#2}
  \newcommand{\sby}[2]{#1 ↦ #2}
  \newcommand{\vdashi}{⊢_{\mathsf{i}}}
  \newcommand{\vdashs}{⊢_{\mathsf{s}}}
  \newcommand{\vdashsimp}{⊢_{\mathsf{s}}^{\mathsf{simp}}}
  \newcommand{\scale}{\constraintfont{\cdot}}
  %% Constraint notations
  \newcommand{\constraintop}[1]{\mathop{\constraintfont{#1}}}
  \newcommand{\aand}{\mathop{\&}}
  \DeclareMathOperator*{\bigaand}{\vcenter{\hbox{\Large\&}}}
  \newcommand{\lollycirc}{\raisebox{-0.255ex}{\scalebox{1.4}{$\circ$}}}
  \newcommand{\Lolly}{\constraintop{=\kern-1.1ex \lollycirc}}
  \newcommand{\RLolly}{\revop{\Lolly}}
  \newcommand{\qtensor}{\constraintop{\otimes}}

  % language keywords
  \newcommand{\keyword}[1]{\mathbf{#1}}
  \newcommand{\klet}{\keyword{let}}
  \newcommand{\kcase}{\keyword{case}}
  \newcommand{\kwith}{\keyword{with}}
  \newcommand{\kpack}{\keyword{pack}}
  \newcommand{\kunpack}{\keyword{unpack}}
  \newcommand{\kin}{\keyword{in}}
  \newcommand{\kof}{\keyword{of}}

  % defining grammars
  \newcommand{\bnfeq}{\mathrel{\Coloneqq}}
  \newcommand{\bnfor}{\mathrel{\mid}}

%%%%%%%%%%%%%%%%% /Domain-specific macros %%%%%%%%%%%%%%%%%

\acmConference[POPL'22]{POPL}{2022}{}
\acmYear{2022}
\copyrightyear{2022}
\setcopyright{none}

\citestyle{acmauthoryear}

\begin{document}

\title{Linear Constraints}

\author{Jean-Philippe Bernardy}
\affiliation{
  \institution{University of Gothenburg}
  \city{Gothenburg}
  \country{Sweden}
}
\email{jean-philippe.bernardy@@gu.se}
\author{Richard A.~Eisenberg}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{rae@@richarde.dev}
\author{Csongor Kiss}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{csongor.kiss14@@imperial.ac.uk}
\author{Arnaud Spiwack}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{arnaud.spiwack@@tweag.io}
\author{Nicolas Wu}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{n.wu@@imperial.ac.uk}

\keywords{GHC, Haskell, laziness, linear logic, linear types,
  constraints, inference}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024</concept_id>
       <concept_desc>Software and its engineering~Language features</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011039</concept_id>
       <concept_desc>Software and its engineering~Formal language definitions</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Language features}
\ccsdesc[300]{Software and its engineering~Functional languages}
\ccsdesc[300]{Software and its engineering~Formal language definitions}

\begin{abstract}
  A linear argument must be consumed exactly once in the body of its
  function.
  A linear type system can verify the
  correct usage of resources such as file handles and manually managed
  memory. But this verification requires bureaucracy.
  %
  This paper presents \emph{linear constraints}, a front-end feature for
  linear typing that
  decreases the bureaucracy of working with linear types.
  %
  Linear constraints are implicit linear arguments that are to
  be filled in automatically by the compiler.
  %
  Linear constraints are presented as a qualified type system,
  together with an inference algorithm which extends OutsideIn,
  \textsc{ghc}'s existing constraint solver algorithm. Soundness of
  linear constraints is ensured by the fact that they desugar into
  Linear Haskell.
\end{abstract}

\maketitle

\renewcommand{\shortauthors}{Bernardy, Eisenberg, Kiss, Spiwack, Wu}

\section{Introduction}
\label{sec:introduction}
\info{There  is an Appendix section with unorganised thoughts and
  examples.}

Linear type systems have seen a renaissance in recent years in
various mainstream programming communities. Rust's ownership system guarantees
memory safety for systems programmers, Haskell's \textsc{ghc}~9.0 includes
support for linear types, and even
dependently typed programmers can now use linear types with Idris 2.
All of these systems are vastly different in ergonomics and scope. In fact,
there seems to be a tradeoff between these two aspects. Rust has
a domain-specific \emph{borrow-checker} that aids the programmer in reasoning about memory
ownership, but it doesn't naturally scale beyond this use case.
Linear Haskell, on the other hand, supports general purpose linear types, but
using them to emulate Rust's ownership model is a painful exercise, because the
compiler doesn't know how to help, requiring the programmer to carefully thread
resource tokens.

To get a sense of the power and the pain of using linear types, consider the
following example from~\citet{LinearHaskell}\footnote{|IOL| is the linear IO monad}:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp =   do  {  h <- openFile fp
                     ;  (h, Ur xs) <- readLine h
                     ;  closeFile h
                     ;  return xs }
\end{code}
This function opens a file, reads its first line, then closes
it. Linearity ensures that the file handle |h| is consumed at the end.
% 
Forgetting to call |closeFile h| would result in a type error because |h| would
remain unused at the end of the function. Notice that |readLine| consumes the
file handle, and returns a fresh |h| that shadows the previous version, to be
used in further interactions with the file. The line's content is a string
|xs| that is returned in an |Ur| wrapper (pronounced ``unrestricted'') to
signify that it can be used arbitrarily many times.
%
Compare the above function with what one would write in a non-linear language:
\begin{code}
firstLine :: FilePath -> IO String
firstLine fp = do  {  h <- openFile fp
                   ;  xs <- readLine h
                   ;  closeFile h
                   ;  return xs }
\end{code}
This version is less safe, because the type system does track the linearity
of the file handle. But it is also
simpler. We see here a clear tension between extra safety and clarity of code---one
we wish, as language designers, to avoid. %%  When
%% modelling a handle as a linear resource, the type system must know at all
%% times where it is being consumed, so the file handle is passed around
%% manually, resulting in extra noise.
%
Reading the non-linear version, it is clear where the
handle is used, and ultimately, consumed. How can we get the compiler
to see that the handle is used safely out without explicit threading?

Rust introduces the \emph{borrow checker} for this very purpose.
Our approach is, in some ways, simpler: we show in this paper
how a natural generalisation of Haskell's type class
constraints does the trick. We call our new constraints \emph{linear constraints}.
Like class constraints,
linear constraints are propagated implicitly by the compiler.
Like linear arguments, they can safely be used to track resources such as file
handles. Thus, linear constraints are the combination of these two
concepts, which have been studied independently elsewhere
\citep{OutsideIn,LinearHaskell,hh-ll,resource-management-for-ll-proof-search}.

With our extension, we can write a new safe version of |firstLine| which does
not require explicit threading of the handle:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp = do  {  pack' h <- openFile fp
                   ;  pack' xs <- readLine h
                   ;  closeFile h
                   ;  return xs }
\end{code}
The only changes from the unsafe version are that this version runs in the linear |IO| monad,
and explicit |pack'| annotations are used to indicate the variables that require
special treatment. (\Fref{sec:implicit-existentials} suggests how we can get rid of
the |pack'|, too.) Crucially, the resource representing the open file is a linear constraint,
and so it longer needs to be passed around manually. 

Our contributions are as follows:
\begin{itemize}
\item A system of qualified types that allows a constraint assumption
  to be given a multiplicity. Linear assumptions are used precisely
  once in the body of a definition
  (\Fref{sec:qualified-type-system}). This system supports examples that have motivated the design of
  several resource-aware systems, such as ownership à la Rust (\Fref{sec:memory-ownership}), or
  capabilities in the style of Mezzo~\cite{mezzo-permissions}
  or \textsc{ats}~\cite{AtsLinearViews}; accordingly, our system
  may serve as a way to unify these lines of research.
\item An inference algorithm that respects the multiplicity of
  assumptions. We prove that this algorithm is sound with respect to
  our type system~(\Fref{sec:type-inference}).
\item A core language (directly adapted
  from Linear Haskell~\cite{LinearHaskell}) that supports linear functions.
Expressions in our qualified
  type system desugar into this core language, and
 we
  prove that the output of our desugaring is well-typed~(\Fref{sec:desugaring}).
\end{itemize}
\jp{It isn't make super clear that this is a 'drop in' extension that you can apply to
Linear Haskell/GHC}

Our design is intended to dovetail well with other features of Haskell and its
implementation within GHC; an implementation is under way. \rae{Does it look bad
that the implementation is not finished? Better not to mention?}

\section{Background: Linear Haskell}
\label{sec:linear-types}

There are \emph{many} approaches to marrying linear logic with functional
programming; see, e.g., \citet[Section 6]{LinearHaskell} for a description
of alternatives. This section, mostly cribbed from \citet[Section 2.1]{LinearHaskell},
describes our baseline approach, as released in GHC~9.0.

Linear Haskell~\cite{LinearHaskell} adds a new type of functions,
dubbed \emph{linear functions}, and written |a ⊸ b|\footnote{The linear function
  type and its notation come from linear
  logic~\cite{girard-linear-logic}, to which the phrase \emph{linear
    types} refers. All the various design of linear typing in the
  literature amount to adding such a linear function type, but details
  can vary wildly. See~\citet[Section 6]{LinearHaskell} for an analysis
  of alternative approaches.}. A linear function consumes its
  argument exactly once. More precisely, Linear Haskell~\cite[Section
  2.1]{LinearHaskell} lays it out thusly:

\begin{quote}
\emph{Meaning of the linear arrow}:
|f :: a ⊸ b| guarantees that if |(f u)| is consumed exactly once,
then the argument |u| is consumed exactly once.
\end{quote}
To make sense of this statement we need to know what ``consumed exactly once'' means.
Our definition is based on the type of the value concerned:
\begin{definition}[Consume exactly once]~ \label{def:consume}
\begin{itemize}
\item To consume a value of atomic base type (like |Int| or |Handle|) exactly once, just evaluate it.
\item To consume a function exactly once, apply it to one argument, and consume its result exactly once.
\item To consume a pair exactly once, pattern-match on it, and consume each component exactly once.
\item In general, to consume a value of an algebraic datatype exactly once, pattern-match on it,
  and consume all its linear components exactly once.
\end{itemize}
\end{definition}
%
Note that a linear arrow specifies \emph{how the function uses its argument}. It does not
restrict \emph{the arguments to which the function can be applied}.
In particular, a linear function cannot assume that it is given the
unique pointer to its argument.  For example, if |f :: a ⊸ b|, then
the following is fine:
\begin{code}
g :: a -> b
g x = f x
\end{code}
The type of |g| makes no guarantees about how it uses |x|.
In particular, |g| can pass |x| to |f|.

The |readLine| example in the introduction consumes
the linear handle |h| created by |openFile|. Therefore |h| can no longer be used to close the file:
doing so would result in a type error.  To resolve this, a new handle for the
same file is produced that can be used with |closeFile|. In the code, the same
name |h| is given to the new handle, thus shadowing the old |h| that can no
longer be used anyway, and giving the illusion that |h| is threaded around.

From the perspective of the programmer, this is unwanted boilerplate.
The approach with linear constraints is to let the handle behave non-linearly, and let its
state (i.e., that the handle is open) be a linear constraint. Once this open state is consumed,
the handle can no longer be read from or be closed without triggering a
compile time error.

\section{Working With Linear Constraints}
\label{sec:what-it-looks-like}

\begin{figure}%
  \centering\small
  \begin{subfigure}{.3\linewidth}%
    \noindent%
\begin{code}
openFile   :: FilePath -> IOL Handle
readLine   :: Handle ⊸ IOL (Handle, Ur String)
closeFile  :: Handle ⊸ IOL ()
\end{code}
\caption{Linear Types}
\label{fig:linear-interface}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.55\linewidth}
\begin{code}
openFile   :: FilePath -> IOL (exists h. Ur (Handle h) .<= constraint (Open h))
readLine   :: constraint (Open h) =>. Handle h -> IOL (Ur String .<= constraint (Open h))
closeFile  :: constraint (Open h) =>. Handle h -> IOL ()
\end{code}
\caption{Linear Constraints}
\label{fig:constraints-interface}
  \end{subfigure}
\caption{Interfaces for file manipulation}
\end{figure}

Consider the Haskell function |show|:
\begin{code}
show :: constraint (Show a) => a -> String
\end{code}
\jp{Why is Show blue?}%
\rae{A good question at this point.}%
In addition to the function arrow |->|, common to all functional
programming languages, the type of this function features a constraint arrow |=>|.
Everything to the
left of a constraint arrow is called a \emph{constraint}. Here |Show a| is a
class constraint.

Constraints are handled implicitly by
the typechecker. That is, if we want to |show| the integer |n :: Int| we would write |show
n|, and the typechecker is responsible for proving that |Show Int| holds, without
intervention from the programmer.

For our |readLine| example, the implicit argument is the state of a handle |h|.
This can be managed as a constraint |Open h|, which indicates that the
file associated with the handle is open. That is, the constraint |Open h|
is provable iff the file associated with |h| is open.
This constraint is linear: it is consumed (that is, used as an assumption
in a function call) exactly once.
In order to manage linearity implicitly, this paper introduces a
linear constraint arrow (|=>.|), much like Linear Haskell introduces a linear
function arrow (|->.|). Constraints to the left of a linear constraint
arrow are \emph{linear constraints}.
Using the linear constraint |Open h|, we can give
the following type to |closeFile|:

\begin{code}
closeFile :: constraint (Open h) =>. Handle h -> IOL ()
\end{code}

There are a few things to notice:
\begin{itemize}
\item First, there is now a type variable |h|. In contrast,
  the version in Figure~\ref{fig:linear-interface} without linear
  constraints  has type |closeFile :: Handle ->. IOL ()|.
  The type variable |h| is a type-level name for the file handle which we are closing.
  Ideally, the linear constraint would refer directly to the
  handle value, and have the type |closeFile :: (h :: Handle) -> constraint (Open h) =>. IOL ()|.
  While giving a name to a function argument is common in some
  dependently typed languages such as \textsc{ats}~\cite{ats-lang} or Idris,
  \rae{There was a reference here to Liquid Haskell, but Liquid Haskell does not
implement dependent types. It does name its arguments, but that doesn't quite fit
in this sentence.}
Our approach, on the other hand, shows how we can still link a runtime value and a compile-time
type variable without needing the full power of dependent types.
\item Second, if we have a single, linear, |constraint (Open h)|
  available, then after |closeFile| there will not be any |constraint (Open h)|
  left to use, thus preventing the file from being closed
  twice. This is precisely what we were trying to achieve.
\end{itemize}

The above deals with closing files and ensuring that a handle cannot be
closed more than once.  However, we still need to explain how a
constraint |constraint (Open h)| can come into existence. For this, we
introduce\footnote{There is a variety of ways existential types can
be worked into a language. A recent publication by \citet{existentials-in-haskell}
works out one such approach. Our development does not build directly on that
work, in order to show how linear constraints can be described independently
of the approach taken there. The existentials we use here might best be understood
as a generalisation of those presented by \citet[Chapter 24]{tapl}. See \Fref{sec:existentials}
for a discussion of how work synergises with that of \citet{existentials-in-haskell}.}
  a type construction |exists a1 ... an. t .<= constraint(Q)|,
where |constraint(Q)| is a linear constraint (scoped over the |a1 ... an|) that is
brought into scope (that is, the constraint is assumed)\footnote{We will freely
omit the |exists a1 ... an.| or |.<= constraint (Q)| parts when they are
empty.}.  The type of |openFile| is thus:
\begin{code}
openFile   :: FilePath -> IOL (exists h. Ur (Handle h) .<= constraint (Open h))
\end{code}
The output of this function is a new unrestricted handle,
along with a new constraint that indicates that the handle is open.
Since the handle is unrestricted it can be freely passed around. The
handle's state is implicitly tracked by the |Open h| constraint.

We must also ensure that |readLine| can both promise to only operate on an open
file and to keep that file open after reading. To do so, its signature
indicates that given a handle |h|, it consumes and produces the implicit |Open h|
constraint, while also producing an unrestricted |String|.
\begin{code}
readLine   :: constraint (Open h) =>. Handle h -> IOL (Ur String .<= constraint (Open h))
\end{code}
%% The fact that existential quantification generates new type-level names
%% is folklore. It's used crucially in the interface of the
%% |ST| monad~\cite{st-monad} and in type-class
%% reflection~\cite{type-class-reflection} (in both of these cases, existential
%% quantification is encoded as rank-2 universal quantification).
%% We
%% shall use it in exactly this way: |openFile| uses an existential
%% quantifier to generate the type-level name of the file
%% handle. Existentially quantified types are paired with a constraint |Q|
%% which we understand as being returned by functions.
\rae{I removed some text here describing the operation of existentials
as creating fresh variables as ``folklore''. I disagree: the fact that
unpacking an existential creates a fresh variable is part of the definition
of an existential in any discrete mathematics textbook.}

Haskell does not have an existential quantifier. However, one can
be encoded as a \textsc{gadt}. For instance, |exists h. Ur (Handle h) .<= constraint (Open h)| can
be implemented as
\begin{code}
data PackHandle where
  Pack :: constraint (Open h) =>. Handle h -> PackHandle
\end{code}
In the implementation of our ideas in \textsc{ghc}, packed linear constraints
piggy-back on the standard \textsc{gadt} syntax. Correspondingly, existential types are
introduced by a data constructor, which we write as |pack|.

When pattern-matching on a |pack| constructor, all existentially quantified
names are brought into scope and all the returned constraints are
assumed. We also use |pack' x| as a shorthand
for |pack (Ur x)|. We have now seen all the ingredients needed to write
the |firstLine| example as in the introduction.

\subsection{Minimal Examples}
\jp{This used to say ``motivating examples'', but I don't think that they motivate much. Even better would be ``Do's and Dont's with linear constraints'' or something like that.}
\label{sec:examples}

To get a sense of how the features we introduce should behave, we now
look at some simple examples. Using constraints to represent
limited resources allows the typechecker to reject certain classes
of ill-behaved programs. Accordingly, the following examples show the
different reasons a program might be rejected.

In what follows, we will be using a constraint |constraint (C)| that is consumed by the |useC|
function.
\begin{code}
useC :: constraint (C) =>. Int
\end{code}
The type of |useC| indicates that it consumes the linear resource |C| exactly once.

\subsubsection{Dithering}

We reject this program:
\begin{code}
dithering :: constraint (C) =>. Bool -> Int
dithering x = if x then useC else 10
\end{code}
The problem with |dithering| is that it does not unconditionally consume |constraint(C)|:
 the branch where |x == True| uses the resource |C|,
whereas the other branch does not.

\subsubsection{Neglecting}

Now consider the type of the linear version of |const|:
\begin{spec}
const :: a ->. b -> a
\end{spec}
This function uses its first argument linearly, and ignores the second. Thus,
the the second arrow is unrestricted.
%
One way to improperly use the linear |const| is by neglecting a linear variable:
\begin{code}
neglecting :: constraint (C) =>. Int
neglecting = const 10 useC
\end{code}
The problem with |neglecting| is that, although |useC| is mentioned in this program,
it is never consumed: |const| does not use its second argument.
The constraint |constraint(C)| is not consumed exactly once, and
thus this program is rejected.
%
The rule is that a linear constraint can only be consumed (linearly)
in a linear context. For example,
\begin{code}
notNeglecting :: constraint (C) =>. Int
notNeglecting = const useC 10
\end{code}
is accepted, because the |constraint (C)| constraint is passed on to |useC| which itself
appears as an argument to a linear function (whose result is itself consumed linearly).

\subsubsection{Overusing}
\label{sec:overusing}

Finally, consider the following program, which should be rejected:
\begin{code}
overusing :: constraint (C) =>. (Int, Int)
overusing = (useC, useC)
\end{code}
This is program is rejected because it uses |C| twice. However, the
following version is accepted:
\begin{code}
notOverusing :: constraint ((C, C)) =>. (Int, Int)
notOverusing = (useC, useC)
\end{code}
We see here that it is possible to have multiple copies of a given
constraint.

%JP: this used to say:
% Our system is designed so that the order of resolution is
% non-deterministic. This corresponds to the assumption that all
% instances of a linear constraint are equivalent.
Because the consumption of constraints is implicit, when there are
several possible ways to consume a constraint, the particular order
chosen will depend on the details of the constraint resolution
algorithm. We do not want the semantics of programs to depend on this
order, which would not be at home in a declarative specification of the type system.
Consequently, if an
\textsc{api} uses linear constraints, it must be designed to ensure that the
runtime behaviour is not dependent on the order of constraint
resolution. \rae{This is a bit disturbing here. What does this mean, precisely?
I would want an example.}

\subsection{Linear I/O}
\label{sec:linear-io}

The file-handling example discussed in sections~\ref{sec:linear-types}
and~\ref{sec:what-it-looks-like} uses a linear version of the |IO| monad, |IOL|.
Compared to the traditional |IO| monad, the
type of the monadic operations |>>=| and |return| are changed to
use linear arrows.
%
\begin{code}
(>>=) :: IOL a ->. (a ->. IOL b) ->. IOL b
return :: a ->. IOL a
\end{code}
%
Bind must be linear because, as explained in the previous section, a linear
constraint can be consumed in only a linear context. Consider
the 
following program:
\begin{code}
readTwo ::  constraint (Open h) =>. Handle h -> IOL (Ur (String, String) .<= constraint (Open h))
readTwo h =  readLine h >>= \case pack' xs ->
             readLine h >>= \case pack' ys ->
             return (pack' (xs, ys))
\end{code}
If bind were not linear, the occurrence of |readLine h| would
not be able to consume the |constraint (Open h)| constraint linearly.

\section{Application: memory ownership}
\label{sec:memory-ownership}

Let us now turn to a more substantial example: manual memory
management.  In Haskell, memory deallocation is normally the
responsibility of a garbage collector. However, garbage collection is
not always desirable, either due to its (unpredictable) runtime costs, or because
pointers exist between separately-managed memory spaces
(for example when calling foreign functions~\cite{linear-inline-java}).
In either case, one must then
resort to explicit memory allocation and deallocation. This task is
error prone: one can easily forget a deallocation (causing a memory
leak) or deallocate several times (corrupting data).

\subsection{Ownership constraints}
\label{sec:atomic-references}

With linear constraints, it is possible to provide a \emph{pure} interface for
manual memory management, which
enforces correct deallocation of memory.  Our approach, in the style of Rust,
is to use a
linear constraint to represent \emph{ownership} of a memory location.
We use a linear constraint
|Own n|, such that a program which has an |constraint(Own n)| constraint in
context is responsible of deallocating the memory associated
with a memory location |n|.  Because of linearity, this constraint
must be consumed exactly once, so it is guaranteed that the memory
is deallocated correctly.
%
In |constraint(Own n)|, |n| is a type variable (of a special kind |Location|)
which represents a memory location. Locations mediate
the relationship between references and ownership constraints.
%
In fact, for more granular control, we will be using three linear constraints:
one for reading, one for writing, and one for ownership.

\begin{minipage}{0.3\linewidth}
> class constraint (Read (n :: Location))
\end{minipage}
\begin{minipage}{0.3\linewidth}
> class constraint (Write (n :: Location))
\end{minipage}
\begin{minipage}{0.3\linewidth}
> class constraint (Own (n :: Location))
\end{minipage}
\\
Thanks to this extended set of constraints, programs are able to read from or write
to a memory reference without owning it. To ensure referential transparency,
writes can be done only when we are sure that no other part of the program has
read access to the reference. Rust disallows mutable aliasing for the same
reason: ensuring that writes cannot be observed through other references is what
allows treating mutable structures as ``pure''.
Therefore, writing also requires the read capability.  We
systematically use the |RW| set of constraints, defined below, instead of
|Write|.

> type constraint (RW n) = constraint ((Read n, Write n))

Likewise, a location cannot be deallocated if any part of the program
has a read or write reference to it, so all 3 capabilities are
needed for ownership. So we use |O|, instead of |Own|, defined thus:

> type constraint (O n) = constraint ((Read n, Write n, Own n))

With these components in place, we can provide an \textsc{api} for ownable
references.

> data AtomRef (a :: Type) (n :: Location)

The type |AtomRef| is the type of references to values of a type |a| at
location |n|. Allocation of a reference can be done using the
following function.

> newRef :: (forall n. ^^ constraint (O n) =>. AtomRef a n -> Ur b) ⊸ Ur b

The reference is made available in a function which we call the
\emph{scope} of the reference. The return type of the scope is,
crucially, |Ur b|.  Indeed, if we would allow returning any type |b|,
then the |O n| constraint could be embedded in |b|, and therefore
escape from the scope.
This would defeat the point of |O n| be a linear constraint, and
no longer guarantee that the reference has a unique owner.\jp{This is because newRef can be called in unrestricted contexts. Do we have enough space/reader attention to explain that ?}

To read a reference, a simple |Read| constraint is demanded, and
immediately returned. Writing is handled similarly.

> readRef :: constraint (Read n) =>. AtomRef a n -> Ur a .<= constraint (Read n)
> writeRef :: constraint (RW n) =>. AtomRef a n -> a -> () .<= constraint (RW n)

Note that the above primitives do not need to explicitly declare
effects in terms of a monad or another higher-order effect-tracking
device: because the |RW n| constraint is linear, passing it suffices
to ensure proper sequencing of effects concerning location |n|.
This is ensured by the combination of the language and library
behaviour. For example, here is how to write two values (|a| and |b|) to the same reference |x|:

> case writeRef x a of
>   pack _ -> case writeRef x b of
>     pack _ -> ...

The language semantics forces the programmer to do case analysis to
access the returned |Write| constraints, and |writeRef| must be strict
in the |Write| constraint that it consumes.

Deallocation consumes all linear constraints associated with |O n|.

> freeRef :: constraint (O n) =>. AtomRef a n -> ()

As an alternative to freeing the reference, one could transfer control
of the memory location to the garbage collector. This operation is
sometimes called ``freezing'':

> freezeRef :: constraint (O n) =>. AtomRef a n -> Ur a

\subsection{Arrays}
\label{sec:arrays}

The above toolkit handles references to base types just fine.  But
what about storing references in objects managed by the ownership
system? In this section, we show how to deal with arrays of
references, (including arrays of arrays), as a typical case.

% JP: the discussion below sounds quite petty. Not many (zero
% non-conflicting?)  people will be aware of the issue with linear
% arrays. So it would need a lot more space to be properly
% justified. Besides, the issue of storing references in structures
% should be motivation enough in general.

% A motivating example of plain Linear Haskell is a pure interface to
% mutable arrays~\cite[Section 2.2]{LinearHaskell}. There we have two
% types: first |MArray a|, used linearly, for mutable arrays; and
% second |Array a|, used unrestricted, for immutable arrays. Mutable
% arrays can be frozen using |freeze :: MArray a ->. Ur (Array
% a)|. Note that this version of |freeze| can not support mutable
% arrays of mutable arrays.  The core of the issue is that mutable
% arrays and immutable arrays have different types.
% The ownership \textsc{api}, on the other hand, is readily extended to
% nested mutable arrays.
% Crucially, and in contrast to the plain linear types \textsc{api} the
% type |PArray| is both the type of mutable arrays and immutable arrays,
% |freezePArray| only changes the permissions. And since permissions are
% linear constraints, this is all managed automatically by the compiler.

> data PArray (a :: Location -> Type) (n :: Location)
> newPArray    :: (forall n. ^^ constraint (O n) =>. PArray a n -> Ur b) ⊸ Ur b

For this purpose we introduce the type |PArray a n|, where the kind of
|a| is |Location -> Type|: this way we can easily enforce that each
reference in the array refers to the same location |n|. Both types
|AtomRef a| and |PArray a| have kind |Location -> Type|, and therefore
one can allocate, and manipulate arrays of arrays with this
\textsc{api}.

When writing a reference (be it an array or an |AtomRef|) in an array,
ownership of the reference is transferred to the array.

> writePArray  :: constraint ((RW n, O p)) =>. PArray a n -> Int -> a p -> () .<= constraint (RW n)

More precisely, the ownership of the location |p| is absorbed into that
of |n|. Therefore, the associated operational semantics is to move the
reference inside the array (and deallocate any previous reference at that index).

We still want to have read and write access to references in the array
so we provide |lendPArrayElt|:

> lendPArrayElt  :: constraint (RW n) =>. PArray a n -> Int -> (forall p. ^^ constraint (RW p) =>. a p -> r .<= constraint (RW p)) ⊸ r .<= constraint (RW n)

The |lendPArrayElt a i k| primitive lends access to the reference at index |i| in
|a|, to a scope function |k| (in Rust terminology, the scope
``borrows'' an element of the array). Here, the return type of the scope, |r|,
is not in |Ur|: since the scope must return the |RW p| constraint, it is
not possible to leak it out by packing it into |r|, so it's not necessary to
wrap the result in |Ur|.
Crucially, with this \textsc{api}, |RW n| and |RW p| are never simultaneously
available. The following function would not be sound:

> extractEltWrong  :: constraint (RW n) =>. PArray a n -> Int -> exists p. Ur (a p) .<= constraint ((RW n, RW p))

Indeed, when called from a context where the program owns |n|,
|n| could immediately be deallocated, even though |RW
p| would remain available, letting us write to freed memory. For the same reason,
gaining read access to an element needs to be done using a
scoped \textsc{api} as well:

> lendPArrayEltRead  ::  constraint (Read n) =>. PArray a n -> Int
>                    ->  (forall p. ^^ constraint (Read p) =>. a p -> r .<= constraint (Read p))
>                    ⊸   r .<= constraint (Read n)

Finally, we can freeze arrays, using the
following primitive:

> freezePArray :: constraint (O n) =>. PArray a n -> () <== constraint (Read n)

After |freezePArray n|, we have unrestricted read access to |n| (and
any element of |n|), as expected.

> readArray :: constraint (Read n) => PArray a n -> Int -> a n


\section{A qualified type system for linear constraints}
\label{sec:qualified-type-system}
\jp{Downplay this OutsideIn relationship (some more)? Today (20210527) there are 46 references to OutsideIn. Seems excessive; I'd like this number to be below 10.}

Having laid out situations which would benefit from linear constraints, we
now present our design for a qualified type system~\cite{jones} that supports
them. Our design is mindful of our goal of integration with Haskell and \textsc{ghc}
and is thus based on the work of \citet{OutsideIn}, which undergirds \textsc{ghc}'s
current approach to type infernce.

\subsection{Multiplicities}
\label{sec:multiplicities}

Like in Linear Haskell~\cite{LinearHaskell} we make use of a
system of \emph{multiplicities}, which describe how
many times a function consumes its input.  For our purposes, we need only the
simplest system of multiplicity: that composed of only $[[{1}]]$ (representing
linear functions) and $[[{omega}]]$ (representing
regular Haskell functions).
$$
\begin{array}{lcll}
  [[{pi}]], [[{rho}]] & \bnfeq & [[{1}]] \bnfor [[{omega}]] & \text{Multiplicities}
\end{array}
$$
The idea of multiplicity goes back at least
to \citet{ghica_bounded_2014}, where it is dubbed a \emph{resource
  semiring}. The power of multiplicities is that they can encode the
structural rules of linear logic with only the semiring
operations: addition and multiplication\footnote{We adopt a convention that equations defining a function by pattern matching are marked with a $\left\{\right.$ to their left}.

\smallskip
\begin{minipage}[c]{0.4\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{pi + rho}]] & = & [[{omega}]]
  \end{array}
\right.
$$
\end{minipage}
\begin{minipage}[c]{0.4\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{1 . pi}]] & = & [[{pi}]] \\
    [[{omega . pi}]] & = & [[{omega}]]
  \end{array}
\right.
$$
\end{minipage}
\smallskip

Even though linear Haskell additionally supports
multiplicity polymorphism,
we do not support multiplicity polymorphism on constraint
arguments, and we believe that such a feature would not be very useful.
Multiplicity polymorphism of regular function arguments is used to avoid
duplicating the definition of higher-order functions. The prototypical example
is |map :: (a poly_arrow b) -> [a] poly_arrow [b]|,
where |poly_arrow| is the notation for a function arrow of multiplicity |m|.
 First-order functions, on the other hand, do not need multiplicity
polymorphism, because
linear functions can be $\eta$-expanded into unrestricted function as
explained in Section~\ref{sec:linear-types}. Higher-order functions
whose arguments are themselves constrained functions are rare,
so we do not yet see the need to extend multiplicity polymorphism
to apply to constraints.
Futhermore, it is not clear how to extend the constraint solver of
Section~\ref{sec:constraint-solver} to support multiplicity-polymorphic
constraints.

\subsection{Simple constraints and Entailment}
\label{sec:constraint-domain}

Types are constrained by \emph{simple constraints} $[[{Q}]]$:
$$
\begin{array}{lcll}
  [[{q}]] & \bnfeq & \ldots & \text{Atomic constraints} \\
  [[{Q}]] & \bnfeq & [[{pi.q}]] \bnfor [[{Q1*Q2}]] \bnfor [[{Empty}]] &
                                                                \text{Simple constraints}
\end{array}
$$
The atomic constraints $[[{q}]]$ refer to the base constraints supported
in a qualified type system: these might include class constraints and equality
constraints, for example. Interestingly, our approach puts no constraints on
what this set includes, and thus could conceivably be used in a qualified-type
language other than Haskell. The multiplicity $\pi$ in front of atomic constraints is the \emph{scaling
  factor}.
It indicates whether the constraint is to be used linearly
($[[{1}]]$) or without restriction ($[[{omega}]]$).
Along with scaled atomic constraints, a simple constraint can also be a conjunction
or a tautology ($[[{Empty}]]$).

\begin{figure}
  $$
    \begin{array}{l}
      [[Q ||- Q]] \\
      \text{if } [[Q1 ||- Q2]] \text{ and } [[Q * Q2 ||- Q3]] \text{ then } [[Q * Q1 ||- Q3]] \\
      \text{if } [[Q ||- Q1 * Q2]] \text{ then there exists } [[{Q'}]] \text{ and } [[{Q''}]]
      \text{ such that } [[{Q}]]=[[{Q' * Q''}]] \text{, } [[Q' ||- Q1]] \text{ and } [[Q'' ||- Q2]] \\
      \text{if } [[Q ||- Empty]] \text{ then there exists } [[{Q'}]] \text{ such that } [[{Q}]]=[[{omega.Q'}]] \\
      \text{if } [[Q1 ||- Q1']] \text{ and } [[Q2 ||- Q2']] \text{ then } [[Q1 * Q2 ||- Q1' * Q2']] \\
      \text{if } [[Q ||- rho. q]] \text{ then } [[pi . Q ||- (pi.rho). q]] \\
      \text{if } [[Q ||- (pi.rho) . q]] \text{ then there exists } [[{Q'}]] \text{ such
      that } [[{Q}]] = [[{pi. Q'}]] \text{ and } [[Q' ||- rho . q]] \\
      \text{if } [[Q1 ||- Q2]] \text{ then } [[omega.Q1 ||- Q2]] \\
      \text{if } [[Q1 ||- Q2]] \text{ then for all } [[{Q'}]], [[omega.Q' * Q1 ||- Q2]]
    \end{array}
  $$
\caption{Requirements for the entailent relation $[[Q1 ||- Q2]]$}
\label{fig:entailment-relation}
\end{figure}

Type checking refers to an entailment relation
$[[Q1 ||- Q2]]$ that defines how some constraints can imply
others. The properties we need from this relation appear in
Figure~\ref{fig:entailment-relation}.
%
\info{See Fig 3, p14 of OutsideIn\cite{OutsideIn}.}
%
%
%% Atomic constraints are treated abstractly by our system
%% (just like they are in OutsideIn). For
%% inference, in Section~\ref{sec:type-inference}, we will need a domain-specific solver, of which
%% we only require that it adheres to the interface given in
%% Section~\ref{sec:constraint-solver}. But for the sake of this section,
%% we only need that the domain be equipped with the entailment relation.

%%  This corresponds to the
%% multiplicative truth $\mathbf{1}$ of linear logic, but we chose this notation
%% because it is more distinct from the $[[{1}]]$ multiplicity.

%% OutsideIn introduces, as part of the constraint domain, a generalised
%% kind of constraint $\mathcal{Q}$, which includes top-level axioms, such
%% as type-class instance declarations. Such top-level axioms
%% are never linear\,--\,just like how top-level definitions are never linear in
%% Linear Haskell\,--\,and as such they don't have
%% interesting interactions with the rest of the system, and we choose to
%% omit them for simplicity.

We consider simple constraints to be equal up to associativity and commutativity
of tensor products, as well as idempotence of the unrestricted constraints.
\rae{What's the technical payload here? I don't think we ever compare two
constraints for equality. So I think this paragraph is really just saying that
the entailment relation must respect these equalities. If that's the case, can't
we just build this into Figure~\ref{fig:entailment-relation}?}
We
require the entailment relation to respect these properties. That is:
$$
\begin{array}{rcl}
  [[{Q1 * Q2}]] & = & [[{Q2 * Q1}]] \\
  [[{(Q1*Q2)*Q3}]] & = & [[{Q1*(Q2*Q3)}]] \\
  [[{omega.q * omega.q}]] & = & [[{omega.q}]] \\
  [[{Q * Empty}]] & = & [[{Q}]]
\end{array}
$$
%
We extend our definition of scaling to work on simple constraints as follows:
\jp{The color system does not look very consistent. Why is the tensor colored but not the dot? What about parentheses? } \rae{Yes,
I do not know what the colors are really buying us.}
\jp{The tensor binds less tightly than the dot, but there are larger spaces around the dot than around the tenor. (Also, not consistently). This is quite confusing in the typing rules. Also the inconsistent coloring further suggests the wrong binding: it looks like the tensor are grouped together in π · blue (Q₁⊗Q₂). I saw later that the dot is blue but it's very hard to spot due to psycho-visual effects.}
\rae{Was this addressed? Looks better now.}
%
$$
\left\{
  \begin{array}{lcl}
    [[{pi.Empty}]] & = & [[{Empty}]]\\
    [[{pi.(Q1 * Q2)}]] & = & [[{pi.Q1 * pi.Q2}]] \\
    [[{pi.(rho. Q)}]]  & = & [[{(pi.rho) . Q}]]
  \end{array}
\right.
$$
%
The rule $[[{omega.(Q1 * Q2)}]] = [[{omega.Q1 * omega.Q2}]]$ is not a
typical feature of linear logic. \rae{This last statement needs more context. I
don't understand the implications. Can we just strike this para?}
Linear Haskell, however, introduces
the corresponding type isomorphism for the sake of polymorphism. While this
article isn't concerned with polymorphism, this equality does make
the overall presentation a bit simpler. Note, in particular, that $[[{1.Q}]] = [[{Q}]]$ and
$[[{omega.Q * omega.Q}]] = [[{omega.Q}]]$.
\rae{Can we please be more careful around equality? Some of our
$=$ are definitional equality, and some are really bi-implications. I think
this is about bi-implication. But is this something implied by the previous
bi-implications, or is this new?}

We will often omit the scaling factor for linear atomic constraints
and write $[[{q}]]$ for $[[{1.q}]]$. %% This convention does not induce any
%% ambiguity. Indeed the only potential ambiguity is between $[[{pi.q}]]$
%% and $[[{pi.(1.q)}]]$, which are  equal.
\rae{Really? This surprises me, given that the linear constraints are the
new ones. I propose not doing this.}

%% \begin{definition}[Requirements for the entailment relation]\label{def:entailment-relation}
%%   The constraint entailment relation must satisfy the properties
%%   in \autoref{fig:entailment-relation}.
%% \end{definition}
%% Apart from linearity, the main difference with OutsideIn is that we
%% don't require the presence of equality constraints. We come back to
%% the motivation for this simplification in
%% Section~\ref{sec:type-inference}.

An important feature of simple constraints is that, while scaling
syntactically happens at the level of atomic constraints, these properties
of scaling extend to scaling of arbitrary constraints.

\begin{lemma}[Scaling]
  \label{lem:q:scaling}
  If $[[Q1 ||- Q2]]$, then $[[pi.Q1 ||- pi.Q2]]$.
\end{lemma}

\begin{lemma}[Inversion of scaling]
  \label{lem:q:scaling-inversion}
  If $[[Q1 ||- pi.Q2]]$, then $[[{Q1}]]=[[{pi.Q'}]]$ and $[[Q' ||- Q2]]$ for some $[[{Q'}]]$.
\end{lemma}

Proofs of these lemmas (and others) appear in our anonymized supplementary material;
they can be proved by straightforward use of the properties in Figure~\ref{fig:entailment-relation}. \rae{Is that true? I'm assuming we use that figure to prove these.}

\subsection{Typing rules}
\label{sec:typing-rules}

With this material in place, we can now present our type system. The
grammar is given in Figure~\ref{fig:declarative:grammar}, and the
typing rules in Figure~\ref{fig:typing-rules}.

\begin{figure}
  \centering\small
  $$
  \begin{array}{lcll}
    [[{a}]], [[{b}]] & \bnfeq & \ldots & \text{Type variables} \\
    [[{x}]], [[{y}]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[{K}]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[{s}]] & \bnfeq & [[{forall as. Q =o t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & [[{a}]] \bnfor [[{exists as. t o= Q}]] \bnfor [[{t1 ->_pi t2}]]
                            \bnfor [[{T ts}]] & \text{Types} \\
    [[{G}]], [[{D}]] & \bnfeq & [[{empty}]] \bnfor [[{G, x:_pi s}]] &
                                                              \text{Contexts} \\
    [[{e}]] & \bnfeq & [[{x}]] \bnfor [[{K}]] \bnfor [[{\x. e}]] \bnfor [[e1
                     e2]] \bnfor [[{pack e}]] & \text{Expressions}\\
                 &\bnfor & [[unpack x=e1 in
                     e2]] \bnfor [[{case_pi e of { alts }}]] &\\
                 &\bnfor & [[let_pi
                     x=e1 in e2]] \bnfor [[{let_pi x : s = e1 in e2}]] &
  \end{array}
  $$
  \caption{Grammar of the qualified type system}
  \label{fig:declarative:grammar}
\end{figure}

\begin{figure}
  \centering\small
  \drules[In]{$[[x :_1 s \in G]]$}{Context membership}{Var,Weaken}
  \drules[S]{$[[{pi . G}]]$}{Context scaling}{Empty,Binding}
  \drules[A]{$[[{G1 + G2}]]$}{Context addition}{Binding,Absent,Empty}
  \drules[E]{$[[Q;G |- e : t]]$}{Expression
    typing}{Var,Abs,App,Pack,Unpack,Let,LetSig,Case,Sub}
  \caption{Qualified type system}
  \label{fig:typing-rules}
\end{figure}
\jp{Regarding the typing rules figure: is the case \(x:_ωσ ∈ ...\) missing?
  omitted intentionally? In the E-Pack rule: Is the ω intentional? I seem to remember that the Ur constructor is not implicitly there. Also the ω is not in the E-Unpack rule, at a glance.   }

A qualified type system~\cite{QualifiedTypes} such as ours introduces a
judgement of the form $[[Q;G |- e : t]]$, where $[[{G}]]$ is a standard
type context, and $[[{Q}]]$ is a constraint we have assumed to be true.
$[[{Q}]]$ behaves
much like $[[{G}]]$, which will be instrumental for
desugaring in Section~\ref{sec:desugaring}; the main difference is
that $[[{G}]]$ is addressed explicitly, whereas $[[{Q}]]$
is used implicitly in \rref{E-Var}.

The type system of Figure~\ref{fig:typing-rules} is purely
declarative: note, for example, that \rref{E-App} does not describe
how to break the typing assumptions into constraints
$[[{Q1}]]$/$[[{Q2}]]$ and contexts $[[{G1}]]$/$[[{G2}]]$. We will see
how to infer constraints in Section~\ref{sec:type-inference}. Yet,
this system is our ground truth: a system with a simple enough
definition that programmers can reason about typing. We do not
directly give a dynamic
semantics to this language; instead, we will give it meaning via
desugaring to a simpler core language in Section~\ref{sec:desugaring}.

Areas of interest include the following:
\info{See Fig 10, p25 of OutsideIn\cite{OutsideIn}.}
\rae{Is there a way to eliminate the $[[binding \in G]]$ judgment?
I find it very counter-intuitive, because it's redefining the meaning
of $\in$. Instead, can we put the unrestricted context in the conclusion
of E-Var? That would eliminate a whole judgment form and be more in keeping
with other presentations of linear types I have seen. If we don't do this,
we need to explain what we're doing here.}
\rae{I do not understand the ``context scaling'' judgment. The bit
in the box does not correspond to what appears below the lines in the rules.
And then, the rules seem just to define a function defined by recursion
on the structure of a typing context. That seems ripe for the \{ notation
we're using. Or is there something else going on here?}
\rae{Ditto the ``context addition'' judgment. This one is maybe a bit
subtler because the function is partial. Or is it? Ah. After much staring,
I realized what's so strange about it: there's no 0. If there were 0, then
context addition would simply require that the two contexts have the same
sequence of bindings, only with different multiplicities. But we can't do
that without 0. In any case, the current definition seems wrong, in that
I don't see how it copes if a binding is mentioned in $[[{G2}]]$ but
omitted from $[[{G1}]]$. Another strangeness is that this judgment is redefining
$=$ again. Much better to use our \{ notation, possibly with a precondition
on the shape of $[[G1]]$ and $[[G2]]$. Strange corner case: what if $[[G1]]$
and $[[G2]]$ are similar but permutations of each other? The current definition
looks like it would fall over.}
\rae{Why does E-Pack scale $[[{Q}]]$ with $[[{omega}]]$?}
\rae{Do we need the multiplicity on |let|? |unpack| always gets a multiplicity
of 1. Presumably, we could have unrestricted existentials by using |Ur|. Similarly,
we should be able to have |let| always have a multiplicity of 1, and then have
users use |Ur| when they want to. As it is, the lack of parallelism here is
surprising.}
\rae{If we have $[[x : Q =o t]]$ and have assumed $[[omega.Q]]$, can we
use $[[x]]$? Presumably, we should be able to. But I'm not quite sure where
the rules allow this. Maybe using E-Sub and the second-to-last property
in Fig. 2? Yes, that looks plausible. But double-check me, please.}
\rae{E-LetSig is missing a $[[{Q1}]]$ in its conclusion, no?}
\rae{E-Case should be clearer that the second two premises are universally
quantified over $i$. Actually, it's worse than that, with strangeness like
$[[pi.pii]]$ written in a subscript, but there's really many $[[pii]]$s, I
think. Do we even need |case|?}
\begin{itemize}
\item The type system has linear functions, written $[[{a ->_1 b}]]$.
  Despite our focus on linear constraints,
  we still need linearity in ordinary arguments.
  For example, the bind combinator for |IOL| (Section~\ref{sec:introduction})
  requires linear arrows: |(>>=) :: IOL a ->. (a ->. IOL b) ->. IOL b|.

  Furthermore, the linearity of arrows interacts in interesting
  ways with linear constraints: If $[[f : a ->_omega b]]$ and
  $[[x : q =o a]]$, then calling $[[{f x}]]$ would actually use $[[{q}]]$
  many times. We must make sure it is impossible to derive
  $[[q ; f :_omega a ->_omega b, x :_omega q =o a |- f x : b]]$.
  Otherwise we could make, for instance, the |overusing| function from
  Section~\ref{sec:overusing}.
  You can check that $[[q ; f :_omega a ->_omega b, x :_omega q =o a |- f x : b]]$
  indeed does not
  type check, because the scaling of $[[{Q2}]]$ in \rref{E-App} ensures that
  the constraint would be $[[{omega.q}]]$ instead. On the other hand,
  it is perfectly fine to have $[[q ; f :_omega a ->_1 b, x :_omega q
  =o a |- f x : b]]$ when $[[{f}]]$ is a linear function.
\item \Rref{E-Let} includes support for local
  assumptions. We thus have the ability to generalise a subset of
  the constraints needed by $[[{e1}]]$ (but not the type variables---no 
  |let|-generalisation here, though it could be added). The inference algorithm of
  Section~\ref{sec:type-inference} will not make use of this
  possibility, but we revisit this capability in Section~\ref{sec:let-generalisation}.
%% \item Data types are not \textsc{gadt}s.
%%   This serves to considerably simplify the \rref*{E-Case}
%%   rule. It would be
%%   straightforward, yet tedious, to extend data types here to full
%%   \textsc{gadt}s.
\item We include $[[{exists as. t o= Q}]]$, as
  introduced in Section~\ref{sec:what-it-looks-like}, together with
  the $\kpack$ and $\kunpack$ constructions. See rules~\rref*{E-Pack} and
  \rref*{E-Unpack}.
%% \item A more minor difference with OutsideIn is that we have an
%%   explicit \rref*{E-Sub} rule, while OutsideIn uses simple constraint
%%   entailment directly in the relevant rules. In OutsideIn, only the
%%   \rref*{E-Var} rule needed subsumption; we would also need it for the
%%   \rref*{E-Pack} rule as well. So we preferred having one shared
%%   dedicated rule.
\end{itemize}
%
\info{No substitution on $[[{Q1}]]$ in the $\kunpack$ rule, because there is
  only existential quantification.}


\section{Constraint inference}
\label{sec:type-inference}

The type system of Figure~\ref{fig:typing-rules} describes declaratively what programs
are acceptable. We now present an algorithm that implements that specification.
\rae{Is this wording too strong?}

Our algorithm is structured, unsurprisingly, around generating and solving
constraints, broadly in the model described by \citet{essence-of-ml-type-inference}.
That is, our algorithm takes a pass over the abstract syntax entered by the
user, generating constraints as it goes. Then, separately, we solve those
constraints (that is, try to satisfy them) in the presence of a set of assumptions,
or we determine that the assumptions do not imply that the constraints hold. In the
latter case, we issue an error to the programmer.

A typical generate-and-solve algorithm is built around unification variables.
A unification variable stands for some unknown type. When we see, for example,
|\x -> ...|, we assign |x| to have a unification variable for its type, and then
constraints arising from the body of the expression (or its context) might tell
us that the unknown type is |Int|. Our system here does not do this, however:
using constraints to solve for unification variables is well understood~\cite{OutsideIn}
and orthogonal to the concerns that drive the innovations in this paper.
Instead, the aspects of our algorithm around inferring types---as opposed to
inferring \emph{constraints}---nondeterministically just guess the correct
type. A real implementation would use unification variables and constraints,
but they just add clutter here and distract us from the novel parts of our
algorithm.

We can make this simplification for two reasons:
\begin{itemize}
\item We do not formalize equality constraints, and our implementation
  in \textsc{ghc} does not allow equality constraints to be
  linear\footnote{\textsc{Ghc} actually uses (at least) two forms of
equality constraint: lifted equality, used by the programmer; and unlifted
equality, used internally~\cite{deferred-type-errors}.
The |~| constraints in Haskell code are lifted
ones, where lifted equality is actually just a fairly ordinary class that
has unlifted equality as a superclass constraint. Our approach to dealing
with linear constraints around equality is simply to have the superclass
equality constraint of |~| be unrestricted. In that way, even if we have
a linear |~| constraint, the actual internal equality will still be 
unrestricted.}. Indeed, a typical treatment of unification
  would be
  unsound for linear equalities, because it reuses the same
  equality many times (or none at all). Linear equalities make sense
  (\citet{shulman2018linear} puts linear
  equalities to great use), but they do not seem to lend themselves to
  automation.
\item We do not support, or intend to support, multiplicity
  polymorphism in constraint arrows. That is, the multiplicity of a
  constraint is always syntactically known to be either linear or
  unrestricted. This way, no equality constraints (which might, conceivably,
  relate multiplicity variables) can interfere with
  constraint resolution.
\end{itemize}
%
%% Our current focus is more narrow than a general typechecking algorithm for
%% all of \textsc{ghc}'s features.
%% As a consequence of these two restrictions (no linear equalities and no
%% multiplicity polymorphic constraints), type inference and (linear) class
%% constraint resolution are completely orthogonal. Therefore, the syntax-directed
%% constraint generation system presented in this section can legitimately assume
%% that type inference is solved elsewhere, greatly simplifying the presentation.


\subsection{Wanted constraints}
\label{sec:wanteds}

The constraints $[[{C}]]$ generated in our system have a richer
logical structure than the simple constraints $[[{Q}]]$, above. Following
\textsc{ghc} and echoing \citet{OutsideIn}, we call these \emph{wanted constraints}:
they are constraints which the constraint solver \emph{wants} to prove.
An unproved wanted constraint is a type error reported to the programmer.
%
$$
\begin{array}{lcll}
  [[{C}]] & \bnfeq & [[{Q}]] \bnfor [[{C1*C2}]] \bnfor [[{C1&C2}]] \bnfor [[{pi.(Q=>C)}]]&
                                                                \text{Wanted constraints}
\end{array}
$$
%
A simple constraint is a valid wanted constraint, and we have two forms of
conjunction for wanted constraints:
the new
$[[{C1 & C2}]]$ construction (read $[[{C1}]]$ \emph{with} $[[{C2}]]$), alongside
the more typical $[[{C1 * C2}]]$. This is a
connective from linear logic: while $[[{C1*C2}]]$ is the
\emph{multiplicative} conjunction, $[[{C1&C2}]]$ is the \emph{additive}
conjunction. Both connectives are conjunctions, but they differ
% rather dramatically % weasel words combo
in meaning. Satisfying $[[{C1*C2}]]$ consumes the (linear)
assumptions consumed by satisfying $[[{C1}]]$ those consumed by $[[{C2}]]$;
if an assumed linear constraint is needed to prove both $[[{C1}]]$ and $[[{C2}]]$,
then $[[{C1*C2}]]$ will not be provable, because that linear assumption cannot
be used twice. On the
other hand, satisfying $[[{C1&C2}]]$ requires that satisfying $[[{C1}]]$
and $[[{C2}]]$ must each
consume the \emph{same} assumptions, $[[{C1 & C2}]]$ will consume as well.
Thus, if $[[{C}]]$ is assumed linearly (and we have no other assumptions),
then $[[{C*C}]]$ is not provable, while $[[{C&C}]]$ is.
The intuition, here, is that in $[[{C1 & C2}]]$, only
one of $[[{C1}]]$ or $[[{C2}]]$ will be eventually used. ``With'' constraints
arise from separate branches in a $\kcase$-expression, inspiring the behaviour
of this connective described here.

The last form of wanted constraint $[[{C}]]$ is an implication $[[{pi.(Q=>C)}]]$.
Proving $[[{pi.(Q=>C)}]]$ allows us to assume $[[{Q}]]$ linearly while proving $[[{C}]]$,
a total of $[[{pi}]]$ times. These implications arise when we unpack an existential
package that contains a linear constraint and also when checking a |let|-binding.
We can define scaling over wanted constraints by recursion as follows, where we
use scaling over simple constraints in the simple-constraint case:
$$
\left\{
  \begin{array}{lcl}
    [[{pi.(C1 * C2)}]] & = & [[{pi.C1 * pi.C2}]] \\
    [[{pi.(C1 & C2)}]] & = & [[{pi.C1 & pi.C2}]] \\
    [[{pi.(rho.(Q => C))}]] & = & [[{(pi.rho).(Q => C)}]]
  \end{array}
\right.
$$
\rae{I simplified this. Is it still correct?}
Like in Section~\ref{sec:constraint-domain}, we will typically drop
the scaling factor for implication when it is $[[{1}]]$ and write $[[Q
=> C]]$ for $[[{1.(Q=>C)}]]$.

We define an entailment relation over wanteds in Figure~\ref{fig:wanted:entailment}.
Note that this relation uses only simple constraints $[[{Q}]]$ as assumptions, as
there is no way to assume the more elaborate $[[{C}]]$.
\begin{figure}
  \centering\small
  \drules[C]{$[[Q |- C]]$} {Wanted-constraint entailment}
  {Dom,Tensor,With,Impl}

  \caption{Wanted-constraint entailment}
  \label{fig:wanted:entailment}
\end{figure}

Note that simple constraints always entail a wanted: we
generate wanted constraints, but programs only mention simple
constraints, and the left-hand side does come from the program. It is
possible to relax the syntax of the left-hand side
though, which \textsc{ghc} does~\cite{quantified-constraints}.

Before we move on to constraint generation proper, let us prove a few
technical, yet essential, lemmas about the wanted-constraint
entailment relation.

\begin{lemma}[Inversion]
  \label{lem:inversion}
  The inference rules of $[[Q |- C]]$ can be read bottom-up as well
  as top-down, as is required of $[[Q1 |- Q2]]$ in
  Definition~\ref{def:entailment-relation}. That is
  \begin{itemize}
  \item If $[[Q |- C1*C2]]$, then there exists $[[{Q1}]]$ and $[[{Q2}]]$
    such that $[[Q1 |- C1]]$, $[[Q2 |- C2]]$, and
    $[[{Q}]] = [[{Q1 * Q2}]]$.
  \item If $[[Q |- C1 & C2]]$, then $[[Q |- C1]]$ and $[[Q |- C2]]$.
  \item If $[[Q |- pi.(Q2 => C)]]$, then there exists $[[{Q1}]]$ such
    that $[[Q1 * Q2 |- C]]$ and  $[[{Q}]] = [[{pi.Q1}]]$
  \end{itemize}
\end{lemma}
\begin{proof}
  The cases $[[Q |- C1 & C2]]$ and $[[Q |- pi.(Q2 => C)]]$ are
  immediate, since there is only one rule (\rref*{C-With} and
  \rref*{C-Impl} respectively) which can have them as their
  conclusion.

  For $[[Q |- C1 * C2]]$ we have two cases:
  \begin{itemize}
  \item either it is the conclusion of a \rref*{C-Tensor} rule, and
    the result is immediate.
  \item or it is the result of a \rref*{C-Dom} rule, in which case we
    have $[[{C1}]]=[[{Q1}]]$, $[[{C2}]]=[[{Q2}]]$, and the result follows from
    the definition of the entailment relation.
  \end{itemize}

  It may look like a slight change in the hypotheses would invalidate this proof.
  After all in a system with
  quantified constraints~\cite{quantified-constraints}, such as the
  current implementation of \textsc{ghc}, there are rules with
  non-atomic wanteds which do not introduce a connective. So, does
  this proof break when moving to the full \textsc{ghc}? Fortunately
  not!

  What we really need is that any proof can be \emph{rewritten} into a
  proof which ends in an introduction rule. Such proofs have been
  called \emph{uniform} in~\citet{hh-ll}. More precisely, a uniform
  proof is a proof where every rule whose conclusion has a non-atomic
  wanted is an introduction rule. The authors prove that all proofs in
  a fragment of linear logic can be rewritten into a uniform
  proof. This fragment includes quantified constraints and linear
  generalisations thereof. So, we can conclude that this lemma indeed
  extends to the complete set of constraints supported by \textsc{ghc}.
\end{proof}

\begin{lemma}[Promotion]
  \label{lem:wanted:promote}
  If $[[Q |- C]]$, then $[[pi.Q |- pi.C]]$
\end{lemma}

\begin{lemma}[Inversion of promotion]
  \label{lem:wanted:demote}
  If $[[Q |- pi.C]]$ then $[[Q' |- C]]$ and $[[{Q}]] = [[{pi.Q'}]]$ for some $[[{Q'}]]$
\end{lemma}

\subsection{Constraint generation}
\label{sec:constraint-generation}

The process of inferring constraints is split into two parts: generating
constraints, which we do in this section, then solving them in
Section~\ref{sec:constraint-solver}. To generate constraints we
introduce a judgement $[[G |-> e : t ~> C]]$ (defined in
Figure~\ref{fig:constraint-generation}).\jp{Please explain how to intuitively read this judgement here, and/or in the caption of the figure.} It is intended that $[[{C}]]$
is understood as an output of this judgement. The definition $[[G |->
e : t ~> C]]$ is syntax directed, so it can directly be read as an
algorithm, taking as input a term $[[{e}]]$ (together with some type
inference oracle, as discussed above) and returning a wanted
constraint $[[{C}]]$.

\info{See Fig.13, p39 of OutsideIn~\cite{OutsideIn}}

\info{Not caring about inferences simplifies $\kpack$ quite a bit, we
  are using the pseudo-inferred type to generate constraint. In a real
  system, we would need $\kpack$ to know its type (\emph{e.g.} using
  bidirectional type checking).}
\begin{figure}
  \centering\small
  \drules[G]{$[[G |-> e : t ~> C]]$}{Constraint generation}{Var, Abs,
    App, Pack, Unpack, Case, Let, LetSig}

  \caption{Constraint generation}
  \label{fig:constraint-generation}
\end{figure}

The rules of Figure~\ref{fig:constraint-generation} constitute a mostly
unsurprising translation of the rules of Figure~\ref{fig:typing-rules}
in the style of OutsideIn~\cite[Section 5.4]{OutsideIn}. In particular, the operations
on typing contexts are the same as in
Figure~\ref{fig:typing-rules}, and are not repeated here. Still, a few
remarks are in order
\begin{itemize}
\item The most conspicuous difference is that, as we explained in
  Section~\ref{sec:wanteds}, where OutsideIn uses a single kind of
  conjunctions, the \rref*{E-Case} rule needs two. OutsideIn
  accumulates constraints across branches, whereas we need to make
  sure that each branch of a $\kcase$-expression consumes the same
  constraints.

  This is easily understood in terms of the file example of
  Section~\ref{sec:introduction}:
  if a file is closed in one branch of a $\kcase$, it had better be closed in
the other branches too.
  Otherwise, after the case its state will be unknown to the type system.

  The reason this distinction is not made in an intuitionistic
  system like the original OutsideIn is elucidated by the
  fact that $[[{omega.(C1&C2)}]] = [[{omega.C1 * omega.C2}]]$. In other
  words, in a purely unrestricted context, linear constraints don't
  need two kinds of conjunctions either.
\item Just like OutsideIn, the introduction of constraints local to a
  premise in the rules of Figure~\ref{fig:typing-rules} corresponds to
  emitting an implicational constraint in
  Figure~\ref{fig:constraint-generation}.
\item However, the~\rref*{G-Let} rule doesn't have an implicational
  constraint corresponding to the local constraint of \rref{E-Let}:
  like in OutsideIn~\cite[Section 4.2]{OutsideIn}, a $\klet$ without a
  signature is not generalised. That being said, while not
  generalising is the easiest choice, here, it is not clear that it is
  the best in the presence of linear constraints, as we discuss in
  Section~\ref{sec:let-generalisation}
\item As we've observed throughout this section, not including type
  inference in the constraint generation algorithm significantly
  simplifies the presentation. There is no particular difficulty
  involved, however, in combining type inference and linear
  constraints; keeping in mind that equality constraints should never
  be linear.
\end{itemize}

% I [aspiwack] removed a section which I felt wasn't relevant. Leaving
% the label to avoid dead links.
\label{sec:constraint-generation-soundness}

The key property of the constraint-generation algorithm is that,
if the generated constraint is solvable, then we can indeed type the
term in the qualified type system of
Section~\ref{sec:qualified-type-system}. That is, we are not defining
a new type system but indeed an inference algorithm for the qualified
type system.

\begin{lemma}[Soundness of constraint generation]\label{lem:generation-soundness}
  For all $[[{Q_g}]]$ if $[[G |-> e : t ~> C]]$ and $[[Q_g |- C]]$ then
  $[[Q_g; G |- e : t]]$
\end{lemma}
\begin{proof}
Soundness is proved by induction on $[[G |-> e : t ~> C]]$. We only
present the two most interesting cases, the full proof can be found
in the supplementary material
  \begin{description}
  \item[\rref*{E-App}] We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q1}]]$, $[[{Q2}]]$ such
    that $[[Q1 |- C1]]$, $[[Q2 |- C2]]$, and
    $[[{Q_g}]] = [[{Q1 * pi.Q2}]]$.  Then, by induction hypothesis
    \begin{itemize}
    \item $[[Q1; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[\rref*{E-Unpack}] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * Q' => C2]]$
    \item $[[Q_g |- C1 * Q' => C2]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Q_2}]]$
    such that
    $[[Q_1 |- C1]]$,
    $[[Q_2 * Q' |- C2]]$, and
    $[[{Q_g}]] = [[{Q1 * Q2}]]$.
    Then, by induction hypothesis
    \begin{itemize}
    \item $[[Q_1; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \end{description}
\end{proof}

\subsection{Constraint solving}
\label{sec:constraint-solver}

In this section,
let us build a \emph{constraint solver} whose purpose it is to prove
that $[[Q_g |- C]]$ holds, as required by Lemma~\ref{lem:generation-soundness}.
The constraint solver is represented by the following judgement
%
$$
[[ UCtx ; LCtx_i |-s C ~> LCtx_o]]
$$
%
The judgement
takes in two contexts: $[[{UCtx}]]$, which holds all the unrestricted
atomic constraints\jp{which can be used?} (scaled by $[[{omega}]]$) and $[[{LCtx_i}]]$, which holds all the linear
atomic constraints\jp{to consume?}. Due to associativity and commutativity of $[[*]]$ we
can always factor any simple constraint into such a pair of contexts. The
unrestricted context $[[{UCtx}]]$ is an unordered set, while the
multiplicative contexts $[[{LCtx_i}]]$ and $[[{LCtx_o}]]$ are ordered lists.

Linearity requires treating constraints as consumable resources. This
is what $[[{LCtx_o}]]$ is for: it is the list of the hypotheses of
$[[{LCtx_i}]]$ which haven't been consumed when proving $[[{Q_w}]]$\jp{What's this? Not introduced?}. As
suggested by the notation, it is an output of the algorithm.

There are three simplifications compared to OutsideIn's solver:\jp{Another OutsideIn heavy part.}

\begin{itemize}
  \item As explained in Section~\ref{sec:constraint-generation}, our solver does
not need to do unification, so we do not return a substitution in the solver
judgement.
\item As discussed in Section~\ref{sec:constraint-domain}, we omit
  top-level constraints, for the sake of simplification.
  \item In case the wanteds can not be solved completely, OutsideIn also returns
a set of \emph{residual} constraints: these are the remaining subproblems to
solve. This set of residual constraints is simply quantified over in the type of
top-level function definitions that don't have signatures. In our solver we omit these residual constraints,
because we never infer a type scheme for a |let| without a signature
(though we revisit this hypothesis in Section~\ref{sec:let-generalisation}).

It is important to reiterate this
distinction: OutsideIn's residual constraints are constraints from the
goal that remain to be
solved, while our output constraints $[[{LCtx_o}]]$ are constraints from
the hypotheses that have yet to be used. OutsideIn's residuals are a
tool for type inference, while our output constraints are an internal
device for the solver.
\end{itemize}

Like OutsideIn~\cite[Section 5.5]{OutsideIn}, the main solver infrastructure
deals with solving the wanted constraints.
To handle simple wanted constraints, we will need  a domain-specific
\emph{simple-constraint solver} to be the algorithmic counterpart of the
abstract entailment relation of Section~\ref{sec:constraint-domain}. The
main solver will appeal to this simple-constraint solver when solving atomic
constraints.  The simple-constraint solver is represented by the following
judgement
%
$$
[[ UCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]
$$

It has a similar structure to the main solver, but only deals with atomic
constraints. Though the solver is parameterised in the particular choice of this
simple-constraint solver, we will give an instantiation in
Section~\ref{sec:simple-constr-solv}.

\begin{definition}[Simple-constraint solver]
\label{def:solver-soundness} The simple constraint solver must verify
that whenever $[[UCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]$, we have
$[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$ and $[[ctx(UCtx) * ctx(LCtx_i) ||-
pi.q * ctx(LCtx_o)]]$ (where the $[[{UCtx}]]$ and the $[[{LCtx}]]$ are
interpreted as simple constraints by taking the tensor of their atomic
constraints).
\end{definition}

\subsubsection{The solver}

To extend the solver to all wanted constraints, we use a linear proof
search algorithm based on the recipe given
by~\citet{resource-management-for-ll-proof-search}. Figure~\ref{fig:constraint-solver}
presents the rules of the constraint solver.

\begin{figure}
  \centering\small
  \drules[S]{$[[UCtx ; LCtx_i |-s C_w ~> LCtx_o ]]$}{Constraint solving}{Simple, Mult, ImplOne, Add, ImplMany}
  \caption{Constraint solver}
  \label{fig:constraint-solver}
\end{figure}

\csongor{Refer to lemma 5.2 here}

\begin{itemize}
  \item The~\rref*{S-Mult} rule proceeds by solving one side of a
conjunction first, then passing the output constraint to the other side.
The unrestricted context is shared between both sides.
  \item The~\rref*{S-Add} rule handles additive conjunction. Here, the linear
constraints are also shared between the branches (since additive conjunction is
generated from case expressions, only one of them is actually going to be
executed). Note that both branches must consume exactly the same
resources.
  \item Implications are handled by~\rref*{S-ImplOne} and~\rref*{S-ImplMany},
for solving linear and unrestricted wanted implications respectively. In
both cases, the assumption of the implication is split to\jp{between?} the unrestricted
$[[{omega . Q1}]]$ and the linear $[[{Q2}]]$ parts (this can be done deterministically).
When solving a linear implication, we add the assumptions to their respective context, and
proceed with solving the conclusion. Importantly (see
Section~\ref{sec:simple-constr-solv} below), the linear assumptions are
added to the front of the list. The side condition requires that the output
context is a subset of the input context: this is to ensure that the implication
actually consumes its assumption and doesn't leak it to the ambient context.
Solving unrestricted implications only allows the conclusion of the implication
to be solved using its own linear assumption, but none of the other linear
constraints. This is because unrestricted implications only use their own
assumption linearly, but use everything from the ambient context $[[{omega}]]$
times.

\end{itemize}

\subsubsection{A simple-constraint solver}
\label{sec:simple-constr-solv}

So far, the simple-constraint domain has been
an abstract parameter. In this section, though, let us give a concrete
domain which supports our examples.

For the sake of our examples, we really need very little: linear
constraints are abstract as far as the type checker is
concerned. So it is enough for the entailment relation
(Figure~\ref{fig:simpl-entailment}) to prove $[[{q}]]$ if and only if it
is already on the left hand side--- while respecting linearity.


\begin{figure}\centering\small
  \begin{subfigure}{\linewidth}
    \drules[Q]{$[[Q1 ||- Q2]]$}{Entailment relation}{Hyp,Prod,Empty}
    \caption{Entailment relation}
    \label{fig:simpl-entailment}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \drules[Simp]{$[[UCtx ; LCtx |-simp pi.q ~>
      LCtx_o]]$}{Simple-constraint solver}{AtomMany,AtomOneL,AtomOneU}
    \caption{Simple-constraint solver}
    \label{fig:simpl-solver}
  \end{subfigure}
  \caption{A stripped-down constraint domain}
  \label{fig:predicate-domain}
\end{figure}

The corresponding simple-constraint solver
(Figure~\ref{fig:simpl-solver}) is more interesting. The first thing
to notice is that the rules are deterministic: in any circumstances,
only one of the three rules can apply. This is a central tenet of
OutsideIn~\cite[Section 6.4]{OutsideIn}: it never makes guesses.

Figure~\ref{fig:simpl-solver} is also where the fact that the
$[[{LCtx}]]$ are lists comes into play. Indeed, \rref{Simp-AtomOneL}
takes great care to use the most recent occurrence of $[[{q}]]$
(remember that \rref{S-ImplOne} adds the new hypotheses on the top of
the list). To understand why, consider the following example:
\begin{code}
f :: FilePath -> IOL ()
f fp =   do  {  pack h <- openFile fp
             ;  let  {  cl :: constraint (Open h) =>. IOL ()
                     ;  cl = closeFile h }
             ;  cl }
\end{code}
In this example it is quite clear that the programmer meant for
|closeFile| to use |Open h| introduced locally in |cl|. In fact this
is the only possible way to attribute constraints.

Another interesting feature of the solver (Figure~\ref{fig:simpl-solver}) is that
no rule solves a linear constraint if it appears both in the
unrestricted and the linear context.
Consider the following (contrived) \textsc{api}:
\begin{code}
class constraint (C)

giveC :: (constraint (C) => Int) -> Int
useC :: constraint (C) =>. Int
\end{code}
|giveC| gives an unrestricted copy of |constraint (C)| to some continuation, while |useC|
uses |constraint (C)| linearly. Now consider a consumer of this \textsc{api}:
\begin{code}
bad :: constraint (C) =>. (Int, Int)
bad = (giveC useC, useC)
\end{code}
It is possible to give a type derivation to |bad| in the qualified type system
of Section~\ref{sec:qualified-type-system}. In fact, the constraint assignment
is unambiguous: the left-most |useC| must use the unrestricted |constraint (C)|, while the
right-most must use the linear |constraint (C)|. This assignment, however, would require of
the constraint solver to make a guess when solving the left-most |useC|. So in
the spirit of OutsideIn, |bad| is rejected.

\section{Desugaring}
\label{sec:desugaring}

The semantics of our language is given by desugaring it into
a simpler core language: a mild\jp{what is 'mild' when applied to lambda-calculi? explain.} variant of the $λ^q$
calculus~\cite{LinearHaskell}. We
define the core language's type system here; its operational semantics
is the same, \emph{mutatis mutandis}, as that of Linear Haskell.

\subsection{The core calculus}
\label{sec:core-calculus}

\begin{figure}
  \centering\small
  $$
  \begin{array}{lcll}
    [[{s}]] & \bnfeq & [[{forall as. t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & ... \bnfor [[{exists as. t o- u}]] & \text{Types} \\
    [[{e}]] & \bnfeq & ... \bnfor [[{pack (e1, e2)}]] \bnfor [[{unpack (y,x)=e1 in e2}]] & \text{Expressions}
  \end{array}
  $$

  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Pack,Unpack}
  \caption{Core calculus (subset)}
  \label{fig:core-typing-rules}\label{fig:core-grammar}
\end{figure}

The core calculus is a variant of the type system defined in
Section~\ref{sec:qualified-type-system}, but without constraints. That is, the evidence for constraints is passed explicitly in this core calculus.
Figure~\ref{fig:core-typing-rules}
highlights the differences from the qualified system:
\begin{itemize}
  \item Type schemes $[[{s}]]$ do not support qualified types.
  \item Existentially quantified types $[[{exists as. t2 o- t1}]]$ now represent a
(linear) pair of values. Accordingly, $\kpack$ and $\kunpack$ operate on pairs.
\end{itemize}
%
The differences between our core calculus and $λ^q$ are as follows
\begin{itemize}
\item We don't have multiplicity polymorphism.
\item We need, on the other hand, type polymorphism.
\item Polymorphism is implicit rather than explicit. This is not an
  essential difference but it simplifies the presentation.
\item We have existential types. These can be realised in regular Haskell as a
  family of \textsc{gadt}s.
\end{itemize}
%
In addition, we assume the existence of data types
\begin{itemize}
\item $[[{t1 * t2}]]$ with sole constructor
  $[[ (,) : forall a b. a ->_1 b ->_1 a * b ]]$. We will write $[[(e1,
  e2)]]$ for $[[{(,) e1 e2}]]$.
\item $[[{unit}]]$ with sole constructor $[[() : unit]]$.
\item $[[{Ur t}]]$ with sole constructor $[[ Ur : forall a. a ->_omega
  Ur a]]$
\end{itemize}
%
These are regular data types and constructors of the language: they
are consumed with $\kcase$.

\subsection{Inferred constraints}
\label{sec:ds:inferred-constraints}

Using Lemma~\ref{sec:constraint-generation-soundness} together with
Definition~\ref{def:solver-soundness} we know that if
$[[G |-> e : t ~> C]]$ and $[[UCtx ; LCtx |-s C~> empty ]]$, then
$[[ctx(UCtx) * ctx(LCtx)  ; G |- e : t]]$.
%
It only remains to desugar derivations of $[[Q;G|-e : t]]$ into the
core calculus.

\subsection{From qualified to core}
\label{sec:ds:from-qualified-core}

\subsubsection{Evidence}
In order to desugar derivations of the qualified system to the core
calculus, we use the classic technique known as evidence-passing
style\footnote{This technique is also often called dictionary-passing
  style because, in the case of type classes, evidences are
  dictionaries, and because type classes were the original form of
  constraints in Haskell.}.
%
To do so, we require some more material from
constraints. Namely, we assume a type $[[{Ev(q)}]]$ for each atomic
constraint\jp{q?}, defined in Figure~\ref{fig:evidence}.  It is extended to all simple constraints.
It ought to be noted that $[[{Ev(Q)}]]$ is not technically well defined, as the
rules in Section~\ref{sec:constraint-domain} define the syntax as being
quotiented by associativity and commutativity of the tensor product, and
idempotence of unrestricted constraints. But core language data types (or
Haskell's for that matter) are not so quotiented. It's not actually hard to fix
the imprecision: give a name to each atomic constraint, and operate on canonical
representatives of the equivalence classes.  This is actually essentially how
\textsc{ghc} deals with constraints today. It is also the mechanism that our
prototype implementation (see Section~\ref{sec:implementation}) uses. However,
we prefer to keep this section a little imprecise, in order to save the
rest of the article from the non-trivial extra tedium that the more precise
presentation entails.\jp{The end of this paragraph feels like it was written in haste. ``Oh btw this does not work, but morally it does. Please move on!'' Rewrite or put in a footnote?}

Furthermore we require that for every $[[{Q1}]]$ and $[[{Q2}]]$
such that $[[Q1 ||- Q2]]$, there is a (linear) function
$[[Ev(Q1 ||- Q2) : Ev(Q1) ->_1 Ev(Q2)]]$.

Let us now define a family of functions $[[{Ds(Hole)}]]$ to translate
the type schemes, types, and typing derivations of the qualified system into the
types, type schemes, and terms of the core calculus.

\subsubsection{Translating types}
Type schemes $[[{s}]]$ are translated by turning the implicit argument $[[{Q}]]$
into an explicit one of type $[[{Ev(Q)}]]$. Translating types $[[{t}]]$ proceeds as
expected.

\begin{minipage}{0.5\linewidth}
%
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(forall as. Q =o t)}]] & = & [[{forall as. Ev(Q) ->_1 Ds(t)}]] \\
  \end{array}
\right.
$$
%
\end{minipage}%
\begin{minipage}{0.5\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(t1 ->_pi t2)}]] & = & [[{Ds(t1) ->_pi Ds(t2)}]] \\
    [[{Ds(exists as. t o= Q)}]] & = & [[{exists as. Ds(t) o- Ev(Q)}]]
  \end{array}
\right.
$$
\end{minipage}

\subsubsection{Translating terms}
Let us finally build, given a derivation $[[Q;G |- e : t]]$, an expression
$[[Ds(z;Q;G |- e : t)]]$, such that
$[[G, z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ (for some variable
$[[{z}]]$). Even though we abbreviate the derivation as its
conclusion, the translation is defined recursively on the
whole typing derivation, in particular we have access to typing rule
premises in the body of the definition.
%
We present some of the interesting cases in Figure~\ref{fig:desugaring}.
\begin{figure}
  \centering
  \begin{subfigure}{0.3\linewidth}%

\small
$$
\left\{
  \begin{array}{lcl}
    [[{Ev(1.q)}]] & = & [[{Ev(q)}]] \\
    [[{Ev(omega.q)}]] & = & [[{Ur (Ev(q))}]] \\
    [[{Ev(Empty)}]] & = & [[{unit}]] \\
    [[{Ev(Q1 * Q2)}]] & = & [[{Ev(Q1) * Ev(Q2)}]]
  \end{array}
\right.
$$
  \caption{Evidence passing}
  \label{fig:evidence}
  \end{subfigure}\hfill
  \begin{subfigure}{0.7\linewidth}%
%{
%format |- = "[[|-]]"
%format ||- = "[[||-]]"
%format G = "[[{G}]]"
%format G1 = "[[{G1}]]"
%format G2 = "[[{G2}]]"
%format Q1 = "[[{Q1}]]"
%format Q2 = "[[{Q2}]]"
%format * = "[[*]]"
%format Ds(x) = "\ottkw{Ds}\,\ottsym{(}" x "\ottsym{)}"
%format Ev(x) = "\ottkw{Ev}\,\ottsym{(}" x "\ottsym{)}"
%format unpack = "\ottkw{unpack}"
%format u = "[[{u}]]"
%format OneOf (a) (b) = a"\mathop{:_{1}}"b
%format Sub t as bs = t "[" as "/" bs "]"
%format case_1 = "[[case_1]]"
%format e1 = "[[{e1}]]"
%format e2 = "[[{e2}]]"
%format t1 = "[[{t1}]]"
%format as = "[[{as}]]"
%format ts = "[[{ts}]]"
%format let_1 = "[[let_1]]"
%format z1 = "[[{z1}]]"
%format z2 = "[[{z2}]]"
%format z2' = "[[{z2}]]''"
$$\small
\left\{
  \;
  \begin{minipage}{0.5\linewidth}
\begin{code}
Ds(z;Q;G |- x : Sub u ts as) = x z
Ds(z;Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t)  =
  case_1 z of { (z1, z2) ->
    unpack (z',x) = Ds(z';Q1;G1 |- e1 : exists as. t1 .<= Q) in
    let_1 z2' = (z2,z') in
    Ds(z2';Q2 * Q;G2, OneOf x t1 |- e2 : t) }
Ds(z;Q;G |- e : t)  =
  let_1 z' = Ev(Q ||- Q1) z in Ds(z';Q1;G |- e : t)
...
\end{code}
  \end{minipage}
\right.
$$
%}
  \caption{Desugaring (subset)}
  \label{fig:desugaring}
  \end{subfigure}
  \caption{Evidence passing and desugaring}
\end{figure}

The cases correspond to the~\rref*{E-Var},~\rref*{E-Unpack}, and~\rref*{E-Sub} rules, respectively.
Variables are stored with qualified types in the environment, so they get
translated to functions that take the evidence as argument. Accordingly, the evidence
is inserted by passing $[[{z}]]$ as an argument.
Handling $\kunpack$ requires splitting the context into two: $[[{e1}]]$ is desugared as a pair, and the evidence
it contains is passed to $[[{e2}]]$. Finally, subsumption summons the function corresponding to the entailment relation $[[Q ||- Q1]]$
and applies it to $[[{z}]]$ : $[[{Ev(Q)}]]$ then proceeds to desugar $[[{e}]]$ with the resulting evidence for $[[{Q1}]]$.
Crucially, since $[[{Ds(Hole)}]]$ is defined on \emph{derivations}, we can access the premises used in the rule.
Namely, $[[{Q1}]]$ is available in this last case from the~\rref*{E-Sub} rule's premise.

It is straightforward by induction, to verify that, indeed,
$[[G, z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ as
announced.

Thanks to the desugaring machinery, the semantics of a language with linear
constraints can be understood in terms of a simple core language with linear
types, such as $λ^q$, or indeed, \textsc{ghc} Core.

\section{Implementation}
\label{sec:implementation}

We have written a prototype implementation of linear constraints on top of \textsc{ghc} 9.1, a version that already
ships with the \emph{LinearTypes} extension. Function arrows (|->|) and context arrows
(|=>|) share the same internal representation in the typechecker, differentiated
only by a boolean flag. Thus, the \emph{LinearTypes} implementation effort has already
laid down the bureaucratic ground work of annotating these arrows with
multiplicity information.

The key changes affect constraint generation and constraint solving. Constraints
are now annotated with a multiplicity, according to the context from
which they arise. With \emph{LinearTypes}, \textsc{ghc} already scales the usage
of term variables. We simply modified the scaling function to capture all the
generated constraints and re-emit a scaled version of them -- a fairly local
change.

The constraint solver maintains a set of given constraints (the \emph{inert} set in \textsc{ghc} jargon),
which corresponds to the $[[{UCtx}]]$ and $[[{LCtx}]]$ contexts in our solver
judgements in Section~\ref{sec:constraint-solver}. A property of the inert set
is that constraints contained in it do not interact pairwise. These interactions are
dictated by the constraint domain. For example, equality constraints interact
with other constraints by applying a substitution.

The treatment of implication constraints is of particular interest.
Implications, by their nature, introduce assumptions which do not necessarily
hold in the outer context, therefore recording these assumptions in the inert
set is a destructive operation. To ensure proper scoping, \textsc{ghc} creates a fresh
copy of the inert set for each nested implication that it solves, so these
destructive operations do not leak out. We modify the inert set so that for each
constraint stored in it, the level (or depth) of the implication is recorded
alongside it. Each interaction with nested assumptions (which might give rise to
additional derived givens) is recorded at the appropriate level and
multiplicity (decomposing a constraint tuple into its constituent parts is done
by the simplifier, which must now record the multiplicities of the components).\jp{This is very tough to follow if one does not already have the solver in one's head. Also: wall of text.}

Atomic wanteds are then solved by finding a matching given in the inert set (or
a top-level given, which is not relevant to linear constraints). When a linear
given is used to solve a linear wanted, our prototype removes the given from the
inert set so it can not be used again. Before, the inert set only held a single
copy of each given, but now it must hold multiple copies of linear givens,
together with their implication level. When solving an atomic wanted, the
matching given with the largest implication level (i.e. the innermost given) is
selected, as per the \rref*{Simp-AtomOneL} rule.

When an implication is finally solved, we must check that every linear
constraint introduced in this implication was consumed, which is done by
checking that the level of every linear constraint in the inert set is less than
the implication.

When a linear equality constraint is encountered, it is automatically promoted
to an unrestricted one and handled accordingly. This may happen in many
different scenarios, as the entailment relation of \textsc{ghc}'s constraint domain does
produce many equalities, thus we need to ensure they are turned into
unrestricted before interacting with the inert set. \textsc{ghc} Core already represents
the equality constraint as a boxed type, so we can simply modify it to store an
unrestricted payload.

As constraint solving proceeds, the compiler pipeline constructs a term in an
intrinsically typed language
known as \textsc{ghc} Core~\cite{system-fc}.
In Core, type class constraints are turned into
explicit evidence (see Section~\ref{sec:desugaring}). Thanks to being fully
annotated, Core has decidable typechecking which is useful in sanity
checking modifications to the compiler. Thus, the soundness of our
implementation is verified by the Core typechecker, which already supports
linearity.

\section{Extensions}
\label{sec:design-decisions}

The system presented in Section~\ref{sec:qualified-type-system} and
Section~\ref{sec:type-inference} is already capable of supporting the examples
in Section~\ref{sec:what-it-looks-like} and Section~\ref{sec:memory-ownership}. In this
section, we consider some potential avenues for extensions.

\subsection{let generalisation}
\label{sec:let-generalisation}

As discussed in Section~\ref{sec:constraint-generation}, the~\rref*{G-Let} rule
of our constraint generator does not generalise the type of let bindings.
Not doing let generalisation is in line with \textsc{ghc}'s existing
behaviour~\cite[Section 4.2]{OutsideIn}. There, this behaviour was guided
by concerns around inferring type variables, which is harder in the
presence of local equality assumptions (that is, \textsc{gadt} pattern
matching).

In this section, however, we argue that generalising over linear constraints
may, in fact, improve user experience.
Let us revisit the |firstLine| example from Section~\ref{sec:introduction}, but
this time, instead of executing |closeFile| directly, we assign it to a variable
in a let binding:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp =   do  {  pack' h <- openFile fp
                     ;  let closer = closeFile h
                     ;  pack' xs <- readLine h
                     ;  closer
                     ;  return xs }
\end{code}
This program looks reasonable; however, it is rejected. The type of
|closer| is |IOL ()|, which means that the definition of |closer|\jp{rename to |closeOp|? (Closer is normally an adjective and it makes me double take at every occurrence.)}
consumes the linear constraint |Open h|. So, by the time we attempt
|readLine h|, the constraint is no longer available.

What the programmer really meant, here, was for |closer| to have type
|constraint (Open h) =>. IOL ()|. After all a |let| definition is not part of the
sequence of instructions, it is just a definition for later; it is not
supposed to consume the current state of the file. With no let
generalisation, the only way to give |closer| the type |constraint (Open h) =>. IOL
()| is to give |closer| a type signature. In current \textsc{ghc}, we
can't write that signature down, since the type variable |h| is not
bound in the program text, and there is no syntax for binding it
(Even though \citet{variables-in-patterns} proposes a syntax for it). But
even ignoring this, it would be rather unfortunate if the default
behaviour of |let|, in presence of linear constraints, almost never was
what the programmer wants.

To handle $\klet$-generalisation, let us consider the following rule
%
$$
\drule{G-LetGen}
$$
%
This rule is non-deterministic, because it requires finding $[[{Q_r}]]$ and
$[[{Q}]]$. We can modify the constraint solver of
Section~\ref{sec:constraint-solver} to find $[[{Q_r*Q}]]$: it's the
residual constraint, from OutsideIn, that we have omitted. But we
still have to determine how to split the residual into $[[{Q_r}]]$ and
$[[{Q}]]$. What we would like to say is ``$[[{Q}]]$ is the set of linear
constraints''. But it's not clear how to make it formal.

Any predictable strategy would do: as long as it's an instance of the
\rref*{G-LetGen} rule, constraint generation will be sound. Experience
will tell whether we can find a better suited strategy than the current
one, which never generalises any constraint.

\subsection{Empty cases}
\label{sec:empty-cases}

Throughout the article we have assumed that $\kcase$-expressions
always have a non-empty list of alternatives. This is, incidentally,
also how Haskell originally behaved; though \textsc{ghc} now has an
|EmptyCase| extension to allow empty lists of alternatives.

Not allowing empty lists of alternatives is, therefore not terrible in
principle. Though it makes empty types more awkward than they need to
be, and, of course, to support the entirety of \textsc{ghc}, we will
need to support empty lists of alternatives.

The reason why it has been omitted from the rest of the article is
that generating constraints for an empty requires an $0$-ary version
of $[[{C1&C2}]]$, usually written $[[{Top}]]$ in Linear Logic. The
corresponding entailment rule would be
$$
\drule{C-Top}
$$
That is $[[{Top}]]$ is unconditionally true, and can consume any number
of linear given constraints.\jp{Indeed, the corresponding program consumes the empty type, so it is safe to do so.} The \rref*{C-Top} rule thus induces a
considerable amount of non-determinism. Eliminating the
non-determinism induced by $[[{Top}]]$ is ultimately
what~\citet{resource-management-for-ll-proof-search} build up
to. Their methods can be adapted to the constraint solver of
Section~\ref{sec:constraint-solver} without any technical
difficulty. We chose, however, to keep empty cases out the
presentation because has a very high overhead and would distract from
the point. Instead, we refer readers
to~\citet[Section 4]{resource-management-for-ll-proof-search} for a
careful treatment of $[[{Top}]]$.

\section{Related work}
\label{sec:related-work}

\paragraph{Rust}

The memory ownership example of Section~\ref{sec:memory-ownership} is
strongly inspired by Rust. There seems to be something inescapable
about Rust's model (itself inspired by C++'s move semantics) of owned
and borrowed references, as the \textsc{api} in
Section~\ref{sec:memory-ownership} follows naturally from the
requirements (namely a pure interface for mutable arrays with the
ability to freeze arrays). It is not
clear that any other \textsc{api} could be essentially different.\jp{This paragraph does not seem to say anything. Delete?}

Rust is built with ownership and borrowing for memory
management from the ground up. As a consequence, it has a much more convenient syntax
than Linear Haskell with linear constraints can propose. Rust's
convenient syntax comes at the price that it is almost impossible to
write tail-recursive functions, which is surprising from the
perspective of a functional programmer.

On the other hand, the focus of Linear Haskell, as well as this paper,
is to provide programmers with the tools to create safe interfaces and
libraries. The language itself is agnostic about what linear
constraints mean. Although linear constraints don't have the
convenience of Rust's syntax, we expect that they will support a
greater variety of abstractions. Even though, Rust programmers have
come up with varied abstractions which leverage the borrowing
mechanism to support applications going beyond memory management (for
instance, safe file handling), it is unclear that all applications
supported by linear constraints are reducible to the borrowing
mechanism.

\paragraph{Languages with capabilities}

Both Mezzo~\cite{mezzo-permissions} and
\textsc{ats}~\cite{AtsLinearViews} served as inspiration for the design
of linear constraints. Of the two, Mezzo is more specialised, being
entirely built around its system of capabilities, and \textsc{ats} is
the closest to our system with an explicit appeal to linear logic, and
the capabilities (known as \emph{stateful views}) being a part of a
bigger language\jp{Cannot parse this sentence?}. However, \textsc{ats} does not have full inference of
capabilities. % , which the programmer may have to fill in.

Other than that, the two systems have a lot of similarities. They have a
finer-grained capability system than is expressible in Rust (or our
in our encoding of it in Section~\ref{sec:memory-ownership}) which makes it possible to change
the type of a reference cell upon write. They also eschew scoped
borrowing in favour of more traditional read and write capabilities. In
exchange, neither Mezzo nor \textsc{ats} support $O(1)$ freezing like
in Section~\ref{sec:memory-ownership}.

Mezzo, being geared towards functional programming, does support
freezing, but freezing a nested data structure requires traversing it.
As far as we know, \textsc{ats} doesn't support
freezing. \textsc{Ats} is more oriented towards system programming.

%if False
We chose an example in the style of rust for
Section~\ref{sec:memory-ownership} because freezing arrays of arrays
in $O(1)$ was one of the initial motivations of this article. However,
a Mezzo or \textsc{ats} style of capabilities could certainly be
encoded within linear constraints.
%endif

Linear constraints are more general than either Mezzo or \textsc{ats},
while maintaining a considerably simpler inference algorithm, and at
the same time supporting a richer set of constraints (such as \textsc{gadt}s). This
simplicity is a benefit of abstracting over the simple-constraint
domain. In fact, it should be possible to see Mezzo or \textsc{ats} as
particular instantiations of the simple-constraint domain, with linear
constraints providing the general inference mechanism.

Though both Mezzo and \textsc{ats} have an advantage that we don't:
they assume that their instructions are properly sequenced, whereas
basing linear constraints on Haskell, a lazy language, we are forced
to make sequencing explicit in \textsc{api}s.

\paragraph{Logic programming}

There are a lot of commonalities between \textsc{ghc}'s constraint and logic
programs. Traditional type classes can be seen as Horn Clauses programs, much
like Prolog's programs. \textsc{ghc} puts further restrictionss in order to
avoid backtracking for speed and predictability.

The recent addition of quantified
constraints~\cite{quantified-constraints} extends type class
resolution to Hereditary Harrop programs. A generalisation of the
Hereditary Harrop fragment to linear logic, described by~\citet{hh-ll},
is the foundation of the Lolli language. Together with this fragment,
the authors coin the notion of \emph{uniform} proof. A fragment where
uniform proofs are complete supports goal-oriented proof search, like
Prolog does.

Uniformity is used in the proof of
Lemma~\ref{lem:inversion}, which, in turn is used in the proof of the
soundness lemma~\ref{lem:generation-soundness}. This seems to indicate
that goal-oriented search is baked into the definition of OutsideIn. An
immediate consequence of this observation, however, is that
the fragment of linear logic described by~\citet{hh-ll} (and
for which~\citet{resource-management-for-ll-proof-search} provides a
refined search strategy) contains the Hereditary Harrop fragment of
intuitionistic logic guarantees that quantified constraints don't
break our proofs.

\section{Conclusion}
\label{sec:conclusion}

We showed how a simple linear type system like that of Linear
Haskell can be extended with an inference mechanism which lets the
compiler manage some of the additional complexity of linear types
instead of the programmer. Linear constraints narrow the gap between linearly
typed languages and dedicated linear-like typing disciplines such as Rust's,
Mezzo's, or \textsc{ats}'s.

We also demonstrate how an existing constraint solver can be extended to handle
linearity. Our design of
linear constraints fits nicely into Haskell. Indeed, linear constraints can be
thought of as an extension of Haskell's type class mechanism. This way, the
design also integrates well into \textsc{ghc}, as demonstrated by our prototype
implementation, which required modest changes to the compiler. Remarkably, all we needed was to
adapt~\citet{resource-management-for-ll-proof-search} to the OutsideIn
framework. It is also quite serendipitous that the notion of uniform
proof from~\citet{hh-ll}, which was introduced to prove the
completeness of a proof search strategy, ends up being crucial to the
soundness of constraint generation.

In some cases, like the file example of Section~\ref{sec:introduction},
linear constraints are a mere convenience that reduce line noise and
make code more idiomatic. But the memory management \textsc{api} of
Section~\ref{sec:memory-ownership} is not feasible without
linear constraints. Certainly, ownership proofs could be managed manually, but
it is hard to imagine a circumstance where this tedious task would be worth the
cost.

This, really, is the philosophy of linear constraints: lowering
the cost of linear types so that more theoretical applications become
practical applications. And we achieved this at a surprisingly low price: teaching
linear logic to \textsc{ghc}'s constraint solver.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\newpage

\appendix

\section{Full descriptions}
\label{sec:appendix:full-descriptions}

In this appendix, we give, for reference, complete descriptions of the
type systems, functions, etc… that we have abbreviated in the main
body of the article.

\subsection{Core calculus}
\label{sec:appendix:core-calculus}

This is the complete version of the core calculus described in
Section~\ref{sec:core-calculus}. The full grammar is given by
Figure~\ref{fig:full:core-grammar} and the type system by
Figure~\ref{fig:full:core-typing-rules}.

\begin{figure}
  \centering\small
  $$
  \begin{array}{lcll}
    [[{a}]], [[{b}]] & \bnfeq & \ldots & \text{Type variables} \\
    [[{x}]], [[{y}]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[{K}]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[{s}]] & \bnfeq & [[{forall as. t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & [[{a}]] \bnfor [[{exists as. t o- u}]] \bnfor [[{t1 ->_pi t2}]]
                            \bnfor [[{T ts}]] & \text{Types} \\
    [[{G}]], [[{D}]] & \bnfeq & [[{empty}]] \bnfor [[{G, x:_pi s}]] &
                                                              \text{Contexts} \\
    [[{e}]] & \bnfeq & [[{x}]] \bnfor [[{K}]] \bnfor [[{\x. e}]] \bnfor [[e1
                     e2]] \bnfor [[{pack (e1, e2)}]] & \text{Expressions}\\
                 &\bnfor & [[unpack (y,x)=e1 in
                           e2]] \bnfor [[{case_pi e of { alts }}]] &\\
                 &\bnfor & [[let_pi
                           x=e1 in e2]] \bnfor [[{let_pi x : s = e1 in e2}]] &
  \end{array}
  $$
  \caption{Grammar of the core calculus}
  \label{fig:full:core-grammar}
\end{figure}

\begin{figure}
  \centering\small
  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Var,Abs,App,Pack,Unpack,Let,Case}
  \caption{Core calculus type system}
  \label{fig:full:core-typing-rules}
\end{figure}

\subsection{Desugaring}
\label{sec:appendix:desugaring}

The complete definition of the desugaring function from
Section~\ref{sec:desugaring} can be found in
Figure~\ref{fig:full:desugaring}.

For the sake of concision, we allow ourselves to write nested patterns
in case expressions of the core language. Desugaring nested patterns
into atomic case expression is routine.

In the complete description, we use a device which was omitted in the
main body of the article. Namely, we'll need a way to turn every
$[[{Ev(omega.Q)}]]$ into an $[[{Ur(Ev(Q))}]]$. For any
$[[e : Ev(omega.Q)]]$, we shall write $[[urify(Q;e) :
Ur(Ev(omega.Q))]]$. As a shorthand, particularly useful in nested
patterns, we will write $[[{case_pi e of {urified(Q;x) -> e'}}]]$ for
$[[{case_pi urify(Q;e) of {Ur x -> e'}}]]$.
%
$$
\left\{
  \begin{array}{lcl}
    [[{urify(Empty;e)}]]& = & [[{case_1 e of {() -> Ur ()}}]] \\
    [[{urify(1.q;e)}]] & = & [[{e}]] \\
    [[{urify(omega.q;e)}]] & = & [[{case_1 e of {Ur x -> Ur (Ur x)}}]] \\
    [[{urify(Q1*Q2;e)}]] & = & [[{case_1 e of {(urified(Q1;x), urified(Q2;y)) -> Ur (x,y)}}]]
  \end{array}
\right.
$$
We will omit the $[[{Q}]]$ in $[[{urify(Q;e)}]]$ and write
$[[{urify(e)}]]$ when it can be easily inferred from the context.


\begin{figure}
  \centering\small

%{
%format (Ds(x)) = "\ottkw{Ds}\,\ottsym{(}" x "\ottsym{)}"
%format * = "[[*]]"
%format ** = "\mathbin{⋅}"
%format ->. = "\to_{1}"
%format ->> = "\to_{\pi}"
%format ->>> = "\to_{\omega}"
%format Ev(x) = "\ottkw{Ev}\,\ottsym{(}" x "\ottsym{)}"
%format G = "[[{G}]]"
%format G1 = "[[{G1}]]"
%format G2 = "[[{G2}]]"
%format ListOf (b) = "\overline{" b "}"
%format Of (w) (a) (b) = a"\mathop{:_{" w "}}"b
%format OneOf (a) (b) = a"\mathop{:_{1}}"b
%format Q1 = "[[{Q1}]]"
%format Q2 = "[[{Q2}]]"
%format Sub t as bs = t "[" as "/" bs "]"
%format alts = "[[alts]]"
%format as = "[[{as}]]"
%format case_1 = "[[case_1]]"
%format case_omega = "[[case_omega]]"
%format e1 = "[[{e1}]]"
%format e2 = "[[{e2}]]"
%format ei = "[[{ei}]]"
%format let_1 = "[[let_1]]"
%format let_omega = "[[let_omega]]"
%format omega = "[[{omega}]]"
%format pi = "[[{pi}]]"
%format pii = "[[{pii}]]"
%format t = "[[{t}]]"
%format t1 = "[[{t1}]]"
%format t2 = "[[{t2}]]"
%format ts = "[[{ts}]]"
%format u = "[[{u}]]"
%format ui = "[[{ui}]]"
%format unpack = "\ottkw{unpack}"
%format urified(x) = "\underline{" x "}"
%format us = "[[{us}]]"
%format xi = "[[{xi}]]"
%format xsi = "[[{xsi}]]"
%format z1 = "[[{z1}]]"
%format z2 = "[[{z2}]]"
%format z2' = "[[{z2}]]''"
%format |- = "[[|-]]"
%format ||- = "[[||-]]"
$$
\left\{\;
\begin{minipage}{0.8\linewidth}
\begin{code}
Ds(z;Q;G |- x : u[ts/as])  =
    x z
Ds(z;Q;G |- \x.e : t1 ->> t2) =
  \x. Ds(z;Q;G, Of pi x t1 |- e : t2)
Ds(z;Q1*Q2; G1+G2 |- e1 e2 : t) =
  case_1 z of { (z1, z2) ->
    (Ds(z1;Q1;G1 |- e1 : t1 ->. t)) (Ds(z2;Q2;G2 |- e2 : t1)) }
Ds(z;Q1* omega ** Q2; G1+ omega ** G2 |- e1 e2 : t) =
  case_1 z of { (z1, urified(z2)) ->
    (Ds(z1;Q1;G1 |- e1 : t1 ->>> t)) (Ds(z2;Q2;G2 |- e2 : t1)) }
Ds(z;Q * Sub Q1 us as;G |- pack e : exists as. t .<= Q1) =
  case_1 z of { (z', z'') ->
    pack (z'', Ds(z'; Q ; G |- e : Sub t us as))}
Ds(z;Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    unpack (z',x) = Ds(z';Q1;G1 |- e1 : exists as. t1 .<= Q) in
    let_1 z2' = (z2,z') in
    Ds(z2';Q2 * Q;G2, OneOf x t1 |- e2 : t)}
Ds(z;Q1 * Q2 ;G1+G2 |- let_1 x = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    let_1 x : Ev(Q) ->. t1 = Ds(z1;Q1*Q;G1 |- e1 : t1)
    in Ds(z2;Q2;G2, OneOf x t1 |- e2 : t)}
Ds(z;omega ** Q1 * Q2 ;omega ** G1+G2 |- let_omega x = e1 in e2 : t) =
  case_1 z of { (urified(z1), z2) ->
    let_omega x : Ev(Q) ->. t1 = Ds(z1;Q1*Q;G1 |- e1 : t1) in
    Ds(z2;Q2;G2, Of omega x t1 |- e2 : t)}
Ds(z;Q1 * Q2 ;G1+G2 |- let_1 x : forall as. Q =>. t1 = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    let_1 x : forall as. Ev(Q) ->. t1 = Ds(z1;Q1*Q;G1 |- e1 : t1) in
    Ds(z2;Q2;G2, OneOf x (forall as. Q =>. t1) |- e2 : t)}
Ds(z;omega ** Q1 * Q2 ;omega ** G1+G2 |- let_omega x : forall as. Q =>. t1 = e1 in e2 : t) =
  case_1 z of { (urified(z1), z2) ->
    let_omega x : forall as. Ev(Q) ->. t1 = Ds(z1;Q1*Q;G1 |- e1 : t1) in
    Ds(z2;Q2;G2, Of omega x t1 |- e2 : t)}
Ds(z;omega ** Q1*Q2;omega ** G1+G2 |- case_1 e of { alts } : t)  =
  case_1 z of { (urified(z1), z2) ->
    case_1 (Ds(z1;Q1;G1 |- e : T ts)) of
      { ListOf(K xsi -> Ds(z2; Q2; G2, ListOf(Of ((pi ** pii)) xi (Sub ui ts as)) |- ei : t))}}
Ds(z;Q1*Q2;G1+G2 |- case_omega e of { alts } : t)  =
  case_1 z of { (z1, z2) ->
    case_omega (Ds(z1;Q1;G1 |- e : T ts)) of
      { ListOf(K xsi -> Ds(z2; Q2; G2, ListOf(Of ((pi**pii)) xi (Sub ui ts as)) |- ei : t))}}
\end{code}
\end{minipage}
\right.
$$

%}

  \caption{Desugaring}
  \label{fig:full:desugaring}
\end{figure}

\section{Proofs of the lemmas}
\label{sec:appendix:proofs-lemmas}

\setcounter{subsection}{3}
\subsection{Lemmas on the qualified type system}
\label{sec:appendix:qual-type-syst}

\begin{proof}[Proof of Lemma~\ref{lem:q:scaling}]
  Let us prove this lemma by induction on the syntax of $[[{Q2}]]$:
  \begin{itemize}
  \item If $[[{Q2}]] = [[{rho.q}]]$, then $[[pi.Q1 ||- (pi.rho).q]]$ holds by
    Definition~\ref{def:entailment-relation}.
  \item If $[[{Q2}]] = [[{Q2' * Q2''}]]$, then, by Definition~\ref{def:entailment-relation}, we know that
    $[[{Q1}]] = [[{Q1' * Q1''}]]$ for some $[[{Q1'}]]$ and $[[{Q1''}]]$, and
    that
    $[[Q1' ||- Q2']]$ and $[[Q1'' ||- Q2'']]$. By induction hypothesis,
    we have
    $[[pi.Q1' ||- pi.Q2']]$ and $[[pi.Q1'' ||- pi.Q2'']]$. From which it
    follows that
    $[[pi.Q1 ||- pi.Q2]]$.
  \item If $[[{Q2}]]=[[{Empty}]]$, then, by
    Definition~\ref{def:entailment-relation}, we know that
    $[[{Q1}]]=[[{omega.Q1'}]]$, for some $[[{Q1'}]]$
    and the result follows from Lemma~\ref{lem:simples:monoid-action},
    since $[[{pi.Q1}]]=[[{pi.(omega.Q1')}]]=[[{omega.Q1'}]]$.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:q:scaling-inversion}]
  Let us prove this lemma by induction on the syntax of $[[{Q2}]]$
  \begin{itemize}
  \item If $[[{Q2}]] = [[{rho.q}]]$, then, by Definition~\ref{def:entailment-relation}, there exists
    $[[{Q1'}]]$ such that $[[{Q1}]]=[[{pi.Q1'}]]$ and $[[Q1' ||- rho.q]]$.
  \item If $[[{Q2}]] = [[{Q2' * Q2''}]]$, then, by Definition~\ref{def:entailment-relation}, we know
    that
    $[[{Q1}]] = [[{Q1' * Q1''}]]$ for some $[[{Q1'}]]$ and $[[{Q1''}]]$, and
    that
    $[[Q1' ||- pi.Q2']]$ and $[[Q1'' ||- pi.Q2'']]$ (remember that, by
    definition, $[[{pi.Q2}]] = [[{pi.Q2' * pi.Q2''}]]$). By induction hypothesis,
    we have
    constraints $[[{Q'}]]$ and $[[{Q''}]]$, such that $[[{Q1'}]] =
    [[{pi.Q'}]]$ and $[[{Q1''}]] = [[{pi.Q''}]]$, and $[[Q' ||- Q2']]$ and
    $[[Q'' ||- Q2'']]$.
    It follows that $[[{Q1}]] = [[{pi.(Q' * Q'')}]]$ and
    $[[Q' * Q'' ||- Q2]]$.
  \item If $[[{Q2}]]=[[{Empty}]]$, then, since $[[{pi.Empty}]]=[[{Empty}]]$, by
    Definition~\ref{def:entailment-relation}, $[[{Q1}]]=[[{Empty}]]$ and
    the result is immediate.
  \end{itemize}
\end{proof}

\begin{lemma}\label{lem:simples:monoid-action}
  The following equality holds $[[{pi.(rho.Q)}]]=[[{(pi.rho).Q}]]$
\end{lemma}
\begin{proof}
  This result can be proved by a straightforward induction on the
  structure of $[[{Q}]]$.
\end{proof}

\setcounter{subsection}{4}
\subsection{Lemmas on constraint inference}
\label{sec:appendix:constraint-inference}

\begin{proof}[Proof of Lemma~\ref{lem:wanted:promote}]
  By induction on the syntax of $[[{C}]]$
  \begin{itemize}
  \item If $[[{C}]]=[[{Q'}]]$, then the result follows
    from Lemma~\ref{lem:q:scaling}
  \item If $[[{C}]]=[[{C1*C2}]]$, then we can prove the result like we
    proved the corresponding case in Lemma~\ref{lem:q:scaling},
    using Lemma~\ref{lem:inversion}.
  \item If $[[{C}]]=[[{C1&C2}]]$, then we the case where $[[{pi}]]=[[{1}]]$ is
    immediate, so we can assume without loss of generality that
    $[[{pi}]]=[[{omega}]]$, and, therefore, that $[[{pi.C}]] = [[pi.C1 *
    pi.C2]]$.
    By Lemma~\ref{lem:inversion}, we have that $[[Q|-C1]]$ and
    $[[Q|-C2]]$; hence, by induction, $[[omega.Q |- omega.C1]]$ and
    $[[omega.Q |- omega.C1]]$.
    Then, by definition of the entailment relation, we have $[[omega.Q
    * omega.Q |- omega.C1 * omega.C2]]$, which concludes,
    since $[[{omega.Q}]] = [[{omega.Q * omega.Q}]]$.
  \item If $[[{C}]]=[[{rho.(Q1 => C')}]]$, then by
    Lemma~\ref{lem:inversion}, there is a $[[{Q'}]]$ such that
    $[[{Q}]]=[[{pi.Q'}]]$ and $[[Q'*Q1 |- C']]$. Applying
    rule~\rref*{C-Impl} with $[[{pi.rho}]]$, we get
    $[[(pi.rho).Q' |- (pi.rho).(Q1 => C')]]$.

    In other words: $[[pi.Q |- pi.(rho.(Q=>C))]]$ as expected.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:wanted:demote}]
  By induction on the syntax of $[[{C}]]$
  \begin{itemize}
  \item If $[[{C}]]=[[{Q'}]]$, then the result follows from
    Lemma~\ref{lem:q:scaling-inversion}
  \item If $[[{C}]]=[[{C1*C2}]]$, then we can prove the result like we
    proved the corresponding case in
    Lemma~\ref{lem:q:scaling-inversion} using
    Lemma~\ref{lem:inversion}.
  \item If $[[{C}]]=[[{C1&C2}]]$, then we the case where $[[{pi}]]=[[{1}]]$ is
    immediate, so we can assume without loss of generality that
    $[[{pi}]]=[[{omega}]]$, and, therefore, that
    $[[{pi.C}]] = [[{pi.C1 * pi.C2}]]$. By Lemma~\ref{lem:inversion},
    there exist $[[{Q1}]]$ and $[[{Q2}]]$ such that $[[Q1|- omega.C1]]$,
    $[[Q2|- omega.C2]]$ and $[[{Q}]]=[[{Q1 * Q2}]]$. By induction
    hypothesis, we get $[[{Q1}]] = [[{omega.Q1'}]]$ and $[[{Q2}]] = [[{omega.Q2'}]]$
    such that $[[Q1' |- C1]]$ and $[[Q2' |- C2]]$. From which it
    follows that $[[omega.Q1'*omega.Q2' |- C1]]$ and
    $[[omega.Q1'*omega.Q2' |- C1]]$ (by
    Lemma~\ref{lem:wanteds:weakening}) and, finally,
    $[[{Q}]]=[[{omega.Q}]]$ (by Lemma~\ref{lem:wanteds:module-action})
    and $[[Q |- C1 & C2]]$.
  \item If $[[{C}]]=[[{rho.(Q1 => C')}]]$, then
    $[[{pi.C}]] = [[{(pi.rho). (Q1 => C')}]]$. The result follows
    immediately by Lemma~\ref{lem:inversion}.
  \end{itemize}
\end{proof}

\begin{proof}[Full proof of Lemma~\ref{lem:generation-soundness}]
  By induction on $[[G |-> e : t ~> C]]$
  \begin{description}
  \item[\rref*{E-Var}] We have
    \begin{itemize}
    \item $[[x :_1 forall as. Q =o u \in G]]$
    \item $[[G |-> x : u[ts/as] ~> Q[ts/as] ]]$
    \item $[[Q_g |- Q[ts/as] ]]$
    \end{itemize}
    Therefore, by rules~\rref*{E-Var} and~\rref*{E-Sub}, it follows
    immediately that $[[Q_g ; G |- x : u[ts/as] ]]$
  \item[\rref*{E-Abs}] We have
    \begin{itemize}
    \item $[[G |-> \x. e : t0 ->_pi t ~> C]]$
    \item $[[Q_g |- C]]$
    \item $[[G, x:_pi t0 |-> e : t ~> C]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_g; G, x:_pi t0 |- e : t]]$
    \end{itemize}
    From which follows that $[[Q_g; G |- \x. e : t0 ->_pi t]]$.
  \item[\rref*{E-Let}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x = e1 in e2 : t ~> pi.C1 * C2]]$
    \item $[[Q_g |- pi.C1 * C2]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q1}]]$ and $[[{Q_2}]]$ such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 |- C2]]$
    \item $[[{Q_g}]] = [[{pi.Q_1 * Q_2}]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_1 ; G1 |- e1 : t1]]$
    \item $[[Q_2; G2, x:_pi  t1 |- e1 : t1]]$
    \end{itemize}
    From which follows that $[[Q_g; pi.G1+G2 |- let_pi x = e1 in e2 :
    t]]$.
  \item[\rref*{E-LetSig}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x : forall as. Q =o t1 = e1 in e2 : t ~>
      C2 * pi.(Q => C1)]]$
    \item $[[Q_g |- C2 * pi.(Q => C1)]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \item $[[G2, x:_pi forall as. Q =o t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q1}]]$, $[[{Q2}]]$ such
    that
    \begin{itemize}
    \item $[[Q2 |- C2]]$
    \item $[[Q1*Q |- C]]$
    \item $[[{Q_g}]] = [[{pi.Q1*Q2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1*Q;G1 |- e1 : t1]]$
    \item $[[Q2; G2, x:_pi forall as. Q =o t1 |- e2 : t]]$
    \end{itemize}
    Hence $[[Q_g; pi.G1+G2 |- let_pi x : forall as. Q =o t1 = e1 in e2 : t]]$
  \item[\rref*{E-App}] \info{Most of the linearity problems are in the App
      rule. Unpack is also relevant.}
    We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q1}]]$, $[[{Q2}]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[{Q_g}]] = [[{Q1 * pi.Q2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[\rref*{E-Pack}] We have
    \begin{itemize}
    \item $[[omega.G |-> pack e : exists as. t o= Q ~> omega.C * Q[ts/as] ]]$
    \item $[[Q_g |- C * Q[us/as] ]]$
    \item $[[G |-> e : t[us/as] ~> C]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Q_2}]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C]]$
    \item $[[Q_2 |- Q[us/as] ]]$
    \item $[[{Q_g}]] = [[{omega.Q_1*Q_2}]]$
    \end{itemize}
    Bu induction hypothesis
    \begin{itemize}
    \item $[[Q_1 ; G |- e : t[us/as] ]]$
    \end{itemize}
    So we have $[[Q_1 * Q[us/as] ; omega.G |- pack e : exists as. t o=
    Q]]$. By rule~\rref*{E-Sub}, we conclude
    $[[Q_g ; omega.G |- pack e : exists as. t o= Q]]$.
  \item[\rref*{E-Unpack}] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * Q' => C2]]$
    \item $[[Q_g |- C1 * Q' => C2]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By Lemma~\ref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Q_2}]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 * Q' |- C2]]$
    \item $[[{Q_g}]] = [[{Q1 * Q2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \item[\rref*{E-Case}] We have
    \begin{itemize}
    \item $[[pi.G + D |-> case_pi e of {alts} : t ~> pi.C * && Ci]]$
    \item $[[Q_g |- pi.C * && Ci]]$
    \item $[[G |-> e : T ss ~> C]]$
    \item For each $i$, $[[D, <xi:_(pi.pii) ui[ss/as]> |-> ei : t ~> Ci]]$
    \end{itemize}
    By repeated uses of Lemma~\ref{lem:inversion}, there exist
    $[[{Q}]]$, $[[{Q'}]]$ such that
    \begin{itemize}
    \item $[[Q |- C]]$
    \item For each $i$, $[[Q' |- Ci]]$
    \item $[[{Q_g}]] = [[{pi.Q * Q'}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q; G |- e : T ss]]$
    \item For each $i$, $[[Q';D, <xi:_(pi.pii) ui[ss/as]> |- ei : t]]$
    \end{itemize}
    Therefore $[[Q_g ; pi.G + D |- case_pi e of {alts} : t]]$.
  \end{description}
\end{proof}

\begin{lemma}[Weakening of wanteds]\label{lem:wanteds:weakening}
  If $[[Q |- C]]$, then $[[omega.Q'*Q |- C]]$
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the derivation of
  $[[Q |- C]]$, using the corresponding property on the
  simple-constraint entailment relation from
  Definition~\ref{def:entailment-relation}, for the \rref*{C-Dom} case.
\end{proof}

\begin{lemma}\label{lem:wanteds:module-action}
  The following equality holds: $[[{pi.(rho.C)}]]=[[{(pi.rho).C}]]$.
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the structure of
  $[[{C}]]$, using Lemma~\ref{lem:simples:monoid-action} for the case
  $[[{C}]]=[[{Q}]]$.
\end{proof}
\end{document}

% Local Variables:
% ispell-dictionary: "british"
% End:


% LocalWords:  sequent typechecker idempotence polymorphism desugar
% LocalWords:  desugaring ghc OutsideIn quotiented gadt typeable
% LocalWords:  combinator sigils equalities wanteds intuitionistic
% LocalWords:  sequents implicational deallocate deallocating monadic
% LocalWords:  deallocated instantiations desugars
