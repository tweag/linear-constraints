% -*- latex -*-

%if style == newcode
module LinearConstraints where

\begin{code}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE ConstraintKinds #-}
{-# LANGUAGE RankNTypes #-}
{-# LANGUAGE TypeOperators #-}
{-# LANGUAGE KindSignatures #-}
{-# LANGUAGE MultiParamTypeClasses #-}

import Data.Kind (Constraint)
--import GHC.IO.Unsafe
import GHC.Base
\end{code}
%endif

\documentclass[acmsmall,usenames,dvipsnames,acmthm=false,review,anonymous]{acmart}

% \usepackage[backend=biber,citestyle=authoryear,style=alphabetic]{biblatex}


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
  }
\usepackage[plain]{fancyref}
\usepackage[capitalize]{cleveref}
\usepackage{mathpartir}
\usepackage{newunicodechar}
\input{newunicodedefs}

%%%%%%%%%%%%%%%%% ott %%%%%%%%%%%%%%%%%

\usepackage[supertabular,implicitLineBreakHack]{ottalt}
\inputott{ott.tex}

%%%%%%%%%%%%%%%%% /ott %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% Workaround %%%%%%%%%%%%%%%%%

% This should be handled by the acmclass article, there are a couple
% of issues about
% this. https://github.com/borisveytsman/acmart/issues/271,
% https://github.com/borisveytsman/acmart/issues/327 . Both have been
% merged long ago, and the version of acmart in the shell.nix is from
% 2020.

%% \usepackage{fontspec}
%% \setmainfont{Linux Libertine O}
%% \setsansfont{Linux Biolinum O}
%% \setmonofont{inconsolata}

%%%%%%%%%%%%%%%%% /Workaround %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%% lhs2tex %%%%%%%%%%%%%%%%%

\let\Bbbk\undefined    % see https://github.com/kosmikus/lhs2tex/issues/82
%include polycode.fmt
%if style == poly
%format a_j = "\Varid a \ensuremath{_j}"
%format ->. = "⊸"
%format => = "\FatArrow "
%format =>. = "\Lolly "
%format poly_arrow = "\mathop{\to_{\multiplicityfont{m}}}"
%format .<= = "\RLolly"
%format <== = "\RFatArrow"
%format omega = "\omega"
%format IOL = "IO_L"
%format . = "."
%format exists = "\exists"
%format forall = "\forall"
%format pack = "\kpack"
%format pack' = "\kpack!"
%format constraint (c) = "\constraintfont{" c "}"
%format multiplicity (p) = "\multiplicityfont{" p "}"
%
%format a1
%format a_n
%format an = a_n
%format ^^ = "\,"
%
%format ! = "!"
  %% ^^ this suppresses some space. I think lhs2TeX would otherwise use \mathop
%format spack = "!\kpack"
%endif

%let full = False

%%%%%%%%%%%%%%%%% /lhs2tex %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  % TOGGLE ME to turn off all the commentary:
  \InputIfFileExists{no-editing-marks}{
    \def\noeditingmarks{}
  }

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      \setlength{\marginparwidth}{1.2cm} % A size that matches the new PACMPL format
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\csongor}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=purple,#1]{#2}}
      \newcommandx{\rae}[2][1=]{\todo[linecolor=magenta,backgroundcolor=magenta!25,bordercolor=magenta,#1]{RAE: #2}}
      \newcommandx{\nw}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{NW: #2}}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2][1=]{}
      \newcommand{\info}[2][1=]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}

      \newcommand{\csongor}[2][1=]{}
      \newcommand{\jp}[2][1=]{}
      \newcommandx{\rae}[2][1=]{}
      \newcommandx{\nw}[2][1=]{}
  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Domain-specific macros %%%%%%%%%%%%%%%%%

  % Fonts and colours
  \newcommand{\constraintcolour}{\color{RoyalBlue}}
  \newcommand{\constraintfont}[1]{{\constraintcolour#1}}
  \newcommand{\multiplicitycolour}{\color{RoyalBlue}}
  \newcommand{\multiplicityfont}[1]{{\multiplicitycolour#1}}

  % Utilities
  \newcommand{\revop}[1]{\mathop{\reflectbox{\ensuremath{#1}}}}

  % Notations
  \newcommand{\cscheme}[1]{\mathcal{#1}}
  \newcommand{\rlolly}{\mathop{\revop{\multimap}}}
  \newcommand{\subst}[2]{[#1]#2}
  \newcommand{\sby}[2]{#1 ↦ #2}
  % \newcommand{\vdashi}{⊢_{\mathsf{i}}}
  \newcommand{\vdashi}{%
      \mathrel{%
          \vdash\hspace*{-4pt}%
          \raisebox{0.9pt}{\scalebox{.66}{\(\blacktriangleright\)}}%
      }%
  }
  \newcommand{\vdashs}{⊢_{\mathsf{s}}}
  \newcommand{\vdashsimp}{⊢_{\mathsf{s}}^{\mathsf{atom}}}
  \newcommand{\scale}{\constraintfont{\cdot}}
  %% Constraint notations
  \newcommand{\constraintop}[1]{\mathop{\constraintfont{#1}}}
  \newcommand{\aand}{\constraintop{\&}}
  \DeclareMathOperator*{\bigaand}{\vcenter{\hbox{\Large\&}}}
  \newcommand{\lollycirc}{\raisebox{-0.255ex}{\scalebox{1.4}{$\circ$}}}
  \newcommand{\Lolly}{\constraintop{=\kern-1.1ex \lollycirc}}
  \newcommand{\FatArrow}{\constraintop{\Rightarrow}}
  \newcommand{\RLolly}{\mathop{\constraintfont \circledless}}
  \newcommand{\RFatArrow}{\constraintop{\rtimes}}
  \newcommand{\qtensor}{\constraintop{\otimes}}
  %% Desugarer notations
  \newcommand{\dsterm}[2]{\llbracket #2 \rrbracket_{#1}}
  \newcommand{\dstype}[1]{\llbracket #1 \rrbracket}
  \newcommand{\dsevidence}[1]{\llbracket #1 \rrbracket^{\mathbf{ev}}}

  % language keywords
  \newcommand{\keyword}[1]{\mathbf{#1}}
  \newcommand{\klet}{\keyword{let}}
  \newcommand{\kcase}{\keyword{case}}
  \newcommand{\kwith}{\keyword{with}}
  \newcommand{\kpack}{\keyword{pack}}
  \newcommand{\kunpack}{\keyword{unpack}}
  \newcommand{\kin}{\keyword{in}}
  \newcommand{\kof}{\keyword{of}}

  % defining grammars
  \newcommand{\bnfeq}{\mathrel{\Coloneqq}}
  \newcommand{\bnfor}{\mathrel{\mid}}

  % theorems
  \theoremstyle{acmplain}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{corollary}[theorem]{Corollary}
  \newtheorem{property}[theorem]{Property}

  \theoremstyle{acmdefinition}
  \newtheorem{definition}[theorem]{Definition}

%%%%%%%%%%%%%%%%% /Domain-specific macros %%%%%%%%%%%%%%%%%

\acmConference[POPL'22]{POPL}{2022}{}
\acmYear{2022}
\copyrightyear{2022}
\setcopyright{none}

\citestyle{acmauthoryear}

\begin{document}

\title{Linear Constraints}

\author{Arnaud Spiwack}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{arnaud.spiwack@@tweag.io}
\author{Csongor Kiss}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{csongor.kiss14@@imperial.ac.uk}
\author{Jean-Philippe Bernardy}
\affiliation{
  \institution{University of Gothenburg}
  \city{Gothenburg}
  \country{Sweden}
}
\email{jean-philippe.bernardy@@gu.se}
\author{Nicolas Wu}
\affiliation{
  \institution{Imperial College London}
  \city{London}
  \country{United Kingdom}
}
\email{n.wu@@imperial.ac.uk}
\author{Richard A.~Eisenberg}
\affiliation{
  \institution{Tweag}
  \city{Paris}
  \country{France}
}
\email{rae@@richarde.dev}

\keywords{GHC, Haskell, laziness, linear logic, linear types,
  constraints, inference}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024</concept_id>
       <concept_desc>Software and its engineering~Language features</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011039</concept_id>
       <concept_desc>Software and its engineering~Formal language definitions</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Language features}
\ccsdesc[300]{Software and its engineering~Functional languages}
\ccsdesc[300]{Software and its engineering~Formal language definitions}

\begin{abstract}
  A linear parameter must be consumed exactly once in the body of its
  function. When declaring resources such as file handles and manually
  managed memory as linear arguments, a linear type system can verify
  these resources are neither duplicated nor forgotten.
  However, writing code with explicit linear arguments requires bureaucracy.
  %
  This paper presents \emph{linear constraints}, a front-end feature for
  linear typing that
  decreases the bureaucracy of working with linear types.
  %
  Linear constraints are implicit linear arguments that are
  filled in automatically by the compiler.
  %
  We present linear constraints as a qualified type system,
  together with an inference algorithm which extends
  \textsc{ghc}'s existing constraint solver algorithm. Soundness of
  linear constraints is ensured by the fact that they desugar into
  Linear Haskell.
\end{abstract}

\maketitle

\renewcommand{\shortauthors}{Bernardy, Eisenberg, Kiss, Spiwack, Wu}
\newcommand{\maybesmall}{\small}

\section{Introduction}
\label{sec:introduction}

Linear type systems have seen a renaissance in recent years in
various mainstream programming communities. Rust's ownership system guarantees
memory safety for systems programmers, Haskell's \textsc{ghc}~9.0 includes
support for linear types, and even
dependently typed programmers can now use linear types with Idris 2.
All of these systems are vastly different in ergonomics and
scope. Rust uses dedicated syntax and code generation
to support management of resources, while Linear Haskell is a
type system change without any core-language or other back-end support. Linear Haskell
is designed to be general purpose, but
using linear arguments to emulate Rust's ownership model is a painful exercise, requiring
% the compiler doesn't know how to help, requiring
the programmer to carefully thread resource tokens.

To get a sense of the power and the pain of using linear types, consider the
following function:
\begin{code}
read  ::  MArray a ->. Int -> (MArray a, Ur a)   -- defined in the library for |MArray|
free  ::  MArray a ->. ()                        -- defined in the library for |MArray|
read2AndDiscard :: MArray a ->. (Ur a, Ur a)
read2AndDiscard arr0 =
  let  (arr1, x)  = read arr0 0
       (arr2, y)  = read arr1 1
       ()         = free arr2
  in (x, y)
\end{code}
This function reads the first two elements of an array and returns them after
deallocating the array.
Linearity enables the array library to ensure that there is only one reference
to the array, and therefore it can be mutated in-place without violating
referential transparency. After the array has been freed, it is no longer
possible to read or write to it.
%
Notice that the |read| function consumes the array and returns a
fresh array, to be used in future operations. Operationally, the array remains
the same, but each operation assigns a new name to it, thus facilitating tracking
references statically. Finally, |free| consumes the array without returning a
new one, statically guaranteeing that it can no longer be used.
\footnote{We assume that all pattern bindings $\klet$-expressions are
\emph{strict}. This ensures that the array actually gets freed. We discuss this
point in \cref{sec:linear-types}.}
The values |x| and |y| read from the array are returned; their
types include elements wrapped by the |Ur|
(pronounced ``unrestricted'') type, allowing them to be used
arbitrarily many times. This works because |read2AndDiscard| takes a restricted-use array
containing unrestricted elements.
%
In a non-linear language, one would have to forgo referential transparency to
handle mutable operations either by using a monadic interface or allowing
arbitrary effects.
Compare the above function with what one would write in a non-linear, impure
language:
\begin{code}
read2AndDiscard :: MArray a -> (a, a)
read2AndDiscard arr =
  let   x    = read arr 0
        y    = read arr 1
        ()   = unsafeFree arr
  in (x, y)
\end{code}
This non-linear version does not guarantee that there is a unique reference to
the array, so freeing the array is a potentially unsafe operation.
However, it is simpler because there is less bureaucracy to manage: we are
clearly interacting with \emph{same} array throughout, and this version makes
that apparent.
We see here a clear tension between extra safety and clarity of code---one
we wish, as language designers, to avoid. %%  When
%% modelling a handle as a linear resource, the type system must know at all
%% times where it is being consumed, so the file handle is passed around
%% manually, resulting in extra noise.
%
% Reading the non-linear version, it is clear that it keeps a single reference to
% the array.
How can we get the compiler to see that the array is used safely
without explicit threading?

Rust introduces the \emph{borrow checker} for this very purpose.
Our approach is, in some ways, more lightweight: we show in this paper
how a natural generalisation of Haskell's type class
constraints does the trick. We call our new constraints \emph{linear constraints}.
Like class constraints,
linear constraints are propagated implicitly by the compiler.
Like linear arguments, they can safely be used to track resources such as arrays
or file handles. Thus, linear constraints are the combination of these two
concepts, which have been studied independently elsewhere
\citep{OutsideIn,LinearHaskell,hh-ll,resource-management-for-ll-proof-search}.

With our extension, we can write a new pure version of |read2AndDiscard| which does
not require explicit threading of the array:
\begin{code}
read   :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)  -- defined in the library for |UArray|
free   :: constraint ((Read n, Write n)) =>. UArray a n -> ()                        -- defined in the library for |UArray|
read2AndDiscard :: constraint ((Read n, Write n)) =>. UArray a n -> (Ur a, Ur a)
read2AndDiscard arr =
  let   pack x   = read arr 0
        pack y   = read arr 1
        pack ()  = free arr
  in (x, y)
\end{code}
The only changes from the impure version are that this version explicitly requires having
read and write access to the array,
and explicit |pack| annotations are used to indicate the binders that require
special treatment (\Fref{sec:implicit-existentials}
suggests how we can get rid of
the |pack|, too). Crucially, the resource representing the ownership of the
array is a linear constraint and is separate from the array itself, which no
longer needs to be passed around manually.
\rae{This is a very nice example. And, the way it's phrased around making it
clear that the array is the same array makes one not worry as much about the
pack.}

Our contributions are as follows:
\begin{itemize}
\item A system of qualified types that allows a constraint assumption
  to be given a multiplicity. Linear assumptions are used precisely
  once in the body of a definition
  (\Fref{sec:qualified-type-system}). This system supports examples that have motivated the design of
  several resource-aware systems, such as ownership à la Rust (\Fref{sec:memory-ownership}), or
  capabilities in the style of Mezzo~\cite{mezzo-permissions}
  or \textsc{ats}~\cite{AtsLinearViews}; accordingly, our system
  points towards a possible unification of these lines of research.
\item An inference algorithm that respects the multiplicity of
  assumptions. We prove that this algorithm is sound with respect to
  our type system~(\Fref{sec:type-inference}).
\item A core language (directly adapted
  from Linear Haskell~\cite{LinearHaskell}) that supports linear functions.
Expressions in our qualified
  type system desugar into this core language, and
 we
  prove that the output of our desugaring is well-typed~(\Fref{sec:desugaring}).
\end{itemize}
%
Our design is intended to work well with other features of Haskell and its
implementation within \textsc{ghc} and we have a prototype implementation.

\section{Background: Linear Haskell}
\label{sec:linear-types}

This section, mostly cribbed from \citet[Section 2.1]{LinearHaskell},
describes our baseline approach, as released in GHC~9.0.
%
Linear Haskell adds a new type of functions,
dubbed \emph{linear functions}, and written |a ⊸ b|.\footnote{The linear function
  type and its notation come from linear
  logic~\cite{girard-linear-logic}, to which the phrase \emph{linear
    types} refers. All the various design of linear typing in the
  literature amount to adding such a linear function type, but details
  can vary wildly. See~\citet[Section 6]{LinearHaskell} for an analysis
  of alternative approaches.} A linear function consumes its
  argument exactly once. More precisely, Linear Haskell lays it out thusly:

\begin{quote}
\emph{Meaning of the linear arrow}:
|f :: a ⊸ b| guarantees that if |(f u)| is consumed exactly once,
then the argument |u| is consumed exactly once.
\end{quote}
To make sense of this statement we need to know what ``consumed exactly once'' means.
Our definition is based on the type of the value concerned:
\begin{definition}[Consume exactly once]~ \label{def:consume}
\begin{itemize}
\item To consume a value of atomic base type (like |Int|) exactly once, just evaluate it.
\item To consume a function exactly once, apply it to one argument, and then consume its result exactly once.
\item To consume a pair exactly once, pattern-match on it, and then consume each component exactly once.
\item In general, to consume a value of an algebraic datatype exactly once, pattern-match on it,
  and then consume all its linear components exactly once.
\end{itemize}
\end{definition}
%
Note that a linear arrow specifies \emph{how the function uses its argument}. It does not
restrict \emph{the arguments to which the function can be applied}.
In particular, a linear function cannot assume that it is given the
unique pointer to its argument.  For example, if |f :: a ⊸ b|, then
the following is fine:
\begin{code}
g :: a -> (b, b)
g x = (f x, f x)
\end{code}
The type of |g| makes no guarantees about how it uses |x|.
In particular, |g| can pass |x| to |f|.

The |read| function in \cref{sec:introduction} consumes the array it
operate on. Therefore, the same array can no longer be used in further
operations: doing so would result in a type error.
To resolve this, a new name for the same array is produced by each operation.

From the perspective of the programmer, this is unwanted boilerplate.
The approach with linear constraints is to let the array behave non-linearly, and let its
capabilities (i.e., having read or write access) be linear constraints. Once
these capabilities are consumed, the array can no longer be read from or written
to without triggering a compile time error.

In \cref{sec:introduction} and in the rest of the paper, we use pattern matches
in $\klet$ bindings. By default in Haskell, patterns in $\klet$s are lazy, which means that
|let  (a, b) = p in ()|
will not actually evaluate the pair. To force evaluation, a strictness annotation can be added:
|let  ^^ !(a, b) = p in ()|.
Pattern matching in linear $\klet$ bindings must always be strict, so writing
the lazy version would be rejected by the compiler. To simplify the
presentation, we assume that all patterns in $\klet$ bindings are strict.

\section{Working With Linear Constraints}
\label{sec:what-it-looks-like}

\begin{figure}%
  \maybesmall
  \centering
  \begin{subfigure}{.3\linewidth}%
    \noindent%
\begin{code}
newMArray  :: (MArray ->. Ur r) ->. Ur r
write :: MArray a ->. Int -> a -> MArray a
read :: MArray a ->. Int -> (MArray a, Ur a)
free :: MArray a ->. ()
\end{code}
\caption{Linear Types}
\label{fig:linear-interface}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.55\linewidth}
\begin{code}
type constraint(RW n) = constraint((Read n, Write n))
newUArray    :: (forall n. ^^ constraint (RW n) =>. UArray a n -> Ur r) ⊸ Ur r
write :: constraint (RW n) =>. UArray a n -> Int -> a -> () .<= constraint (RW n)
read :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)
free :: constraint (RW n) =>. UArray a n -> ()
\end{code}
\caption{Linear Constraints}
\label{fig:constraints-interface}
  \end{subfigure}
\caption{Interfaces for file manipulation}
\end{figure}

Consider the Haskell function |show|:
\begin{code}
show :: constraint (Show a) => a -> String
\end{code}
In addition to the function arrow |->|, common to all functional
programming languages, the type of this function features a constraint arrow |=>|.
Everything to the
left of a constraint arrow is called a \emph{constraint}, and will be
highlighted in \constraintfont{blue} throughout the paper. Here
|constraint (Show a)| is a
class constraint.

Constraints are handled implicitly by
the typechecker. That is, if we want to |show| the integer |n :: Int| we would write |show
n|, and the typechecker is responsible for proving that |constraint (Show Int)| holds, without
intervention from the programmer.

For our |read2AndDiscard| example, the |constraint((Read n, Write n))| (abbreviated as |constraint(RW n)|)
constraint represents read and write
access to the array tagged with the type variable |n|. (The full \textsc{api} under consideration appears
in \cref{fig:constraints-interface}.)
That is, the constraint |constraint(RW n)|
is provable if and only if the array tagged with |n| is readable and writable.
This constraint is linear: it must be consumed (that is, used as an assumption
in a function call) exactly once.
In order to manage linearity implicitly, this paper introduces a
linear constraint arrow (|=>.|), much like Linear Haskell introduces a linear
function arrow (|->.|). Constraints to the left of a linear constraint
arrow are \emph{linear constraints}.
Using the linear constraint |constraint(RW n)|, we can give
the following type to |free|:

\begin{code}
free :: constraint (RW n) =>. UArray a n -> ()
\end{code}

There are a few things to notice:
\begin{itemize}
\item We have introduced a new type variable |n|. In contrast,
  the version in \Cref{fig:linear-interface} without linear
  constraints  has type |free :: MArray a ->. ()|.
  The type variable |n| is a type-level tag used to identify the array.
  \rae{I'm trying to avoid ``location'' because I initially confused ``location'' with
  ``index'', and our reviewers might, too.}
  Ideally, the linear constraint would refer directly to the
  array value, and have the dependent type |free :: (n :: Array a) -> constraint (RW n) =>. ()|.
  While giving a compile-time name to a function argument is common in
  dependently typed languages such as \textsc{ats}~\cite{ats-lang} or Idris, our
  approach, on the other hand, shows how we can still link a run-time value and a
  compile-time tag without needing any dependent types.
\item The run-time variable representing the array can now be used multiple
  times. Instead of restricting the use of this variable,
  the linear constraint |constraint(RW n)| now controls access to the
  array.
\item If we have a single, linear, |constraint (RW n)|
  available, then after |free| there will not be any |constraint (RW n)|
  left to use, thus preventing the array from being used after freeing.
  This is precisely what we were trying to achieve.
\end{itemize}

The above deals with freeing an array and ensuring that it cannot be used afterwards.
However, we still need to explain how a constraint |constraint (RW n)| can come
into scope.
The type of |newUArray| is:
\begin{code}
newUArray    :: (forall n. ^^ constraint (RW n) =>. UArray a n -> Ur r) ->. Ur r
\end{code}
This higher-rank function can be thought of as a computation that allocates an
array in a \emph{scope}. The scope is given an array with read and write
capability. The scope must return an unrestricted value to ensure that
the linear constraint cannot be embedded into the return value. The construction here
ensures that, within the scope, we can be sure both that one
unique pointer refers to the array at all times, and that the array is freed at the
end, given that the only way to remove the |constraint(RW n)| constraint is to use |free|.
\rae{I'm a little worred about "one unique pointer refers to the array at all times". When
we call |read|, for example, the |read| function has a local copy of the pointer to the
array. There will really be two pointers to the array on the stack. I mean, morally, the
one-unique-pointer property is true, but I don't know how best to state it so that the
statement is \emph{actually} true.}

We must also ensure that |read| can both promise to operate only on a readable array
and that the array remains readable afterwards. That is, |read| must both consume
a linear constraint |constraint(Read n)| and also produce a fresh linear constraint
|constraint(Read n)|, as we see in \cref{fig:constraints-interface}, and repeated
here:
\begin{code}
read :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)
\end{code}
%% The fact that existential quantification generates new type-level names
%% is folklore. It's used crucially in the interface of the
%% |ST| monad~\cite{st-monad} and in type-class
%% reflection~\cite{type-class-reflection} (in both of these cases, existential
%% quantification is encoded as rank-2 universal quantification).
%% We
%% shall use it in exactly this way: |openFile| uses an existential
%% quantifier to generate the type-level name of the file
%% handle. Existentially quantified types are paired with a constraint |Q|
%% which we understand as being returned by functions.
This type has a new symbol, |.<=|, which allows us to pack a produced constraint
with a returned value. We will see \rae{where?} that these produced constraints
will also sometimes need to come with fresh type variables. Combining these
ideas, we
introduce\footnote{There is a variety of ways existential types can
  be worked into a language. The existentials we use here might best be understood
  as a generalisation of those presented by \citet[Chapter
  24]{tapl}.
  However, a recent publication by \citet{existentials} works out an
  approach that will make linear constraints easier to use, as we
  discuss in \Fref{sec:implicit-existentials}.}
a type construction |exists a1 ... an. t .<= constraint(Q)|,
where |constraint(Q)| is a linear constraint (with the |a1 ... an| in scope) that is
paired with the type |t|.\footnote{We freely
omit the |exists a1 ... an.| or |.<= constraint (Q)| parts when they are
empty.}

Today's Haskell does not have an existential quantifier. However,
existentially quantified types can
be encoded as datatypes. For instance, |exists h. Ur (Handle h) .<= constraint (Open h)| can
be implemented as
\begin{code}
data PackHandle where
  Pack :: forall h. constraint (Open h) =>. Handle h -> PackHandle
\end{code}
In our implementation (\Fref{sec:implementation}), packed linear constraints
piggy-back on \textsc{ghc}'s standard \textsc{gadt} syntax. Correspondingly, existential types are
introduced by a data constructor, which we write as |pack|.

When pattern-matching on a |pack| constructor, all existentially quantified
type variables are brought into scope and all the packed constraints are
assumed. We have now seen all the ingredients needed to write
the |read2AndDiscard| example as in \cref{sec:introduction}.

\subsection{Minimal Examples}
\label{sec:examples}

To get a sense of how the features we introduce should behave, we now
look at some simple examples. Using constraints to represent
limited resources allows the typechecker to reject certain classes
of ill-behaved programs. Accordingly, the following examples show the
different reasons a program might be rejected.

In what follows, we will be using a constraint |constraint (C)| that is consumed by the |useC|
function.
\begin{code}
useC :: constraint (C) =>. Int
\end{code}
The type of |useC| indicates that it consumes the linear resource |C| exactly once.

\subsubsection{Dithering}

We reject this program:
\begin{code}
dithering :: constraint (C) =>. Bool -> Int
dithering x = if x then useC else 10
\end{code}
The problem with |dithering| is that it does not unconditionally consume |constraint(C)|:
 the branch where |x == True| uses the resource |C|,
whereas the other branch does not.

\subsubsection{Neglecting}

Now consider the type of the linear version of |const|:
\begin{spec}
const :: a ->. b -> a
\end{spec}
This function uses its first argument linearly, and ignores the second. Thus,
the the second arrow is unrestricted.
%
One way to improperly use the linear |const| is by neglecting a linear variable:
\begin{code}
neglecting :: constraint (C) =>. Int
neglecting = const 10 useC
\end{code}
The problem with |neglecting| is that, although |useC| is mentioned in this program,
it is never consumed: |const| does not use its second argument.
The constraint |constraint(C)| is not consumed exactly once, and
thus this program is rejected.
%
The rule is that a linear constraint can only be consumed (linearly)
in a linear context. For example,
\begin{code}
notNeglecting :: constraint (C) =>. Int
notNeglecting = const useC 10
\end{code}
is accepted, because the |constraint (C)| constraint is passed on to |useC| which itself
appears as an argument to a linear function (whose result is itself consumed linearly).

\subsubsection{Overusing}
\label{sec:overusing}

Finally, the following program is rejected because it uses |C| twice:
\begin{code}
overusing :: constraint (C) =>. (Int, Int)
overusing = (useC, useC)
\end{code}
%
% However, the
% following version is accepted:
% \begin{code}
% notOverusing :: constraint ((C, C)) =>. (Int, Int)
% notOverusing = (useC, useC)
% \end{code}
% We see here that it is possible to have multiple copies of a given
% constraint.

% Because the consumption of constraints is implicit, when there are
% several possible ways to consume a constraint, the particular order
% chosen will depend on the details of the constraint resolution
% algorithm. We do not want the semantics of programs to depend on this
% order, which would not be at home in a declarative specification of
% the type system. Specifically, we require that the choice of
% given constraint (when there are multiple) cannot influence the
% behavior of the running program. In the domain of class constraints,
% this property is called \emph{coherence}: that only one instance of
% a specific type is in scope. Haskell already has mechanisms to ensure
% coherence~\cite{coherence}, though expert users have discovered ways
% to introduce incoherence\footnote{Kmett's \textsf{reflection} library,
% \url{https://hackage.haskell.org/package/reflection}, both uses incoherence
% to its advantage and also demonstrates techniques to keep its interface
% safe.}. As usual, any introduction of incoherence in the design of
% an \textsc{api} must be carefully considered for possible negative effects;
% the addition of linear constraints does not change this.

% \subsection{Linear I/O}
% \label{sec:linear-io}

% The file-handling example discussed in sections~\ref{sec:linear-types}
% and~\ref{sec:what-it-looks-like} uses a linear version of the |IO| monad, |IOL|.
% Compared to the traditional |IO| monad, the
% type of the monadic operations |>>=| (aka \emph{bind}) and |return| are changed to
% use linear arrows.
% %
% \begin{code}
% (>>=) :: IOL a ->. (a ->. IOL b) ->. IOL b
% return :: a ->. IOL a
% \end{code}
% %
% Bind must be linear because, as explained in the previous section, a linear
% constraint can be consumed in only a linear context. Consider
% the
% following program:
% \begin{code}
% readTwo ::  constraint (Open h) =>. Handle h -> IOL (Ur (String, String) .<= constraint (Open h))
% readTwo h =  readLine h >>= \case pack' xs ->
%              readLine h >>= \case pack' ys ->
%              return (pack' (xs, ys))
% \end{code}
% If bind were not linear, the first occurrence of |readLine h| would
% not be able to consume the |constraint (Open h)| constraint
% linearly.

\section{Application: memory ownership}
\label{sec:memory-ownership}\csongor{Now this picks up from the Introduction} %
Let us now turn to a more substantial example: manual memory
management.  In Haskell, memory deallocation is normally the
responsibility of a garbage collector. However, garbage collection is
not always desirable, either due to its (unpredictable) runtime costs, or because
pointers exist between separately-managed memory spaces
(for example when calling foreign functions~\cite{linear-inline-java}).
In either case, one must then
resort to explicit memory allocation and deallocation. This task is
error prone: one can easily forget a deallocation (causing a memory
leak) or deallocate several times (corrupting data).

\subsection{Capability constraints}
\label{sec:atomic-references}

With linear constraints, it is possible to provide a \emph{pure} interface for
manual memory management, which
enforces correct deallocation of memory.  Our approach, in the style of Rust, is
to represent \emph{ownership} of a memory location, and more specifically,
whether the reference is mutable or read-only.
We use the linear constraints |constraint(Read n)| and |constraint(Write n)|,
guarding read access and write access to a reference respectively.
Because of linearity, these constraints
must be consumed, so the \textsc{api} can guarantee that the memory
is deallocated correctly.
%
In |constraint(Read n)|, |n| is a type variable (of a special kind |Location|)
which represents a memory location. Locations mediate
the relationship between references and ownership constraints.
%

\begin{minipage}{0.5\linewidth}
> class constraint (Read (n :: Location))
\end{minipage}
\begin{minipage}{0.5\linewidth}
> class constraint (Write (n :: Location))
\end{minipage}
% \begin{minipage}{0.3\linewidth}
% > class constraint (Own (n :: Location))
% \end{minipage}
\\
To ensure referential transparency,
writes can be done only when we are sure that no other part of the program has
read access to the reference.
% Rust disallows mutable aliasing for the same reason: ensuring that writes cannot be observed through other references is what allows treating mutable structures as ``pure''.
% jp: commented this tangent because it obscures the logical connection. It could perhaps go in a parenthesis
Therefore, writing also requires the read capability. Thus we
systematically use the |constraint(RW n)|, pairing both the read and write
capabilities.

With these components in place, we can provide an \textsc{api} for mutable
references.

> data AtomRef (a :: Type) (n :: Location)

The type |AtomRef| is the type of references to values of a type |a| at
location |n|. Allocation of a reference can be done using the
following function. As with |newUArray|, the return value must be unrestricted.

> newRef :: (forall n. ^^ constraint (RW n) =>. AtomRef a n -> Ur b) ⊸ Ur b

To read a reference, a simple |constraint(Read)| constraint is demanded, and
immediately returned. Writing is handled similarly.

> readRef :: constraint (Read n) =>. AtomRef a n -> Ur a .<= constraint (Read n)
> writeRef :: constraint (RW n) =>. AtomRef a n -> a -> () .<= constraint (RW n)

Note that the above primitives do not need to explicitly declare
effects in terms of a monad or another higher-order effect-tracking
device: because the |constraint(RW n)| constraint is linear, passing it suffices
to ensure proper sequencing of effects concerning location |n|.

% This is ensured by the combination of the language and library
% behaviour. For example, here is how to write two values (|a| and |b|) to the same reference |x|:

% > case writeRef x a of
% >   pack _ -> case writeRef x b of
% >     pack _ -> ...

% The language semantics forces the programmer to do case analysis to
% access the returned |Write| constraints, and |writeRef| must be strict
% in the |Write| constraint that it consumes.

Since there is a unique |constraint(RW n)| constraint per reference, we
can also use it to represent ownership of the reference: access to |RW
n| represents responsibility (and obligation) to deallocate |n|:

> freeRef :: constraint (RW n) =>. AtomRef a n -> ()

Instead of deallocating the reference, one could transfer ownership
of the memory location to the garbage collector. This operation is
sometimes called ``freezing'':

> freezeRef :: constraint (RW n) =>. AtomRef a n -> Ur a

\subsection{Arrays}
\label{sec:arrays}

The above toolkit handles references to base types just fine.  But
what about storing references in objects managed by the ownership
system? In \cref{sec:introduction}, we presented an interface for mutable
arrays whose contents are themselves immutable. Our approach
scales beyond that use case, supporting arrays of
references, including arrays of (mutable) arrays.

% JP: the discussion below sounds quite petty. Not many (zero
% non-conflicting?)  people will be aware of the issue with linear
% arrays. So it would need a lot more space to be properly
% justified. Besides, the issue of storing references in structures
% should be motivation enough in general.

% A motivating example of plain Linear Haskell is a pure interface to
% mutable arrays~\cite[Section 2.2]{LinearHaskell}. There we have two
% types: first |MArray a|, used linearly, for mutable arrays; and
% second |Array a|, used unrestricted, for immutable arrays. Mutable
% arrays can be frozen using |freeze :: MArray a ->. Ur (Array
% a)|. Note that this version of |freeze| can not support mutable
% arrays of mutable arrays.  The core of the issue is that mutable
% arrays and immutable arrays have different types.
% The ownership \textsc{api}, on the other hand, is readily extended to
% nested mutable arrays.
% Crucially, and in contrast to the plain linear types \textsc{api} the
% type |PArray| is both the type of mutable arrays and immutable arrays,
% |freezePArray| only changes the permissions. And since permissions are
% linear constraints, this is all managed automatically by the compiler.

> data PArray (a :: Location -> Type) (n :: Location)
> newPArray    :: (forall n. ^^ constraint (RW n) =>. PArray a n -> Ur b) ⊸ Ur b

For this purpose we introduce the type |PArray a n|, where the kind of
|a| is |Location -> Type|: this way we can easily enforce that each
reference in the array refers to the same location |n|. Both types
|AtomRef a| and |PArray a| have kind |Location -> Type|, and therefore
one can allocate, and manipulate arrays of arrays with this
\textsc{api}. For example, an array of integers would have type
|PArray (AtomRef Int) n|, and indeed, the |UArray| type from
\cref{sec:introduction} is a synonym for an array of atomic references. \rae{Really?
This surprises me, because we end up discarding the references. Key question: are
the elements in the array restricted or not?}
An array of arrays of integers would have type |PArray (PArray Int) n|. \rae{That type
is ill-kinded. Did you mean |PArray (PArray (AtomRef Int)) n|?} Thus,
the framework handles nested mutable structures without any additional
difficulty.

When writing a reference (be it an array or an |AtomRef|) in an array,
ownership of the reference is transferred to the array. Given a
generic deallocation mechanism for references (|Freeable|), we could
write:

> write  :: constraint ((RW n, RW p)) =>. Freeable p => PArray a n -> Int -> a p -> () .<= constraint (RW n)

The |write| function can be written in terms of another, more primitive and precise,
function which which swaps a reference with an element of the array.

> replace  :: constraint ((RW n, RW p)) =>. PArray a n -> Int -> a p -> () .<= constraint ((RW n, RW p))

\rae{We need to say more here. What does |replace| do? What is |Freeable|? I'm pretty lost here.}

% JP: I propose to cut down the following, since it appears unused in the rest of the paper.
% (The "quicksort" example is a lot more convincing as an example)

% For the special case where the array stores atomic references (|UArray|) which we know how
% to deallocate, the |write| function can be implemented using |replace|:


% > write :: constraint (RW n) =>. UArray a n -> Int -> a -> () .<= constraint (RW n)
% > write arr i x =
% >   let   pack' ref = newRefBeside arr
% >         pack () = writeRef ref x
% >         pack () = replace arr i ref
% >   in freeRef ref

% Because having access to a |constraint(RW n)| is proof that we are inside a scope
% with a unique resource, we can also allocate a reference to the same scope as |n|:

% > newRefBeside :: constraint (RW n) =>. a n -> exists m. Ur (AtomRef m) .<= constraint ((RW n, RW m))

% \csongor{I think this is now the first time we talk about existentials? Need to be careful not to forget introducting them}




\subsubsection{Borrowing}

The |lendMut arr i k| primitive lends access to the reference at index |i| in
|arr|, to a scope function |k| (in Rust terminology, the scope
\emph{borrows} an element of the array). Note that the scope must return the
read-write capability, so the ownership transfer is indeed temporary, and the
type system guarantees that the borrowed reference can not be shared, let alone deallocated.
%
% Here, the return type of the scope, |r|,
% is not in |Ur|: since the scope must return the |constraint(RW p)| constraint, it is
% not possible to leak it out by packing it into |r|, so it's not necessary to
% wrap the result in |Ur|.
Indeed, with this \textsc{api}, |constraint(RW n)| and |constraint(RW p)| are
never simultaneously available. \rae{Even more confused. If |RW n| and |RW p| are never simultaneously available, then how can we ever call |replace|?}

> lendMut  :: constraint (RW n) =>. PArray a n -> Int -> (forall p. ^^ constraint (RW p) =>. a p -> r .<= constraint (RW p)) ⊸ r .<= constraint (RW n)

\rae{The type of |lendMut| is different than the other scoped functions: its continuation's return value is paried with |RW p|.
Other examples have an unrestricted return value. Why the difference? What changes here? This version seems more powerful.
Should it be used with e.g. |newUArray|?}
Because the elements of an array can be mutable structures (such as
other arrays), reading can only be done safely if we can ensure that
no one else has access to the array \rae{which one? the inner one or the outer one?}
in the meantime. Therefore,
gaining simple read access to an element needs to be done using a
scoped \textsc{api} as well:

> lend  ::  constraint (Read n) =>. PArray a n -> Int ->  (forall p. ^^ constraint (Read p) =>. a p -> r .<= constraint (Read p)) ⊸ r .<= constraint (Read n)

For the special case of |UArray|s, a more traditional reading
operation can be implemented, by lending the reference to |readRef|
which creates an unrestricted \emph{copy} of the value. This copy is
under control of the garbage collector, and can escape the scope of
the borrowing freely.

> read :: constraint (Read n) =>. UArray a n -> Int -> Ur a .<= constraint (Read n)
> read arr i = lend arr i readRef


% \subsubsection{Freezing}

% Finally, we can freeze arrays, using the
% following primitive:

% > freezePArray :: constraint (RW n) =>. PArray a n -> () .<= constraint (omega ⋅ (Read n))

% Where |constraint(omega ⋅ (Read n))| means that the returned constraint is not linear. That is, after |freezePArray n|, we have unrestricted read access to |n| (and
% any element of |n|), as expected. We describe this syntax in more
% details in the next
% section, where, similarly, we shall treat |constraint(Read n) =>| as
% an abbreviation for |constraint(omega ⋅ (Read n)) =>.|.

% With an unrestricted |constraint(Read n)| capability, we can read from the array
% more directly with the following primitive

% > readP :: constraint (Read n) => PArray a n -> Int -> a n

\jp{That's inconsistent with the above discussion. If we have no
  generic way to deallocate a reference, we won't be able to transfer
  control to the garbage collector for EVERY PArray.  This section
  should either be restricted to UArray, or deleted (as I have done tentatively: you won't see the offending text in the pdf).}

\subsubsection{Slices}

Finally, it is also possible to give a safe interface to array
\emph{slices}. A slice represents a part of an array, and allows
splitting the ownership of the array into multiple disjoint parts to
be shared between different consumers.  Thanks to the
ownership system, creating a slice does not require a physical copy to
be made.

Splitting consumes all capabilities of an array and returns two new
arrays that represent the contiguous blocks of memory before and
starting at a given index.

> split :: constraint (RW n) =>. PArray a n -> Int -> exists l r. Ur (PArray a l, PArray a r) .<= constraint ((RW l, RW r, omega ⋅ (Slices n l r)))

\rae{The |omega| there is jarring. Maybe |UrC| for ``unrestricted constraint''?}
In addition to the array capabilities, the output constraints also include
|constraint (omega ⋅ (Slices n l r))|, which witnesses the fact that locations
|l| and |r| indeed make up location |n|, so that they can be joined back
together:

> join :: constraint ((RW r, RW l, omega ⋅ (Slices n l r))) =>. PArray a l -> PArray a r -> Ur (PArray a n) .<= constraint (RW n)

\rae{More |omega| here. Why not just put the |Slices| constraint to the left of an |=>|? Actually, this
all suggests that we may want an extra widget in our existential: packed unrestricted constraints. But if
this is the only place in the paper we do this, maybe just make |split| higher-ranky. That might simplify
this all, especially because the reader has already paid the cost of understanding the higher-ranky approach.}
Note that the constraint |constraint (omega ⋅ (Slices n l r))| is
unrestricted, which means that it is not \emph{necessary} to join the
two arrays before deallocating them: the can be deallocated separately.

With these building blocks, we can now implement various utility
functions on arrays, such as swapping two elements of an array.  It is
not so simple to implement\footnote{Indeed, Rust's implementation uses
  an \emph{unsafe} block.}, because we need two elements of an array
simultaneously, but only one element can be borrowed at a time. To
solve this problem, we split the array into two slices such that the two
indices fall in two different slices. Then simply borrow the element
from the first slice, and replace it with the element in the second
slice. Finally, join the two slices back together.

\begin{code}
swap :: RW n =>. PArray a n -> Int -> Int -> () .<= RW n
swap arr i j   | i == j   = arr
               | i > j    = swap arr j i
               | i < j    =  let   (l, r)   = split arr (i + 1)
                                   pack ()  = lendMut l i (replace r (j - (i + 1)))
                             in join l r
\end{code}
\rae{I don't see how this works at compile-time or at run-time. Compile-time problem:
This function says it returns `()` but it actually returns `Ur (PArray a n)`.
Run-time problem: I don't see how index |i| is written to. Maybe a description
of |replace| would fix this problem.}

\subsubsection{In-place quicksort}

As an example of using the machinery defined above, we implement an in-place, pure
quicksort algorithm.

\begin{minipage}{0.5\linewidth}
  \begin{code}
    quicksort :: constraint (RW n) =>. UArray Int n -> () .<= constraint (RW n)
    quicksort arr =
      let pack len = length arr
      in   if len <= 1 then pack ()
           else   let  pack pivotIdx  = partition arr
                       pack (l, r)    = split arr pivotIdx
                       pack ()        = quicksort l
                       pack ()        = quicksort r
                  in join l r
  \end{code}
\end{minipage}
\jp{I propose that we consider the length of arrays statically
  fixed. This way |length| needs no linear constraint, and the
  code can be shorter.}
\rae{I don't follow J-P's idea, because the length changes in
recursive calls.}
\rae{We should give at least the type of |length|.}
\rae{The return value is wrong: we need to produce a |()|.}
\begin{minipage}{0.5\linewidth}
  \begin{code}
partition :: constraint (RW n) =>. UArray Int n -> Int .<= constraint (RW n)
partition arr
  = let   pack len = length arr
          pack pivot = read arr (len - 1)
    in foldl (\(pack i) j ->
            let  pack a_j = read arr j
            in   if a_j <= pivot then pack (i + 1)
                 else  let pack _ = swap arr i j
                       in pack i
          ) (pack 0) [0..len-1]
        \end{code}
\end{minipage}

\rae{I'm having a hard time with that |foldl|. Firstly, I have a hard time
believing that type inference will sort that out, with the pattern-match on |pack|.
Secondly, can't we just write this out using explicit recursion? That
seems so much easier to understand than |foldl| over a list of numbers.}
\rae{Have we actually tried this in the implementation? Does it work? If not,
have we tried this without linear constraints but with ordinary linear types?}

\section{A qualified type system for linear constraints}
\label{sec:qualified-type-system}

Having laid out situations which would benefit from linear constraints, we
now present our design for a qualified type system~\cite{QualifiedTypes} that supports
them. Our design is mindful of our goal of integration with Haskell and \textsc{ghc}
and is thus based on the work of \citet{OutsideIn}, which undergirds \textsc{ghc}'s
current approach to type inference.

\subsection{Multiplicities}
\label{sec:multiplicities}

Like in Linear Haskell % ~\cite{LinearHaskell}  \nw{I don't think we need to cite this again at this stage}
we make use of a
system of \emph{multiplicities}, which describe how
many times a function consumes its input. As multiplicities are
central to our constraint calculus, we will colour them in
\multiplicityfont{blue} like constraints. For our purposes, we need only the
simplest system of multiplicity: that composed of only $[[{1}]]$ (representing
linear functions) and $[[{omega}]]$ (representing
regular Haskell functions).
$$
\begin{array}{lcll}
  [[{pi}]], [[{rho}]] & \bnfeq & [[{1}]] \bnfor [[{omega}]] & \text{Multiplicities}
\end{array}
$$
The idea of multiplicity goes back at least
to \citet{linearity-and-pi-calculus}. The power of multiplicities is that they can encode the
structural rules of linear logic with only the semiring
operations: addition and multiplication. Here and in the rest of the paper we adopt the convention that equations defining a function by pattern matching are marked with a $\left\{\right.$ to their left.

\smallskip
\begin{minipage}[c]{0.4\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{pi + rho}]] & = & [[{omega}]]
  \end{array}
\right.
$$
\end{minipage}
\begin{minipage}[c]{0.4\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{1 . pi}]] & = & [[{pi}]] \\
    [[{omega . pi}]] & = & [[{omega}]]
  \end{array}
\right.
$$
\end{minipage}
\smallskip

Even though linear Haskell additionally supports
multiplicity polymorphism,
we do not support multiplicity polymorphism on constraint
arguments.
Multiplicity polymorphism of regular function arguments is used to avoid
duplicating the definition of higher-order functions. The prototypical example
is |map :: (a poly_arrow b) -> [a] poly_arrow [b]|,
where |poly_arrow| is the notation for a function arrow of multiplicity |multiplicity(m)|.
 First-order functions, on the other hand, do not need multiplicity
polymorphism, because
linear functions can be $\eta$-expanded into unrestricted function as
explained in \cref{sec:linear-types}. Higher-order functions
whose arguments are themselves constrained functions are rare,
so we do not yet see the need to extend multiplicity polymorphism
to apply to constraints.
Futhermore, it is not clear how to extend the constraint solver of
\cref{sec:constraint-solver} to support multiplicity-polymorphic
constraints.

\subsection{Simple Constraints and Entailment}
\label{sec:constraint-domain}

Let us now turn to constraints themselves. We call constraints such as
|constraint(Read n)| or |constraint(Write n)| \emph{atomic
  constraints}. The exact nature of atomic constraints is left
unspecified: the set of atomic constraints is a parameter of our
qualified type system.

\begin{definition}[Atomic constraints]
  The qualified type system is parameterised by a set, whose elements
  are called \emph{atomic constraints}. We use the variable $[[{q}]]$
  to denote atomic constraints.
\end{definition}

Atomic constraints are assembled into \emph{simple constraints}
$[[{Q}]]$, which play the hybrid role of constraint contexts and
(linear) logic formulae.
The following operations work with simple constraints:
\begin{description}
\item[Scaled atomic constraints] $[[{pi.q}]]$ is a simple constraint,
  where $[[{pi}]]$ specifies whether $[[{q}]]$ is to be used linearly
  or not.
\item[Conjunction] Two simple constraints can be paired up
  $[[{Q1 * Q2}]]$. Semantically, this corresponds to the multiplicative
  conjunction of linear logic. Tensor products represent pairs of
  constraints such as |constraint((Read n, Write n))| from Haskell.
\item[Empty conjunction] Finally we need a neutral element
  $[[{Empty}]]$ to the tensor product. The empty conjunction is used
  to represent functions which don't require any constraints.
\end{description}
%
However, we do not define $[[{Q}]]$ inductively, because we
require certain equalities to hold:
$$
\begin{array}{rcl}
  [[{Q1 * Q2}]] & = & [[{Q2 * Q1}]] \\
  [[{(Q1*Q2)*Q3}]] & = & [[{Q1*(Q2*Q3)}]] \\
  [[{omega.q * omega.q}]] & = & [[{omega.q}]] \\
  [[{Q * Empty}]] & = & [[{Q}]]
\end{array}
$$
We thus say that a simple constraint is a pair combining a set of unrestricted
constraints $[[{UCtx}]]$ and a multiset of linear constraints $[[{LCtx}]]$.
The linear constraints must
be stored in a multiset, because assuming the same constraint twice is distinct
from assuming it only once.

\begin{definition}[Simple constraints]
$$
\begin{array}{rcll}
  [[{UCtx}]] & \bnfeq & \ldots & \text{set of atomic constraints $[[{q}]]$} \\
  [[{LCtx}]] & \bnfeq & \ldots & \text{multiset of atomic constraints $[[{q}]]$} \\
  [[{Q}]] & \bnfeq & [[{(UCtx,LCtx)}]] & \text{simple constraints}
\end{array}
$$
We can now straightforwardly define the operations we need on simple
constraints:
$$
\begin{array}{ccc}
  \begin{array}{r@@{\;}c@@{\;}l}
    [[{Empty}]] &=& [[{(emptyset, emptyset)}]]
  \end{array}
  &
    \left\{
    \begin{array}{r@@{\;}c@@{\;}l}
      [[{1.q}]] &=& [[{(emptyset, q)}]] \\
      [[{omega.q}]] &=& [[{(q, emptyset)}]]
    \end{array}
                        \right.
  &
    \begin{array}{r@@{\;}c@@{\;}l}
      [[{(UCtx1,LCtx1)*(UCtx2,LCtx2)}]] &=& [[{(UCtx1 \u UCtx2, LCtx1 \u LCtx2)}]]
    \end{array}
\end{array}
$$
\end{definition}

In practice, we do not need to concern ourselves with the
concrete representation of $[[{Q}]]$ as a pair of sets, instead
using the operations defined just above.

The semantics of simple constraints (and, indeed, of atomic
constraints) is given by an \emph{entailment relation}. Just like the
set of atomic constraints, the entailment relation is a parameter of
our system

\begin{figure}
  \maybesmall
  $$
    \begin{array}{l}
      [[Q ||- Q]] \\
      \text{if } [[Q1 ||- Q2]] \text{ and } [[Q * Q2 ||- Q3]] \text{ then } [[Q * Q1 ||- Q3]] \\
      \text{if } [[Q ||- Q1 * Q2]] \text{ then there exists } [[{Q'}]] \text{ and } [[{Q''}]]
      \text{ such that } [[{Q}]]=[[{Q' * Q''}]] \text{, } [[Q' ||- Q1]] \text{ and } [[Q'' ||- Q2]] \\
      \text{if } [[Q ||- Empty]] \text{ then there exists } [[{Q'}]] \text{ such that } [[{Q}]]=[[{omega.Q'}]] \\
      \text{if } [[Q1 ||- Q1']] \text{ and } [[Q2 ||- Q2']] \text{ then } [[Q1 * Q2 ||- Q1' * Q2']] \\
      \text{if } [[Q ||- rho. q]] \text{ then } [[pi . Q ||- (pi.rho). q]] \\
      \text{if } [[Q ||- (pi.rho) . q]] \text{ then there exists } [[{Q'}]] \text{ such
      that } [[{Q}]] = [[{pi. Q'}]] \text{ and } [[Q' ||- rho . q]] \\
      \text{if } [[Q1 ||- Q2]] \text{ then } [[omega.Q1 ||- Q2]] \\
      \text{if } [[Q1 ||- Q2]] \text{ then for all } [[{Q'}]], [[omega.Q' * Q1 ||- Q2]]
    \end{array}
  $$
\caption{Requirements for the entailment relation $[[Q1 ||- Q2]]$}
\label{fig:entailment-relation}
\end{figure}

\begin{definition}[Entailment relation]
  \label{def:entailment-relation}
  The qualified type system is parameterised by a relation $[[Q1 ||-
    Q2]]$ between two simple constraints. The entailment relation must
    obey the laws listed in \Fref{fig:entailment-relation}.
\end{definition}
%
\info{See Fig 3, p14 of OutsideIn\cite{OutsideIn}.}
%

An important feature of simple constraints is that, while scaling
syntactically happens at the level of atomic constraints, these properties
of scaling extend to scaling of arbitrary constraints. Define
$[[{pi.Q}]]$ as:

$$
                      \left\{
                      \begin{array}{r@@{\;}c@@{\;}l}
                        [[{1.(UCtx,LCtx)}]] &=& [[{(UCtx,LCtx)}]] \\
                        [[{omega.(UCtx,LCtx)}]] &=& [[{(UCtx \u LCtx, emptyset)}]]
                      \end{array}
                                                    \right.
$$
%
Then the following properties hold
%
\begin{lemma}[Scaling]
  \label{lem:q:scaling}
  If $[[Q1 ||- Q2]]$, then $[[pi.Q1 ||- pi.Q2]]$.
\end{lemma}

\begin{lemma}[Inversion of scaling]
  \label{lem:q:scaling-inversion}
  If $[[Q1 ||- pi.Q2]]$, then $[[{Q1}]]=[[{pi.Q'}]]$ and $[[Q' ||- Q2]]$ for some $[[{Q'}]]$.
\end{lemma}

\begin{corollary}[Linear assumptions]
If $[[Q1 ||- omega.Q2]]$, then $[[{Q1}]]$ contains no linear assumptions.
\end{corollary}

Proofs of these lemmas (and others) appear in our anonymised supplementary material;
they can be proved by straightforward use of the properties in \Cref{fig:entailment-relation}.

\subsection{Typing rules}
\label{sec:typing-rules}

With this material in place, we can now present our type system. The
grammar is given in \Cref{fig:declarative:grammar}, which also
includes the definitions of scaling on contexts $[[{pi.G}]]$ and addition
of contexts $[[{G1+G2}]]$. Note that addition on contexts is actually
a partial function, as it requires that, if a variable $[[{x}]]$ is bound
in both $[[{G1}]]$ and $[[{G2}]]$, then $[[{x}]]$ is assigned the same
type in both (but perhaps different multiplicities). This partiality
is not a problem in practice, as the required condition for combining
contexts is always satisfied.

\begin{figure}
  \maybesmall
  $$
  \begin{array}{lcll}
    [[{a}]], [[{b}]] & \bnfeq & \ldots & \text{Type variables} \\
    [[{x}]], [[{y}]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[T]] & \bnfeq & \ldots & \text{Type constructors} \\
    [[{K}]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[{s}]] & \bnfeq & [[{forall as. Q =o t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & [[{a}]] \bnfor [[{exists as. t o= Q}]] \bnfor [[{t1 ->_pi t2}]]
                            \bnfor [[{T ts}]] & \text{Types} \\
    [[{G}]], [[{D}]] & \bnfeq & [[{empty}]] \bnfor [[{G, x:_pi s}]] &
                                                              \text{Contexts} \\
    [[{e}]] & \bnfeq & [[{x}]] \bnfor [[{K}]] \bnfor [[{\x. e}]] \bnfor [[e1
                     e2]] \bnfor [[{pack e}]] & \text{Expressions}\\
                 &\bnfor & [[unpack x=e1 in
                     e2]] \bnfor [[{case_pi e of { alts }}]] &\\
                 &\bnfor & [[let_pi
                     x=e1 in e2]] \bnfor [[{let_pi x : s = e1 in e2}]] &
  \end{array}
  $$
  Context scaling $[[{pi.G}]]$ and addition of contexts $[[{G1+G2}]]$ is defined as follows:
  $$
  \begin{array}{cc}
  \left\{
  \begin{array}{r@@{\;}c@@{\;}l}
  [[{pi.empty}]] &=& [[{empty}]] \\
  [[{pi.(G,x:_rho s)}]] &=& [[{pi.G, x:_( pi.rho ) s}]]
  \end{array}
  \right.
  &
  \left\{
  \begin{array}{r@@{\;}c@@{\;}lll}
  [[{(G1, x:_pi s) + G2}]] &=& [[{ G1 + G2', x:_( pi+rho ) s }]] & \text{where} & [[G2 = {x:_rho s} \u G2']] \\
  &&&& [[x \notin G2']] \\
  [[{(G1, x:_pi s) + G2}]] &=& [[{ G1 + G2, x:_pi s }]] & \text{where} & [[x \notin G2]] \\
  [[{empty + G2}]] &=& [[{G2}]]
  \end{array}
  \right.
  \end{array}
  $$
  \caption{Grammar of the qualified type system}
  \label{fig:declarative:grammar}
\end{figure}

\begin{figure}
  \centering
  \drules[E]{$[[Q;G |- e : t]]$}{Expression
    typing}{Var,Abs,App,Pack,Unpack,Let,LetSig,Case,Sub}
  \caption{Qualified type system}
  \label{fig:typing-rules}
\end{figure}

The typing rules are in \Cref{fig:typing-rules}.
A qualified type system~\cite{QualifiedTypes} such as ours introduces a
judgement of the form $[[Q;G |- e : t]]$, where $[[{G}]]$ is a standard
type context, and $[[{Q}]]$ is a constraint we have assumed to be true.
$[[{Q}]]$ behaves
much like $[[{G}]]$, which will be instrumental for
desugaring in \cref{sec:desugaring}; the main difference is
that $[[{G}]]$ is addressed explicitly, whereas $[[{Q}]]$
is used implicitly in \rref{E-Var}.

The type system of \Cref{fig:typing-rules} is purely
declarative: note, for example, that \rref{E-App} does not describe
how to break the typing assumptions into constraints
$[[{Q1}]]$/$[[{Q2}]]$ and contexts $[[{G1}]]$/$[[{G2}]]$. We will see
how to infer constraints in \cref{sec:type-inference}. Yet,
this system is our ground truth: a system with a simple enough
definition that programmers can reason about typing. We do not
directly give a dynamic
semantics to this language; instead, we will give it meaning via
desugaring to a simpler core language in \cref{sec:desugaring}.

We survey several distinctive features of our qualified type system below:
%
\info{See Fig 10, p25 of OutsideIn\cite{OutsideIn}.}
\rae{E-Case should be clearer that the second two premises are universally
quantified over $i$. Actually, it's worse than that, with strangeness like
$[[pi.pii]]$ written in a subscript, but there's really many $[[pii]]$s, I
think.}
\paragraph{Linear functions}
The type of linear functions is written $[[{a ->_1 b}]]$.
  Despite our focus on linear constraints,
  we still need linearity in ordinary arguments.
  For example, the bind combinator for |IOL| (\cref{sec:introduction})
  requires linear arrows: |(>>=) :: IOL a ->. (a ->. IOL b) ->. IOL b|.

  Indeed, the linearity of arrows interacts in interesting
  ways with linear constraints: If $[[f : a ->_omega b]]$ and
  $[[x : 1.q =o a]]$, then calling $[[{f x}]]$ would actually use $[[{q}]]$
  many times. We must make sure it is impossible to derive
  $[[1.q ; f :_omega a ->_omega b, x :_omega 1.q =o a |- f x : b]]$.
  Otherwise we could make, for instance, the |overusing| function from
  \cref{sec:overusing}.
  You can check that $[[1.q ; f :_omega a ->_omega b, x :_omega 1.q =o a |- f x : b]]$
  indeed does not
  type check, because the scaling of $[[{Q2}]]$ in \rref{E-App} ensures that
  the constraint would be $[[{omega.q}]]$ instead. On the other hand,
  it is perfectly fine to have $[[1.q ; f :_omega a ->_1 b, x :_omega 1.q
  =o a |- f x : b]]$ when $[[{f}]]$ is a linear function.

\paragraph{Variables}
As is standard, the \rref{E-Var} rule can weaken variables, but only
unrestricted variables can be weaken (linear variables \emph{must} be
used), hence the weakened context being $[[{omega.G_2}]]$. The rule
also requires the variable $[[{x}]]$ to be available for linear use ($[[{G_1}]]$);
but note that $[[{G_2}]]$ can contain a binding for $[[{x}]]$, in
which case $[[{G_1+omega.G_2}]]=[[{omega.G_2}]]$. This takes care of
the case where $[[{x}]]$ is unrestricted.

\paragraph{Data constructors}
Data constructors $[[{K}]]$ don't have a dedicated typing
rule. Instead they are typed using the \rref{E-Var}, where they are
treated as if they were unrestricted variables.

\paragraph{let-bindings}
Bindings in a |let| may be for either linear or unrestricted variables.
  We could require all bindings to be linear and to implement unrestricted
  information only using |Ur|, but it is very easy to add a multiplicity
  annotation on |let|, and so we do.
\paragraph{Local assumptions}
\Rref{E-Let} includes support for local
  assumptions. We thus have the ability to generalise a subset of
  the constraints needed by $[[{e1}]]$ (but not the type variables---no
  |let|-generalisation here, though it could be added). The inference algorithm of
  \cref{sec:type-inference} will not make use of this
  possibility, but we revisit this capability in \cref{sec:let-generalisation}.
%% \item Data types are not \textsc{gadt}s.
%%   This serves to considerably simplify the \rref*{E-Case}
%%   rule. It would be
%%   straightforward, yet tedious, to extend data types here to full
%%   \textsc{gadt}s.
\paragraph{Existentials}
 We include $[[{exists as. t o= Q}]]$, as
  introduced in \cref{sec:what-it-looks-like}, together with
  the $\kpack$ and $\kunpack$ constructions. See rules~\rref*{E-Pack} and
  \rref*{E-Unpack}.
\info{No substitution on $[[{Q1}]]$ in the $\kunpack$ rule, because there is
  only existential quantification.}


\section{Constraint inference}
\label{sec:type-inference}

The type system of \Cref{fig:typing-rules} gives a declarative description
of what programs are acceptable. We now present the algorithmic counterpart to
this system. Our algorithm is structured, unsurprisingly, around generating and
solving constraints, broadly following the template of
\citet{essence-of-ml-type-inference}.
That is, our algorithm takes a pass over the abstract syntax entered by the
user, generating constraints as it goes. Then, separately, we solve those
constraints (that is, try to satisfy them) in the presence of a set of assumptions,
or we determine that the assumptions do not imply that the constraints hold. In the
latter case, we issue an error to the programmer.

The procedure is responsible for inferring both \emph{types} and \emph{constraints}.
For our type system, type inference can be done independently from constraint
inference. Indeed, we focus on the latter, and defer type inference to
an external oracle (such as~\cite{linear-types-inference}).
That is, we assume an algorithm that produces typing derivations for the
judgement $[[G |- e : t]]$, ignoring all the constraints. Then, we describe a
constraint generation algorithm that passes over these typing derivations.
%
% A typical generate-and-solve algorithm is built around unification variables.
% A unification variable stands for some unknown type. When we see, for example,
% |\x -> ...|, we let the type of |x| be a unification variable, and then
% constraints arising from the body of the expression (or its context) might tell
% us that the unknown type is |Int|. Our system does not do this, however:
% using constraints to solve for unification variables is well understood~\cite{OutsideIn}
% and orthogonal to the concerns that drive the innovations in this paper.
% Instead, the description of the aspects of our algorithm regarding inferring types---as opposed to
% inferring \emph{constraints}---nondeterministically just guess the correct
% type. A real implementation would use unification variables and constraints,
% but they just add clutter here and distract us from the novel parts of our
% algorithm.
%
We can make this simplification for two reasons:
\begin{itemize}
\item We do not formalise type equality constraints, and our implementation
  in \textsc{ghc} (\cref{sec:equality-constraints}) takes care to not allow linear equality constraints to influence type inference.
  Indeed, a typical treatment of unification
  would be unsound for linear equalities, because it reuses the same
  equality many times (or none at all). Linear equalities make sense
  (\citet{shulman2018linear} puts linear
  equalities to great use), but they do not seem to lend themselves to
  automation.
\item We do not support, or intend to support, multiplicity
  polymorphism in constraint arrows. That is, the multiplicity of a
  constraint is always syntactically known to be either linear or
  unrestricted. This way, no equality constraints (which might, conceivably,
  relate multiplicity variables) can interfere with
  constraint resolution.
\end{itemize}
%
%% Our current focus is more narrow than a general typechecking algorithm for
%% all of \textsc{ghc}'s features.
%% As a consequence of these two restrictions (no linear equalities and no
%% multiplicity polymorphic constraints), type inference and (linear) class
%% constraint resolution are completely orthogonal. Therefore, the syntax-directed
%% constraint generation system presented in this section can legitimately assume
%% that type inference is solved elsewhere, greatly simplifying the presentation.


\subsection{Wanted constraints}
\label{sec:wanteds}

The constraints $[[{C}]]$ generated in our system have a richer
logical structure than the simple constraints $[[{Q}]]$, above. Following
\textsc{ghc} and echoing \citet{OutsideIn}, we call these \emph{wanted constraints}:
they are constraints which the constraint solver \emph{wants} to prove.
An unproved wanted constraint results in a type error reported to the programmer.
%
$$
\begin{array}{lcll}
  [[{C}]] & \bnfeq & [[{Q}]] \bnfor [[{C1*C2}]] \bnfor [[{C1&C2}]] \bnfor [[{pi.(Q=>C)}]]&
                                                                \text{Wanted constraints}
\end{array}
$$
%
A simple constraint is a valid wanted constraint, and we have two forms of
conjunction for wanted constraints:
the new
$[[{C1 & C2}]]$ construction (read $[[{C1}]]$ \emph{with} $[[{C2}]]$), alongside
the more typical $[[{C1 * C2}]]$. These are
connectives from linear logic: $[[{C1*C2}]]$ is the
\emph{multiplicative} conjunction, and $[[{C1&C2}]]$ is the \emph{additive}
conjunction. Both connectives are conjunctions, but they differ
% rather dramatically % weasel words combo
in meaning. To satisfy $[[{C1*C2}]]$ one consumes the (linear)
assumptions consumed by satisfying $[[{C1}]]$ and those consumed by $[[{C2}]]$;
if an assumed linear constraint is needed to prove both $[[{C1}]]$ and $[[{C2}]]$,
then $[[{C1*C2}]]$ will not be provable, because that linear assumption cannot
be used twice. On the
other hand, satisfying $[[{C1&C2}]]$ requires that satisfying $[[{C1}]]$
and $[[{C2}]]$ must each
consume the \emph{same} assumptions, which $[[{C1 & C2}]]$ consumes as well.
Thus, if $[[{C}]]$ is assumed linearly (and we have no other assumptions),
then $[[{C*C}]]$ is not provable, while $[[{C&C}]]$ is.
The intuition, here, is that in $[[{C1 & C2}]]$, only
one of $[[{C1}]]$ or $[[{C2}]]$ will be eventually used. ``With'' constraints
arise from the branches in a $\kcase$-expression.

The last form of wanted constraint $[[{C}]]$ is an implication $[[{pi.(Q=>C)}]]$.
Proving $[[{pi.(Q=>C)}]]$ allows us to assume $[[{Q}]]$ linearly while proving $[[{C}]]$,
a total of $[[{pi}]]$
times.
These implications arise when we unpack an existential
package that contains a linear constraint and also when checking a |let|-binding.
We can define scaling over wanted constraints by recursion as follows, where we
use scaling over simple constraints in the simple-constraint case:
$$
\left\{
  \begin{array}{lcl}
    [[{pi.(C1 * C2)}]] & = & [[{pi.C1 * pi.C2}]] \\
    [[{1.(C1 & C2)}]] & = & [[{C1 & C2}]] \\
    [[{omega.(C1 & C2)}]] & = & [[{omega.C1 * omega.C2}]] \\
    [[{pi.(rho.(Q => C))}]] & = & [[{(pi.rho).(Q => C)}]]
  \end{array}
\right.
$$
For the most part, scaling of wanted constraints is straightforward. The only
peculiar case is when we scale the additive conjunction $[[{C1&C2}]]$ by
$[[{omega}]]$, the result is a multiplicative conjunction. The intuition here is
that when if we have both $[[{omega.C1}]]$ and $[[{omega.C2}]]$, then
we can offer a choice of $[[{C1}]]$ and $[[{C2}]]$\,--\,that is $[[{C1 &
  C2}]]$\,--\,$[[{omega}]]$ times, and vice-versa.

We define an entailment relation over wanteds in \Cref{fig:wanted:entailment}.
Note that this relation uses only simple constraints $[[{Q}]]$ as assumptions, as
there is no way to assume the more elaborate $[[{C}]]$\footnote{Allowing the full wanted-constraint syntax
in assumptions is the subject of work by \citet{quantified-constraints}.}.
\begin{figure}
  \maybesmall
  \centering
  \drules[C]{$[[Q |- C]]$} {Wanted-constraint entailment}
  {Dom,Tensor,With,Impl}
  \caption{Wanted-constraint entailment}
  \label{fig:wanted:entailment}
\end{figure}

Before we move on to constraint generation proper, let us highlight a few
technical, yet essential, lemmas about the wanted-constraint
entailment relation.

\begin{lemma}[Inversion]
  \label{lem:inversion}
  The inference rules of $[[Q |- C]]$ can be read bottom-up as well
  as top-down, as is required of $[[Q1 ||- Q2]]$ in
  \Cref{fig:entailment-relation}. That is:
  \begin{itemize}
  \item If $[[Q |- C1*C2]]$, then there exists $[[{Q1}]]$ and $[[{Q2}]]$
    such that $[[Q1 |- C1]]$, $[[Q2 |- C2]]$, and
    $[[{Q}]] = [[{Q1 * Q2}]]$.
  \item If $[[Q |- C1 & C2]]$, then $[[Q |- C1]]$ and $[[Q |- C2]]$.
  \item If $[[Q |- pi.(Q2 => C)]]$, then there exists $[[{Q1}]]$ such
    that $[[Q1 * Q2 |- C]]$ and  $[[{Q}]] = [[{pi.Q1}]]$
  \end{itemize}
\end{lemma}

\begin{lemma}[Scaling]
  \label{lem:wanted:promote}
  If $[[Q |- C]]$, then $[[pi.Q |- pi.C]]$.
\end{lemma}

\begin{lemma}[Inversion of scaling]
  \label{lem:wanted:demote}
  If $[[Q |- pi.C]]$ then $[[Q' |- C]]$ and $[[{Q}]] = [[{pi.Q'}]]$ for some $[[{Q'}]]$.
\end{lemma}

\subsection{Constraint generation}
\label{sec:constraint-generation}
\label{sec:constraint-generation-soundness}

The process of inferring constraints is split into two parts: generating
constraints, which we do in this section, then solving them in
\cref{sec:constraint-solver}. Constraint generation is described by
the judgement $[[G |-> e : t ~> C]]$ (defined in
\Cref{fig:constraint-generation}) which outputs a constraint $[[{C}]]$
required to make $[[e]]$ typecheck.
The definition
$[[G |-> e : t ~> C]]$ is syntax directed, so it can directly be read as an
algorithm, taking as input a \emph{typing derivation} for $[[G |- e : t]]$
(produced by an external type inference oracle as discussed above). Notably, the
algorithm has access to the context splitting from the (previously computed)
typing derivation, and is
thus indeed syntax directed.
\info{See Fig.13, p39 of OutsideIn~\cite{OutsideIn}}

\info{Not caring about inferences simplifies $\kpack$ quite a bit, we
  are using the pseudo-inferred type to generate constraint. In a real
  system, we would need $\kpack$ to know its type (\emph{e.g.} using
  bidirectional type checking).}
\begin{figure}
  \maybesmall
  \centering
  \drules[G]{$[[G |-> e : t ~> C]]$}{Constraint generation}{Var, Abs,
    App, Pack, Unpack, Case, Let, LetSig}

  \caption{Constraint generation}
  \label{fig:constraint-generation}
\end{figure}

The rules of \Cref{fig:constraint-generation} constitute a mostly
unsurprising translation of the rules of \Cref{fig:typing-rules},
except for the following points of interest:

\paragraph{Case expressions}
Note the use of $[[&]]$ in the conclusion of \rref{G-Case}.
We require that each branch of a |case| expression use the exact
same (linear) assumptions; this is enforced by combining the
emitted constraints with $[[&]]$, not $[[*]]$.
%
  This can also be understood in terms of the file-handle example of
  \cref{sec:introduction}:
  if a file is closed in one branch of a $\kcase$, we require it to be closed in
the other branches too.
  Otherwise, the file handle's state will be unknown to the type system
  after the |case|.
%
\paragraph{Implications} The introduction of constraints local to a
  definition (\rref{G-LetSig}) corresponds to
  emitting an implication constraint.
\paragraph{Unannotated |let|}
 However, the~\rref*{G-Let} rule does not produce an implication
  constraint, as we do not model |let|-generalisation~\cite{let-should-not-be-generalised}.
  \cref{sec:let-generalisation} discusses this design
  choice further.


\vspace{1ex}
The key property of the constraint-generation algorithm is that,
if the generated constraint is solvable, then we can indeed type the
term in the qualified type system of
\cref{sec:qualified-type-system}. That is,
these rules are simply an implementation of our declarative qualified
type system.

\begin{lemma}[Soundness of constraint generation]\label{lem:generation-soundness}
  For all $[[{Q_g}]]$, if $[[G |-> e : t ~> C]]$ and $[[Q_g |- C]]$ then
  $[[Q_g; G |- e : t]]$.
\end{lemma}

\subsection{Constraint solving}
\label{sec:constraint-solver}

In this section,
we build a \emph{constraint solver} that proves
that $[[Q_g |- C]]$ holds, as required by \cref{lem:generation-soundness}.
The constraint solver is represented by the following judgement:
%
$$
[[ UCtx ; LCtx_i |-s C ~> LCtx_o]]
$$
%
The judgement
takes in two contexts: $[[{UCtx}]]$, which holds all the unrestricted
atomic constraint assumptions and $[[{LCtx_i}]]$, which holds all the linear
atomic constraint assumptions.
The linear contexts $[[{LCtx_i}]]$ and $[[{LCtx_o}]]$ have been described
as multisets (\cref{sec:constraint-domain}), but we treat them
as ordered lists in the more concrete setting here; we will see soon
why this treatment is necessary.

Linearity requires treating constraints as consumable resources. This
is what $[[{LCtx_o}]]$ is for: it contains the hypotheses of
$[[{LCtx_i}]]$ which are not consumed when proving $[[{C}]]$. As
suggested by the notation, it is an output of the algorithm.

If the constraint solver finds a solution, then the output linear constraints
must be a subset of the input linear constraints, and the solution must indeed
be entailed from the given assumptions.
\begin{lemma}[Constraint solver soundness]
\label{lem:solver-soundness} If $[[UCtx; LCtx_i |-s C ~> LCtx_o]]$, then:
\begin{enumerate}
\item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
\item $[[(UCtx, LCtx_i) |- C * (emptyset, LCtx_o)]]$
\end{enumerate}
\end{lemma}

To handle simple wanted constraints, we will need  a domain-specific
\emph{atomic-constraint solver} to be the algorithmic counterpart of the
abstract entailment relation of \cref{sec:constraint-domain}. The
main solver will appeal to this atomic-constraint solver when solving atomic
constraints.  The atomic-constraint solver is represented by the following
judgement:
%
$$
[[ UCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]
$$

It has a similar structure to the main solver, but only deals with atomic
constraints. Even though the main solver is parameterised by this
atomic-constraint solver, we will give an instantiation in
\cref{sec:simple-constr-solv}.
%
We require the following property of the atomic-constraint solver:

\begin{property}[Atomic-constraint solver soundness]
\label{prop:atomic-solver-soundness} If $[[UCtx; LCtx_i |-simp pi.q ~> LCtx_o]]$, then:
\begin{enumerate}
\item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
\item $[[(UCtx, LCtx_i) ||- pi.q * (emptyset, LCtx_o)]]$
\end{enumerate}
\end{property}

\subsubsection{Constraint solver algorithm}

Building on this atomic-constraint solver, we use a linear proof
search algorithm based on the recipe given
by~\citet{resource-management-for-ll-proof-search}. \Cref{fig:constraint-solver}
presents the rules of the constraint solver.

\begin{figure}
  \maybesmall
  \centering
  \drules[S]{$[[UCtx ; LCtx_i |-s C_w ~> LCtx_o ]]$}{Constraint solving}{Atom, Mult, ImplOne, Add, ImplMany}
  \caption{Constraint solver}
  \label{fig:constraint-solver}
\end{figure}

\begin{itemize}
  \item The~\rref*{S-Mult} rule proceeds by solving one side of a
conjunction first, then passing the output constraints to the other side.
The unrestricted context is shared between both sides.
  \item The~\rref*{S-Add} rule handles additive conjunction. Here, the linear
constraints are also shared between the branches (since additive conjunction is
generated from case expressions, only one of them is actually going to be
executed). Note that both branches must consume exactly the same
resources.
  \item Implications are handled by~\rref*{S-ImplOne} and~\rref*{S-ImplMany},
for solving linear and unrestricted implications, respectively. In
both cases, the assumption of the implication is split into its
unrestricted and linear components.
When solving a linear implication, we union the assumptions with their respective context, and
proceed with solving the conclusion. Importantly (see
\cref{sec:simple-constr-solv} below), the linear assumptions are
added to the front of the list.
 The side condition requires that the output
context is a subset of the input context: this is to ensure that the implication
actually consumes its assumption and does not leak it to the ambient context.
Solving unrestricted implications allows only the conclusion of the implication
to be solved using its own linear assumption, but none of the other linear
constraints. This is because unrestricted implications use their own
assumption linearly, but use everything from the ambient context $[[{omega}]]$
times.
\end{itemize}

\subsubsection{An atomic-constraint solver}
\label{sec:simple-constr-solv}

So far, the atomic-constraint domain has been
an abstract parameter. In this section, though, we offer a concrete
domain which supports our examples.

For the sake of our examples, we need very little: linear
constraints can remain abstract. It is thus sufficient for the entailment relation
(\Cref{fig:simpl-entailment}) to prove $[[{q}]]$ if and only if it
is already assumed---while respecting linearity.

\begin{figure}
  \maybesmall
  \centering
  \begin{subfigure}{\linewidth}
    \drules[Q]{$[[Q1 ||- Q2]]$}{Entailment relation}{Hyp,Prod,Empty}
    \caption{Entailment relation}
    \label{fig:simpl-entailment}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \drules[Atom]{$[[UCtx ; LCtx |-simp pi.q ~>
      LCtx_o]]$}{Atomic-constraint solver}{Many,OneL,OneU}
    \caption{Atomic-constraint solver}
    \label{fig:simpl-solver}
  \end{subfigure}
  \caption{A stripped-down constraint domain}
  \label{fig:predicate-domain}
\end{figure}

The corresponding atomic-constraint solver
(\Cref{fig:simpl-solver}) is more interesting.
It is deterministic: in all circumstances,
only one of the three rules can apply. This means that the
algorithm does not guess, thus never needs to backtrack.
Avoiding guesses is a key property of \textsc{ghc}'s solver~\cite[Section~6.4]{OutsideIn},
one we must maintain if we are to be compatible with \textsc{ghc}.

\Cref{fig:simpl-solver} is also where the fact that the
$[[{LCtx}]]$ are lists comes into play. Indeed, \rref{Atom-OneL}
takes care to use the most recent occurrence of $[[{q}]]$
(remember that \rref{S-ImplOne} adds the new hypotheses on the front of
the list). To understand why, consider the following example:
\begin{code}
f :: FilePath -> IOL ()
f fp =   do  {  pack h <- openFile fp
             ;  let  {  cl :: constraint (Open h) =>. IOL ()
                     ;  cl = closeFile h }
             ;  cl }
\end{code}
In this example, the programmer meant for
|closeFile| to use the |Open h| constraint introduced locally in the type
of |cl|. Yet
there are actually two |Open h| constraints: this local one and the
one assumed in the unpacking of the result of |openFile|. The wrong
choice among the constraints will lead the algorithm to fail.
Choosing the first $[[{q}]]$ linear assumption guarantees we get the
most local one.

Another interesting feature of the solver (\Cref{fig:simpl-solver}) is that
no rule solves a linear constraint if it appears both in the
unrestricted and the linear context.
Consider the following (contrived) \textsc{api}:
\begin{code}
class constraint (C)

giveC :: (constraint (C) => Int) -> Int
useC :: constraint (C) =>. Int
\end{code}
|giveC| gives an unrestricted copy of |constraint (C)| to some continuation, while |useC|
uses |constraint (C)| linearly. Now consider a consumer of this \textsc{api}:
\begin{code}
bad :: constraint (C) =>. (Int, Int)
bad = (giveC useC, useC)
\end{code}
It is possible to give a type derivation to |bad| in the qualified type system
of \cref{sec:qualified-type-system}. In this case, the constraint assignment
is actually unambiguous:
the first |useC| must use the unrestricted |constraint (C)|, while the
second must use the linear |constraint (C)|. This assignment, however, would require
the constraint solver to guess when solving the constraint from the first |useC|.
Accordingly, in order to both avoid backtracking and to keep type inference
independent of the order terms appear in the program text, |bad| is
rejected.
This introduces incompleteness with respect the entailment
relation. We conjecture that this is the only source of
incompleteness that we introduce beyond what is already in
\textsc{ghc}~\cite[Section~6]{OutsideIn}.

\section{Desugaring}
\label{sec:desugaring}

The semantics of our language is given by desugaring it into
a simpler core language: a variant of the $λ^q$
calculus~\cite{LinearHaskell}. We
define the core language's type system here; its operational semantics
is the same, \emph{mutatis mutandis}, as that of Linear Haskell.

\subsection{The core calculus}
\label{sec:core-calculus}
\label{sec:ds:inferred-constraints}

\begin{figure}
  \maybesmall
  \centering
  $$
  \begin{array}{lcll}
    [[{s}]] & \bnfeq & [[{forall as. t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & ... \bnfor [[{exists as. t o- u}]] & \text{Types} \\
    [[{e}]] & \bnfeq & ... \bnfor [[{pack (e1, e2)}]] \bnfor [[{unpack (x,y)=e1 in e2}]] & \text{Expressions}
  \end{array}
  $$

  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Pack,Unpack}
  \caption{Core calculus (subset)}
  \label{fig:core-typing-rules}\label{fig:core-grammar}
\end{figure}

The core calculus is a variant of the type system defined in
\cref{sec:qualified-type-system}, but without constraints. That is, the evidence for constraints is passed explicitly in this core calculus.
%
Following $λ^q$, we assume the existence of the following data types:
\begin{itemize}
\item $[[{t1 ** t2}]]$ with sole constructor
  $[[ (,) : forall a b. a ->_1 b ->_1 a ** b ]]$. We will write $[[(e1,
  e2)]]$ for $[[{(,) e1 e2}]]$.
\item $[[{unit}]]$ with sole constructor $[[() : unit]]$.
\item $[[{Ur t}]]$ with sole constructor $[[ Ur : forall a. a ->_omega
  Ur a]]$
\end{itemize}
%
\Cref{fig:core-typing-rules}
highlights the differences from the qualified system:
\begin{itemize}
  \item Type schemes $[[{s}]]$ do not support qualified types.
  \item Existentially quantified types ($[[{exists as. t o= Q}]]$) are now represented as an (existentially quantified, linear) pair of values ($[[{exists as. t2 o- t1}]]$).
Accordingly, $\kpack$ and $\kunpack$ operate on pairs.
\end{itemize}
%
The differences between our core calculus and $λ^q$ are as follows:
\begin{itemize}
\item We do not support multiplicity polymorphism.
\item On the other hand, we do include type polymorphism.
\item Polymorphism is implicit rather than explicit. This is not an
  essential difference, but it simplifies the presentation. We could,
for example, include more details in the terms in order to make type-checking
more obvious; this amounts essentially to an encoding of typing derivations
in the terms\footnote{See, for example, \citet{weirich-icfp17} and their
comparison between an implicit core language D and an explicit one DC.}.
\item We have existential types. These can be realised in regular Haskell as a
  family of datatypes.
\end{itemize}

Using \cref{lem:generation-soundness} together with
\cref{lem:solver-soundness} we know that if
$[[G |-> e : t ~> C]]$ and $[[UCtx ; LCtx |-s C ~> emptyset ]]$, then
$[[(UCtx, LCtx) ; G |- e : t]]$.
%
It only remains to desugar derivations of $[[Q;G|-e : t]]$ into the
core calculus.

\subsection{From qualified to core}
\label{sec:ds:from-qualified-core}

\subsubsection{Evidence}
In order to desugar derivations of the qualified system to the core calculus,
we pass evidence explicitly\footnote{This technique is also often called
  dictionary-passing style \cite{type-classes-impl} because, in the case of type classes, evidences are
  dictionaries, and because type classes were the original form of constraints
  in Haskell.}.
%
To do so, we require some more material from
constraints. Namely, we assume a type $[[{Ev(q)}]]$ for each atomic
constraint $[[{q}]]$,
defined in \Cref{fig:evidence}.  The $[[Ev(Hole)]]$ operation
extends to simple constraints as
$[[{Ev(Q)}]]$.
Furthermore, we require that for every $[[{Q1}]]$ and $[[{Q2}]]$
such that $[[Q1 ||- Q2]]$, there is a (linear) function
$[[Ev(Q1 ||- Q2) : Ev(Q1) ->_1 Ev(Q2)]]$.

Let us now define a family of functions $[[{Ds(Hole)}]]$ to translate
the type schemes, types, contexts, and typing derivations of the qualified system into the
types, type schemes, contexts, and terms of the core calculus.

\subsubsection{Translating types}
Type schemes $[[{s}]]$ are translated by turning the implicit argument $[[{Q}]]$
into an explicit one of type $[[{Ev(Q)}]]$. Translating types $[[{t}]]$
and contexts $[[{G}]]$ proceeds as
expected.

\begin{minipage}{0.5\linewidth}
%
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(forall as. Q =o t)}]] & = & [[{forall as. Ev(Q) ->_1 Ds(t)}]] \\
  \end{array}
\right.
$$
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(t1 ->_pi t2)}]] & = & [[{Ds(t1) ->_pi Ds(t2)}]] \\
    [[{Ds(exists as. t o= Q)}]] & = & [[{exists as. Ds(t) o- Ev(Q)}]]
  \end{array}
\right.
$$
%
\end{minipage}%
\begin{minipage}{0.5\linewidth}
$$
\left\{
  \begin{array}{lcl}
    [[{Ds(empty)}]] &=& [[{empty}]] \\
    [[{Ds(G, x :_pi t)}]] &=& [[{Ds(G), x :_pi Ds(t)}]]
  \end{array}
\right.
$$
\end{minipage}

\subsubsection{Translating terms}
Given a derivation $[[Q;G |- e : t]]$, we can build an expression
$[[Ds(z;Q;G |- e : t)]]$, such that
$[[Ds(G), z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$ (for some fresh variable
$[[{z}]]$). Even though we abbreviate the derivation as only its
concluding judgement, the translation is defined recursively on the
whole typing derivation: in particular, we have access to typing rule
premises in the body of the definition.
%
We present some of the interesting cases in \Cref{fig:desugaring}.
\begin{figure}
    \maybesmall
\centering
  \begin{subfigure}{0.3\linewidth}%
$$
\left\{
  \begin{array}{lcl}
    [[{Ev(1.q)}]] & = & [[{Ev(q)}]] \\
    [[{Ev(omega.q)}]] & = & [[{Ur (Ev(q))}]] \\
    [[{Ev(Empty)}]] & = & [[{unit}]] \\
    [[{Ev(Q1 * Q2)}]] & = & [[{Ev(Q1) ** Ev(Q2)}]]
  \end{array}
\right.
$$
  \caption{Evidence passing}
  \label{fig:evidence}
  \end{subfigure}\hfill
  \begin{subfigure}{0.7\linewidth}%
%{
%format |- = "[[|-]]"
%format ||- = "[[||-]]"
%format G = "[[{G}]]"
%format G1 = "[[{G1}]]"
%format G2 = "[[{G2}]]"
%format Q = "[[{Q}]]"
%format Q1 = "[[{Q1}]]"
%format Q2 = "[[{Q2}]]"
%format * = "[[*]]"
%format Ds(z)(j) = "\dsterm{\ottmv{" z "}}{" j "}"
%format Ev(x) = "\dsevidence{" x "}"
%format unpack = "\ottkw{unpack}"
%format u = "[[{u}]]"
%format OneOf (a) (b) = a"\mathop{:_{1}}"b
%format Sub t as bs = t "[" as "/" bs "]"
%format case_1 = "[[case_1]]"
%format e1 = "[[{e1}]]"
%format e2 = "[[{e2}]]"
%format t1 = "[[{t1}]]"
%format as = "[[{as}]]"
%format ts = "[[{ts}]]"
%format let_1 = "[[let_1]]"
%format z1 = "[[{z1}]]"
%format z2 = "[[{z2}]]"
%format z2' = "[[{z2}]]''"
%format + = "\ottsym{+}"
%format t = "\tau"
$$
\left\{
  \;
  \begin{minipage}{0.5\linewidth}
\begin{code}
Ds(z)(Q;G |- x : Sub u ts as) = x z
Ds(z)(Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t)  =
  case_1 z of { (z1, z2) ->
    unpack (z',x) = Ds(z1)(Q1;G1 |- e1 : exists as. t1 .<= Q) in
    let_1 z2' = (z2,z') in
    Ds(z2')(Q2 * Q;G2, OneOf x t1 |- e2 : t) }
Ds(z)(Q;G |- e : t)  =  -- \rref{E-Sub}
  let_1 z' = Ev(Q ||- Q1) z in Ds(z')(Q1;G |- e : t)
...
\end{code}
  \end{minipage}
\right.
$$
%}
  \caption{Desugaring (subset)}
  \label{fig:desugaring}
  \end{subfigure}
  \caption{Evidence passing and desugaring}
\end{figure}

The cases correspond to the~\rref*{E-Var},~\rref*{E-Unpack}\footnote{The attentive
reader may note that the case for $\ottkw{unpack}$ extracts out $[[{Q1}]]$ and $[[{Q2}]]$
from the provided simple constraint. Given that simple constraints $[[{Q}]]$ have no
internal ordering and allow duplicates (in the non-linear component), this splitting
is not well defined. To fix this, an implementation would have to \emph{name} individual
components of $[[{Q}]]$, and then the typing derivation can indicate which constraints
go with which sub-expression. Happily, \textsc{ghc} \emph{already} names its constraints,
and so this approach fits easily in the implementation. We could also augment our formalism
here with these details, but they add clutter with little insight.}, and~\rref*{E-Sub} rules, respectively.
Variables are stored with qualified types in the environment, so they get
translated to functions that take the evidence as argument. Accordingly, the evidence
is inserted by passing $[[{z}]]$ as an argument.
Handling $\kunpack$ requires splitting the context into two: $[[{e1}]]$ is desugared as a pair, and the evidence
it contains is passed to $[[{e2}]]$. Finally, subsumption summons the function corresponding to the entailment relation $[[Q ||- Q1]]$
and applies it to $[[{z}]]$ : $[[{Ev(Q)}]]$ then proceeds to desugar $[[{e}]]$ with the resulting evidence for $[[{Q1}]]$.
Crucially, since $[[{Ds(z;Hole)}]]$ is defined on \emph{derivations}, we can access the premises used in the rule.
Namely, $[[{Q1}]]$ is available in this last case from the~\rref*{E-Sub} rule's premise.

It is straightforward by induction, to verify that desugaring is correct:
%
\begin{theorem}[Desugaring]
If $[[Q;G |- e : t]]$, then
$[[Ds(G), z:_1 Ev(Q) |- Ds(z;Q;G |- e : t) : Ds(t)]]$, for any fresh
variable $[[{z}]]$.
\end{theorem}

Thanks to the desugaring machinery, the semantics of a language with linear
constraints can be understood in terms of a simple core language with linear
types, such as $λ^q$, or indeed, \textsc{ghc} Core.

\section{Integrating into \textsc{ghc}}

One of the guiding principles behind our design was ease of integration with
modern Haskell. In this section we describe some of the particulars of adding
linear constraints to \textsc{ghc}.

\subsection{Implementation}
\label{sec:implementation}

We have written a prototype implementation of linear constraints on top of \textsc{ghc} 9.1, a version that already
ships with the @LinearTypes@ extension. Function arrows (|->|) and context arrows
(|=>|) share the same internal representation in the typechecker, differentiated
only by a boolean flag. Thus, the @LinearTypes@ implementation effort has already
laid down the bureaucratic ground work of annotating these arrows with
multiplicity information.

The key changes affect constraint generation and constraint solving. Constraints
are now annotated with a multiplicity, according to the context from
which they arise. With @LinearTypes@, \textsc{ghc} already scales the usage
of term variables. We simply modified the scaling function to capture all the
generated constraints and re-emit a scaled version of them, which is a fairly local
change.

The constraint solver maintains a set of given constraints (the \emph{inert set}
in \textsc{ghc} jargon), which corresponds to the $[[{UCtx}]]$ and $[[{LCtx}]]$
contexts in our solver judgements in \cref{sec:constraint-solver}. When
the solver goes under an implication, the assumptions of the implication are
added to set of givens. When a new given is added, we record the \emph{level} of
the implication (how many implications deep the constraint arises from) along
with the constraint. This is to ensure that in case there are multiple matching
givens, the constraint solver selects the innermost one
(in \cref{sec:constraint-solver} we use an ordered list of linear assumptions
for this purpose).

As constraint solving proceeds, the compiler pipeline constructs a
term in a typed language known as \textsc{ghc} Core~\cite{system-fc}.
In Core, type class constraints are turned into explicit evidence (see
\cref{sec:desugaring}). Thanks to being fully annotated, Core has
decidable typechecking which is useful in debugging modifications to
the compiler. Thus, the Core typechecker verifies that the desugaring
procedure produced a linearity-respecting program before code
generation occurs.

\subsection{Interaction with other features}

Since constraints play an important role in \textsc{ghc}'s type system, we must
pay close attention to the interaction of linearity with other language features
related to constraints. Of these, we point out two that require some extra care.

\subsubsection{Superclasses}

Haskell's type classes can have \emph{superclasses}, which place constraints on
all of the instances of that class. For example, the |Ord| class is defined as
\begin{code}
class constraint(Eq a) => constraint(Ord a) where
  ...
\end{code}
which means that every ordered type must also support equality. Such
superclass declarations extend the entailment relation: if we know that a type
is ordered, we also know that it supports equality. This is troublesome if we
have a linear occurrence of |constraint(Ord a)|, because then using this entailment, we could
conclude that a linear constraint (|constraint(Ord a)|) implies an unrestricted constraint
(|constraint(Eq a)|), which violates \cref{lem:q:scaling-inversion}.

But even linear superclass constraints cause trouble. Consider a version of |constraint(Ord a)|
that has |constraint(Eq a)| as a linear superclass.
\begin{code}
class constraint(Eq a) =>. constraint(Ord a) where
  ...
\end{code}
When given a linear |constraint(Ord a)|, should we keep it as |constraint(Ord a)|, or rewrite to
|constraint(Eq a)| using the entailment? Short of backtracking, the constraint solver
needs to make a guess, which \textsc{ghc} never does.

To address both of these issues at once, we make the following rule: the
superclasses of a linear constraint are \emph{never} expanded.

\subsubsection{Equality constraints}
\label{sec:equality-constraints}

In \cref{sec:type-inference} we argued that \emph{type} inference and
\emph{constraint} inference can be performed independently. However, this is not
the case for \textsc{ghc}'s constraint domain, because it supports equality
constraints, which allows unification problems to be deferred, and potentially
be solvable only after solving other constraints first.

To reconcile this with our presentation, we need to ensure that
\emph{unrestricted constraint} inference and \emph{linear constraint} inference
can be performed independently. That is, solving a linear constraint should
never be required for solving an unrestricted constraint. This is ensured by
\cref{lem:q:scaling-inversion}.

They key is to represent unification problems as \emph{unrestricted} equality
constraints, so a given linear equality constraint cannot be used during type
inference.  Then, the only way a given linear equality constraint can be used is
to solve a wanted linear equality. This way, linear equalities require no
special treatment, and are harmless.

\section{Extensions}
\label{sec:design-decisions}

The system presented in \cref{sec:qualified-type-system,sec:type-inference} is already capable of supporting the examples
in \cref{sec:what-it-looks-like,sec:memory-ownership}. In this
section, we consider some potential avenues for extensions.

\subsection{let generalisation}
\label{sec:let-generalisation}

As discussed in \cref{sec:constraint-generation}, the~\rref*{G-Let} rule of our
constraint generator does not generalise the type of |let|-bindings, which is in
line with \textsc{ghc}'s existing behaviour~\cite[Section 4.2]{OutsideIn}.
There, this behaviour was guided by concerns around inferring type variables,
which is harder in the presence of local equality assumptions (\emph{i.e.}
\textsc{gadt} pattern matching).

In this section, however, we argue that generalising over linear constraints
may, in fact, improve user experience.
Let us revisit the |firstLine| example from \cref{sec:introduction}, but
this time, instead of executing |closeFile| directly, we assign it to a variable
in a |let|-binding:
\begin{code}
firstLine :: FilePath -> IOL String
firstLine fp =   do  {  pack (Ur h) <- openFile fp
                     ;  let closeOp = closeFile h
                     ;  pack (Ur xs) <- readLine h
                     ;  closeOp
                     ;  return xs }
\end{code}
This program looks reasonable; however, it is rejected. The type of
|closeOp| is |IOL ()|, which means that the definition of |closeOp|
consumes the linear constraint |Open h|. So, by the time we attempt
|readLine h|, the constraint is no longer available.

What the programmer really meant, here, was for |closeOp| to have type
|constraint (Open h) =>. IOL ()|. After all, a |let| definition is not part of the
sequence of instructions: it is just a definition for later, not
intended to consume the current state of the file. With no |let|-generalisation,
the only way to give |closeOp| the type |constraint (Open h) =>. IOL
()| is to give |closeOp| a type signature. In current \textsc{ghc}, we
can't write that signature down, since there is no syntax to bind
the type variable |h| in the program text\footnote{\citet{variables-in-patterns} describe a way to fix this shortcoming.}. But
even ignoring this, it would be rather unfortunate if the default
behaviour of |let|, in the presence of linear constraints, almost never was
what the programmer wants.

To handle $\klet$-generalisation, let us consider the following rule
%
$$
\drule{G-LetGen}
$$
%
This rule is non-deterministic, because it requires finding $[[{Q_r}]]$ and
$[[{Q}]]$. We can modify the constraint solver of
\cref{sec:constraint-solver} to find $[[{Q_r*Q}]]$, but we
still have to split the residual into $[[{Q_r}]]$ and
$[[{Q}]]$ somehow.

%% What we would like to say is ``$[[{Q}]]$ is the set of linear
%% constraints''. But it's not clear how to make it formal.

Any predictable strategy would do: as long as our rule an instance of the
\rref*{G-LetGen} rule, constraint generation will be sound. Experience
will tell whether we can find a better suited strategy than the current
one, which never generalises any constraint.

\subsection{Empty cases}
\label{sec:empty-cases}

Throughout the article we have assumed that $\kcase$-expressions
always have a non-empty list of alternatives. This is, incidentally,
also how Haskell originally behaved; though \textsc{ghc} now has an
|EmptyCase| extension to allow empty lists of alternatives.

% Not allowing empty lists of alternatives is, therefore not terrible in
% principle. Though it makes empty types more awkward than they need to
% be, and, of course, to support the entirety of \textsc{ghc}, we will
% need to support empty lists of alternatives.

The reason why it has been omitted from the rest of the article is
that generating constraints for an empty $\kcase$ requires an $0$-ary version
of $[[{C1&C2}]]$, usually written $[[{Top}]]$ in Linear Logic. The
corresponding entailment rule would be
$$
\drule{C-Top}
$$
That is $[[{Top}]]$ is unconditionally true, and can consume any number
of linear given constraints--- indeed, the corresponding program is already crashing.
The \rref*{C-Top} rule thus induces a
considerable amount of non-determinism in the constraint solver. Eliminating the
non-determinism induced by $[[{Top}]]$ is ultimately
what~\citet{resource-management-for-ll-proof-search} builds up
to. Their methods can be adapted to the constraint solver of
\cref{sec:constraint-solver} without any technical
difficulty. We chose, however, to keep empty cases out the
presentation because they have a very high overhead and would distract from
the point. Instead, we refer readers
to~\citet[Section 4]{resource-management-for-ll-proof-search} for a
careful treatment of $[[{Top}]]$.

\subsection{Inferring $\ottkw{pack}$ and $\ottkw{unpack}$}
\label{sec:implicit-existentials}
Recent work~\cite{existentials} describes an algorithm (call it \textsc{edwl}, after the
authors' names) that
can infer the location of $\ottkw{pack}$s and $\ottkw{unpack}$s\footnote{Actually,
\citet{existentials} use an $\ottkw{open}$ construct instead of $\ottkw{unpack}$
to access the contents of an existential package, but that distinction does
not affect our usage of existentials with linear constraints.}
in a user's program. In Section~9.2 of that paper,
the authors extend their system to include class constraints,
much as we allow our existential packages to carry linear constraints.

Accordingly, \textsc{edwl} would work well for us here.
The \textsc{edwl} algorithm is only a small change on the way some types are treated during
bidirectional type-checking. Though the presentation of linear constraints is not
written using a bidirectional algorithm, our implementation in \textsc{ghc}
is indeed bidirectional (as \textsc{ghc}'s existing type inference algorithm
is bidirectional, as described by \citet{practical-type-inference} and
\citet{visible-type-application}) and produces constraints much like we
have presented here, formally. None of this would change in adapting \textsc{edwl}.
Indeed, it would seem that the two extensions are orthogonal in implementation,
though avoiding the need for explicit $\ottkw{pack}$ and $\ottkw{unpack}$ would
make linear constraints easier to use. Then |quicksort| could
just be written as

\begin{minipage}{0.5\linewidth}
> quicksort :: constraint (RW n) =>. PArray a n -> () .<= constraint (RW n)
> quicksort arr =
>   if length arr <= 1 then ()
>   else let   (l, r) = split arr (partition arr)
>              () = quicksort l
>              () = quicksort r
>        in join l r
\end{minipage}
\begin{minipage}{0.5\linewidth}
> partition :: constraint (RW n) =>. PArray a n -> Int .<= constraint (RW n)
> partition arr
>   = let pivot = read arr (length arr - 1) in
>     foldl (\i j ->
>             if read arr j <= pivot then i + 1
>             else let _ = swap arr i j in i
>           ) 0 [0..len-1]
\end{minipage}


\csongor{Should this example be included at all? do we need to say anything more?}

% One challenge if existential packing and unpacking are inferred is that
% the order in which a program is executed might become ambiguous.
% Picking up the example from \cref{sec:atomic-references},
% consider this function:

% > writeTwo :: constraint (RW n) =>. AtomRef a n -> a -> a -> () .<= constraint (RW n)
% > writeTwo ref val1 val2 =  let  unit1 = writeRef ref val1
% >                                unit2 = writeRef ref val2 in
% >                           ()

% In understanding this code, we first must worry about laziness. Haskell's |let|
% is lazy, meaning that the calls to |writeRef| might never be evaluated, if nothing
% forces them. Surprisingly, whether or not these get evaluated depends on the types
% that are inferred for |unit1| and |unit2|.

% One possibility is
% |unit1, unit2 :: constraint (RW n) =>. () .<= constraint (RW n)|. With this inferred type,
% the calls to |writeRef| are never evaluated: the provided |constraint (RW n)| is just
% packed with the |()| in the result, ignoring the two |unit|s.

% Another possibility is |unit1, unit2 :: ()|. With this inferred type, both |writeRef|s
% must consume and re-produce the |constraint (RW n)| constraint. Because the produced
% constraint is used, the |writeRef| call must be evaluated (so it can produce that |constraint (RW n)|
% constraint), and the reference |ref| will be updated. However, we still do not
% know the order in which the |writeRef|s will be evaluated: perhaps |unit1| consumes
% the |constraint (RW n)| produced by |unit2|, forcing the |writeRef|s in the
% opposite order to how they are written.

% This is disappointing: we cannot have type inference tell us what order
% our program is to be evaluated. One solution is to reject ambiguous programs
% like |writeTwo|. This could be achieved by changing \rref{Atom-OneL} of
% \Cref{fig:simpl-solver} to require that the desired constraint
% has precisely one matching assumption. Then, when there is any ambiguity,
% we simply reject. Would this be too restrictive in practice? A middle ground
% might be to track (in our list of linear assumptions) when we enter a more
% local scope (this is actually already done by \textsc{ghc}), and reject only
% programs with multiple identical assumptions in the same local scope.

% The issues here---similar to those raised in \cref{sec:let-generalisation}---have
% potential solutions, but the precise way forward is best informed by
% experience, as users begin to adopt these features and we get a better
% sense for prominent patterns and how our choices should be tuned.

\section{Related work}
\label{sec:related-work}

\paragraph{OutsideIn}
\label{sec:outsidein}

Our aim is to integrate the present work in \textsc{ghc}, and
accordingly the qualified type system in
\cref{sec:qualified-type-system} and the constraint inference
algorithm in \cref{sec:type-inference} follow a similar
presentation to that of OutsideIn~\cite{OutsideIn}, \textsc{ghc}'s
constraint solver algorithm.  Even though our presentation is
self-contained, we outline some of the differences from that work.

The solver judgement in OutsideIn takes the following form:
%
\[\mathcal{Q}\ ;\ Q_{\mathit{given}}\ ;\ \overline{\alpha}_{\mathit{tch}} \overset{\mathit{solv}}{\mapsto} C_{\mathit{wanted}} \leadsto Q_{\mathit{residual}}\  ; \ \theta\]
%
The main differences from our solver judgement in \cref{sec:constraint-solver} are:
\begin{itemize}
  \item OutsideIn's judgement includes top-level axioms schemes separately
($\mathcal{Q}$), which we have omitted for the sake of brevity and are instead
included in $Q_{\mathit{given}}$.
  \item We present the \emph{given} constraints ($Q_{\mathit{given}}$ in OutsideIn) as two separate
constraint sets $[[{UCtx}]]$ and $[[{LCtx}]]$, standing for the unrestricted and linear
parts respectively.
  \item In addition to constraint inference, OutsideIn performs type
inference, requiring additional bookkeeping in the solver judgment. The solver
takes as input a set of \emph{touchable} variables $\overline{\alpha}_{tch}$
which record the type variables that can be unified at any given time, and
produces a type substitution $\theta$ as an output (whose domain is a subset of
the touchable variables).
As discussed in \cref{sec:type-inference}, we do not perform\jp{describe?} type
inference, only constraint inference. Therefore, our solver need not\jp{explicitly?} return a
type assignment.
  \item The most important difference is the output of the algorithms. Both
OutsideIn and our solver output a set of constraints, $Q_{\mathit{residual}}$ and
$[[{LCtx_o}]]$ respectively. However, the meaning of these contexts is different.
OutsideIn's \emph{residual} constraints $Q_{\mathit{residual}}$
correspond to the part of $C_{\mathit{wanted}}$ that could not be solved from the
assumptions. These residuals are then quantified over in the generalisation step
of the inference algorithm. We omit these residuals, which means that our
algorithm cannot infer qualified types (though see \cref{sec:let-generalisation}).
Our \emph{output} constraints $[[{LCtx_o}]]$ instead correspond to the part of the
\emph{linear} givens $[[{LCtx_i}]]$ that were not used in the solution for $[[{C_w}]]$.
Keeping track of these unused constraints is crucial: ultimately we need to
make sure that every linear constraint is used.

\item Finally, while OutsideIn has a single kind of conjunction, our constraint
language requires two: $[[{Q1*Q2}]]$ and $[[{Q1&Q2}]]$. This shows up when
generating constraints for $\kcase$ expressions in the~\rref{G-Case} rule.
OutsideIn accumulates constraints across branches (taking the union of each
branch), whereas we need to make sure that each branch of a $\kcase$-expression
consumes the same constraints.
%
This is easily understood in terms of the file example of
\cref{sec:introduction}: if a file is closed in one branch of a $\kcase$,
it must be closed in the other branches, too. Otherwise, its state will
be unknown to the type system after the $\kcase$.
\end{itemize}

%% I (csongor) don't think this is worth mentioning
%% \item A more minor difference with OutsideIn is that we have an
%%   explicit \rref*{E-Sub} rule, while OutsideIn uses simple constraint
%%   entailment directly in the relevant rules. In OutsideIn, only the
%%   \rref*{E-Var} rule needed subsumption; we would also need it for the
%%   \rref*{E-Pack} rule as well. So we preferred having one shared
%%   dedicated rule.
\paragraph{Rust}

The memory ownership example of \cref{sec:memory-ownership} is
strongly inspired by Rust.
%
Rust is built with ownership and borrowing for memory
management from the ground up. As a consequence, it has a much more convenient syntax
than Linear Haskell with linear constraints can propose. Rust's
convenient syntax comes at the price that it is almost impossible to
write tail-recursive functions, which is surprising from the
perspective of a functional programmer.

On the other hand, the focus of Linear Haskell, as well as this paper,
is to provide programmers with the tools to create safe interfaces and
libraries. The language itself is agnostic about what linear
constraints mean. Although linear constraints do not have the
convenience of Rust's syntax, we expect that they will support a
greater variety of abstractions. Even though Rust programmers have
come up with varied abstractions which leverage the borrowing
mechanism to support applications going beyond memory management (for
instance, safe file handling), it is unclear that all applications
supported by linear constraints are reducible to the borrowing
mechanism.

\paragraph{Languages with capabilities}

Both Mezzo~\cite{mezzo-permissions} and
\textsc{ats}~\cite{AtsLinearViews} served as inspiration for the
design of linear constraints. Of the two, Mezzo is more specialised,
being entirely built around its system of capabilities.  \textsc{Ats}
is the closest to our system because it appeals explicitly to linear
logic, and because the capabilities (known as \emph{stateful views})
are not tied to a particular use case. However,
\textsc{ats} does not have full inference of capabilities.

Other than that, the two systems have a lot of similarities. They have a
finer-grained capability system than is expressible in Rust (or our
encoding of it in \cref{sec:memory-ownership}) which makes it possible to change
the type of a reference cell upon write. They also eschew scoped
borrowing in favour of more traditional read and write capabilities. In
exchange, neither Mezzo nor \textsc{ats} support $O(1)$ freezing like
in \cref{sec:memory-ownership}.

Mezzo, being geared towards functional programming, does support
freezing, but freezing a nested data structure requires traversing it.
As far as we know, \textsc{ats} doesn't support
freezing. \textsc{Ats} is more oriented towards system programming.

%if False
We chose an example in the style of rust for
\cref{sec:memory-ownership} because freezing arrays of arrays
in $O(1)$ was one of the initial motivations of this article. However,
a Mezzo or \textsc{ats} style of capabilities could certainly be
encoded within linear constraints.
%endif

Linear constraints are more general than either Mezzo or \textsc{ats},
while maintaining a considerably simpler inference algorithm, and at
the same time supporting a richer set of constraints (such as \textsc{gadt}s). This
simplicity is a benefit of abstracting over the simple-constraint
domain. In fact, it should be possible to see Mezzo or \textsc{ats} as
particular instantiations of the simple-constraint domain, with linear
constraints providing the general inference mechanism.

However, both Mezzo and \textsc{ats} have an advantage that we do not:
they assume that their instructions are properly sequenced, whereas
basing linear constraints on Haskell, a lazy language, we are forced
to make sequencing explicit in \textsc{api}s.

\paragraph{Logic programming}

There are a lot of commonalities between \textsc{ghc}'s constraint and logic
programs. Traditional type classes can be seen as Horn clause programs, much
like Prolog programs. \textsc{ghc} puts further restrictionss in order to
avoid backtracking for speed and predictability.

The recent addition of quantified
constraints~\cite{quantified-constraints} extends type class
resolution to Hereditary Harrop~\cite{hereditary-harrop} programs. A generalisation of the
Hereditary Harrop fragment to linear logic, described by~\citet{hh-ll},
is the foundation of the Lolli language~\cite{hodas-thesis-lolli}.
The authors also coin the notion of \emph{uniform} proof. A fragment where
uniform proofs are complete supports goal-oriented proof search, like
Prolog does.

Completeness of uniform proofs is equivalent to
\cref{lem:inversion}, which, in turn, is used in the proof of the
soundness lemma~\ref{lem:generation-soundness}. This seems to indicate
that goal-oriented search is baked into the definition of OutsideIn. An
immediate consequence of this observation, however, is that
the fragment of linear logic described by~\citet{hh-ll} (and
for which~\citet{resource-management-for-ll-proof-search} provides a
refined search strategy) contains the Hereditary Harrop fragment of
intuitionistic logic guarantees that quantified constraints do not
break our proofs.

\paragraph{Resource usage analysis}

\Citet{resource-usage-analysis} introduce a framework which can be
instantiated into many resource analyses (such as proper deallocation
of resources) can be modelled and analysed. In particular they give a
decision procedure for a wide class of such analyses. Although our
objects of study intersect, our purpose is quite different as theirs
is a tool for language designer to design analyses, while ours is for
programmers to implement new abstractions as libraries.

\section{Conclusion}
\label{sec:conclusion}

We showed how a simple linear type system like that of Linear
Haskell can be extended with an inference mechanism which lets the
compiler manage some of the additional complexity of linear types
instead of the programmer. Linear constraints narrow the gap between linearly
typed languages and dedicated linear-like typing disciplines such as Rust's,
Mezzo's, or \textsc{ats}'s.

We also demonstrate how an existing constraint solver can be extended to handle
linearity. Our design of
linear constraints fits nicely into Haskell. Indeed, linear constraints can be
thought of as an extension of Haskell's type class mechanism. This way, the
design also integrates well into \textsc{ghc}, as demonstrated by our prototype
implementation, which required modest changes to the compiler. Remarkably, all we needed to do was to
adapt the work of \citet{resource-management-for-ll-proof-search} to the OutsideIn
framework. It is also quite serendipitous that the notion of uniform
proof from \citet{hh-ll}, which was introduced to prove the
completeness of a proof search strategy, ends up being crucial to the
soundness of constraint generation.

In some cases, like the file example of \cref{sec:introduction},
linear constraints are a mere convenience that reduce line noise and
make code more idiomatic. But the memory management \textsc{api} of
\cref{sec:memory-ownership} is not practical without
linear constraints. Certainly, ownership proofs could be managed manually, but
it is hard to imagine a circumstance where this tedious task would be worth the
cost.

This, really, is the philosophy of linear constraints: lowering
the cost of linear types so that more theoretical applications become
practical applications. And we achieved this at a surprisingly low price: teaching
linear logic to \textsc{ghc}'s constraint solver.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\newpage

\appendix

\section{Full descriptions}
\label{sec:appendix:full-descriptions}

In this appendix, we give, for reference, complete descriptions of the
type systems, functions, etc. that we have abbreviated in the main % etc should never be followed by '…'
body of the article.

\subsection{Core calculus}
\label{sec:appendix:core-calculus}

This is the complete version of the core calculus described in
\cref{sec:core-calculus}. The full grammar is given by
\Cref{fig:full:core-grammar} and the type system by
\Cref{fig:full:core-typing-rules}.

\begin{figure}
  \maybesmall
  \centering
  $$
  \begin{array}{lcll}
    [[{a}]], [[{b}]] & \bnfeq & \ldots & \text{Type variables} \\
    [[{x}]], [[{y}]] & \bnfeq & \ldots & \text{Expression variables} \\
    [[{K}]] & \bnfeq & \ldots & \text{Data constructors} \\
    [[{s}]] & \bnfeq & [[{forall as. t}]] & \text{Type schemes} \\
    [[{t}]], [[{u}]] & \bnfeq & [[{a}]] \bnfor [[{exists as. t o- u}]] \bnfor [[{t1 ->_pi t2}]]
                            \bnfor [[{T ts}]] & \text{Types} \\
    [[{G}]], [[{D}]] & \bnfeq & [[{empty}]] \bnfor [[{G, x:_pi s}]] &
                                                              \text{Contexts} \\
    [[{e}]] & \bnfeq & [[{x}]] \bnfor [[{K}]] \bnfor [[{\x. e}]] \bnfor [[e1
                     e2]] \bnfor [[{pack (e1, e2)}]] & \text{Expressions}\\
                 &\bnfor & [[unpack (y,x)=e1 in
                           e2]] \bnfor [[{case_pi e of { alts }}]] &\\
                 &\bnfor & [[let_pi
                           x=e1 in e2]] \bnfor [[{let_pi x : s = e1 in e2}]] &
  \end{array}
  $$
  \caption{Grammar of the core calculus}
  \label{fig:full:core-grammar}
\end{figure}

\begin{figure}
  \maybesmall
  \centering
  \drules[L]{$[[G |- e : t]]$}{Core language
    typing}{Var,Abs,App,Pack,Unpack,Let,Case}
  \caption{Core calculus type system}
  \label{fig:full:core-typing-rules}
\end{figure}

\subsection{Desugaring}
\label{sec:appendix:desugaring}

The complete definition of the desugaring function from
\cref{sec:desugaring} can be found in
\Cref{fig:full:desugaring}.

For the sake of concision, we allow ourselves to write nested patterns
in case expressions of the core language. Desugaring nested patterns
into atomic case expression is routine.

In the complete description, we use a device which was omitted in the
main body of the article. Namely, we'll need a way to turn every
$[[{Ev(omega.Q)}]]$ into an $[[{Ur(Ev(Q))}]]$. For any
$[[e : Ev(omega.Q)]]$, we shall write $[[urify(Q;e) :
Ur(Ev(omega.Q))]]$. As a shorthand, particularly useful in nested
patterns, we will write $[[{case_pi e of {urified(Q;x) -> e'}}]]$ for
$[[{case_pi urify(Q;e) of {Ur x -> e'}}]]$.
%
$$
\left\{
  \begin{array}{lcl}
    [[{urify(Empty;e)}]]& = & [[{case_1 e of {() -> Ur ()}}]] \\
    [[{urify(1.q;e)}]] & = & [[{e}]] \\
    [[{urify(omega.q;e)}]] & = & [[{case_1 e of {Ur x -> Ur (Ur x)}}]] \\
    [[{urify(Q1*Q2;e)}]] & = & [[{case_1 e of {(urified(Q1;x), urified(Q2;y)) -> Ur (x,y)}}]]
  \end{array}
\right.
$$
We will omit the $[[{Q}]]$ in $[[{urify(Q;e)}]]$ and write
$[[{urify(e)}]]$ when it can be easily inferred from the context.


\begin{figure}
  \small
  \centering

%{
%format Ds(z)(j) = "\dsterm{\ottmv{" z "}}{" j "}"
%format * = "[[*]]"
%format ** = "\mathbin{⋅}"
%format ->. = "\to_{1}"
%format ->> = "\to_{\pi}"
%format ->>> = "\to_{\omega}"
%format Ev(x) = "\dsevidence{" x "}"
%format G = "[[{G}]]"
%format G1 = "[[{G1}]]"
%format G2 = "[[{G2}]]"
%format ListOf (b) = "\overline{" b "}"
%format Of (w) (a) (b) = a"\mathop{:_{" w "}}"b
%format OneOf (a) (b) = a"\mathop{:_{1}}"b
%format Q = "[[{Q}]]"
%format Q1 = "[[{Q1}]]"
%format Q2 = "[[{Q2}]]"
%format Sub t as bs = t "[" as "/" bs "]"
%format alts = "[[alts]]"
%format as = "[[{as}]]"
%format case_1 = "[[case_1]]"
%format case_omega = "[[case_omega]]"
%format e1 = "[[{e1}]]"
%format e2 = "[[{e2}]]"
%format ei = "[[{ei}]]"
%format let_1 = "[[let_1]]"
%format let_omega = "[[let_omega]]"
%format omega = "[[{omega}]]"
%format pi = "[[{pi}]]"
%format pii = "[[{pii}]]"
%format t = "[[{t}]]"
%format t1 = "[[{t1}]]"
%format t2 = "[[{t2}]]"
%format ts = "[[{ts}]]"
%format u = "[[{u}]]"
%format ui = "[[{ui}]]"
%format unpack = "\ottkw{unpack}"
%format urified(x) = "\underline{" x "}"
%format us = "[[{us}]]"
%format xi = "[[{xi}]]"
%format xsi = "[[{xsi}]]"
%format z1 = "[[{z1}]]"
%format z2 = "[[{z2}]]"
%format z2' = "[[{z2}]]''"
%format |- = "[[|-]]"
%format ||- = "[[||-]]"
$$
\left\{\;
\begin{minipage}{0.8\linewidth}
\begin{code}
Ds(z)(Q;G |- x : u[ts/as])  =
    x z
Ds(z)(Q;G |- \x.e : t1 ->> t2) =
  \x. Ds(z)(Q;G, Of pi x t1 |- e : t2)
Ds(z)(Q1*Q2; G1+G2 |- e1 e2 : t) =
  case_1 z of { (z1, z2) ->
    (Ds(z1)(Q1;G1 |- e1 : t1 ->. t)) (Ds(z2)(Q2;G2 |- e2 : t1)) }
Ds(z)(Q1* omega ** Q2; G1+ omega ** G2 |- e1 e2 : t) =
  case_1 z of { (z1, urified(z2)) ->
    (Ds(z1)(Q1;G1 |- e1 : t1 ->>> t)) (Ds(z2)(Q2;G2 |- e2 : t1)) }
Ds(z)(Q * Sub Q1 us as;G |- pack e : exists as. t .<= Q1) =
  case_1 z of { (z', z'') ->
    pack (z'', Ds(z')( Q ; G |- e : Sub t us as))}
Ds(z)(Q1 * Q2;G1 + G2 |- unpack x = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    unpack (z',x) = Ds(z1)(Q1;G1 |- e1 : exists as. t1 .<= Q) in
    let_1 z2' = (z2,z') in
    Ds(z2')(Q2 * Q;G2, OneOf x t1 |- e2 : t)}
Ds(z)(Q1 * Q2 ;G1+G2 |- let_1 x = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    let_1 x : Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1)
    in Ds(z2)(Q2;G2, OneOf x t1 |- e2 : t)}
Ds(z)(omega ** Q1 * Q2 ;omega ** G1+G2 |- let_omega x = e1 in e2 : t) =
  case_1 z of { (urified(z1), z2) ->
    let_omega x : Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1) in
    Ds(z2)(Q2;G2, Of omega x t1 |- e2 : t)}
Ds(z)(Q1 * Q2 ;G1+G2 |- let_1 x : forall as. Q =>. t1 = e1 in e2 : t) =
  case_1 z of { (z1, z2) ->
    let_1 x : forall as. Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1) in
    Ds(z2)(Q2;G2, OneOf x (forall as. Q =>. t1) |- e2 : t)}
Ds(z)(omega ** Q1 * Q2 ;omega ** G1+G2 |- let_omega x : forall as. Q =>. t1 = e1 in e2 : t) =
  case_1 z of { (urified(z1), z2) ->
    let_omega x : forall as. Ev(Q) ->. t1 = Ds(z1)(Q1*Q;G1 |- e1 : t1) in
    Ds(z2)(Q2;G2, Of omega x t1 |- e2 : t)}
Ds(z)(omega ** Q1*Q2;omega ** G1+G2 |- case_1 e of { alts } : t)  =
  case_1 z of { (urified(z1), z2) ->
    case_1 (Ds(z1)(Q1;G1 |- e : T ts)) of
      { ListOf(K xsi -> Ds(z2)( Q2; G2, ListOf(Of ((pi ** pii)) xi (Sub ui ts as)) |- ei : t))}}
Ds(z)(Q1*Q2;G1+G2 |- case_omega e of { alts } : t)  =
  case_1 z of { (z1, z2) ->
    case_omega (Ds(z1)(Q1;G1 |- e : T ts)) of
      { ListOf(K xsi -> Ds(z2)( Q2; G2, ListOf(Of ((pi**pii)) xi (Sub ui ts as)) |- ei : t))}}
\end{code}
\end{minipage}
\right.
$$

%}

  \caption{Desugaring}
  \label{fig:full:desugaring}
\end{figure}

\section{Proofs}
\label{sec:appendix:proofs-lemmas}

\setcounter{subsection}{4}
\subsection{Lemmas on the qualified type system}
\label{sec:appendix:qual-type-syst}

\begin{proof}[Proof of \cref{lem:q:scaling}]
  Let us prove separately the cases $[[{pi}]]=[[{1}]]$ and
  $[[{pi}]]=[[{omega}]]$.
  \begin{itemize}
  \item When $[[{pi}]]=[[{1}]]$, then $[[{pi.Q}]]=[[{Q}]]$ for all
    $[[{Q}]]$, hence $[[Q1||-Q2]]$ implies $[[pi.Q1 ||- pi.Q2]]$.
  \item For the case $[[{pi}]]=[[{omega}]]$, let us consider a few
    properties. First note that, for any $[[{Q}]]$,
    $[[{omega.Q}]]=[[{omega.Q * omega.Q}]]$. From which it follows,
    using the laws of \cref{def:entailment-relation}, that
    $[[omega.Q ||- Q1 * Q2]]$ if and only if $[[omega.Q ||- Q1]]$ and
    $[[omega.Q ||- Q2]]$.

    This means that to verify that
    $[[omega.Q1 ||- omega.Q2]]$, it is equivalent to prove that $[[omega.Q1
    ||- omega.q2]]$ for each
    $[[{q2}]]∈[[{UCtx}]]$ (letting $[[{omega.Q2}]]=[[{(UCtx,
      emptyset)}]]$). In turn, by \cref{def:entailment-relation} and
    observing that $[[{omega.(omega.Q1)}]] = [[{Q1}]]$, this is
    equivalent to $[[omega.Q1 ||- 1.q2]]$.

    This follows from the fact that $[[Q1 ||- Q2]]$ implies
    $[[omega.Q1 ||- Q2]]$ (\cref{def:entailment-relation}) and the
    property, shown above, that $[[omega.Q1 ||- Q2 * Q2']]$ if and
    only if $[[omega.Q1 ||- Q2]]$ and $[[omega.Q1 ||- Q2']]$.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{lem:q:scaling-inversion}]
  Let us prove separately the cases $[[{pi}]]=[[{1}]]$ and
  $[[{pi}]]=[[{omega}]]$.
  \begin{itemize}
  \item When $[[{pi}]]=[[{1}]]$, then $[[{pi.Q}]]=[[{Q}]]$ for all
    $[[{Q}]]$, in particular $[[Q1||- 1.Q2]]$ implies that
    $[[{Q1}]]=[[{1.Q1}]]$ with $[[Q1 ||- Q2]]$.
  \item When $[[{pi}]]=[[{omega}]]$, then let us first remark, letting
    $[[{omega.Q2}]]=[[{(UCtx, emptyset)}]]$ that, by a straightforward
    induction on the cardinality of $[[{UCtx}]]$ it is sufficient to
    prove that the result holds for atomic constraints.

    That is, we need to prove that if $[[Q1 ||- omega.q2]]$ then
    there exists $[[{Q'}]]$ such that $[[{Q1}]]=[[{omega.Q'}]]$ and
    $[[Q' ||- rho.q2]]$ (for all $[[{rho}]]$).

    This result, in turns, holds by \cref{def:entailment-relation}.
  \end{itemize}
\end{proof}

\begin{lemma}\label{lem:simples:monoid-action}
  The following equality holds $[[{pi.(rho.Q)}]]=[[{(pi.rho).Q}]]$
\end{lemma}
\begin{proof}
  Immediate by case analysis of $[[{pi}]]$ and $[[{rho}]]$.
\end{proof}

\setcounter{subsection}{5}
\subsection{Lemmas on constraint inference}
\label{sec:appendix:constraint-inference}

\begin{proof}[Proof of \cref{lem:inversion}]
  The cases $[[Q |- C1 & C2]]$ and $[[Q |- pi.(Q2 => C)]]$ are
  immediate, since there is only one rule (\rref*{C-With} and
  \rref*{C-Impl} respectively) which can have them as their
  conclusion.

  For $[[Q |- C1 * C2]]$ we have two cases:
  \begin{itemize}
  \item either it is the conclusion of a \rref*{C-Tensor} rule, and
    the result is immediate.
  \item or it is the result of a \rref*{C-Dom} rule, in which case we
    have $[[{C1}]]=[[{Q1}]]$, $[[{C2}]]=[[{Q2}]]$, and the result follows from
    \cref{def:entailment-relation}.
  \end{itemize}
\end{proof}


\begin{proof}[Proof of \cref{lem:wanted:promote}]
  By induction on the syntax of $[[{C}]]$
  \begin{itemize}
  \item If $[[{C}]]=[[{Q'}]]$, then the result follows
    from \cref{lem:q:scaling}
  \item If $[[{C}]]=[[{C1*C2}]]$, then we can prove the result like we
    proved the corresponding case in \cref{lem:q:scaling},
    using \cref{lem:inversion}.
  \item If $[[{C}]]=[[{C1&C2}]]$, then we the case where $[[{pi}]]=[[{1}]]$ is
    immediate, so we can assume without loss of generality that
    $[[{pi}]]=[[{omega}]]$, and, therefore, that $[[{pi.C}]] = [[pi.C1 *
    pi.C2]]$.
    By \cref{lem:inversion}, we have that $[[Q|-C1]]$ and
    $[[Q|-C2]]$; hence, by induction, $[[omega.Q |- omega.C1]]$ and
    $[[omega.Q |- omega.C1]]$.
    Then, by definition of the entailment relation, we have $[[omega.Q
    * omega.Q |- omega.C1 * omega.C2]]$, which concludes,
    since $[[{omega.Q}]] = [[{omega.Q * omega.Q}]]$.
  \item If $[[{C}]]=[[{rho.(Q1 => C')}]]$, then by
    \cref{lem:inversion}, there is a $[[{Q'}]]$ such that
    $[[{Q}]]=[[{pi.Q'}]]$ and $[[Q'*Q1 |- C']]$. Applying
    rule~\rref*{C-Impl} with $[[{pi.rho}]]$, we get
    $[[(pi.rho).Q' |- (pi.rho).(Q1 => C')]]$.

    In other words: $[[pi.Q |- pi.(rho.(Q=>C))]]$ as expected.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{lem:wanted:demote}]
  By induction on the syntax of $[[{C}]]$
  \begin{itemize}
  \item If $[[{C}]]=[[{Q'}]]$, then the result follows from
    \cref{lem:q:scaling-inversion}
  \item If $[[{C}]]=[[{C1*C2}]]$, then we can prove the result like we
    proved the corresponding case in
    \cref{lem:q:scaling-inversion} using
    \cref{lem:inversion}.
  \item If $[[{C}]]=[[{C1&C2}]]$, then we the case where $[[{pi}]]=[[{1}]]$ is
    immediate, so we can assume without loss of generality that
    $[[{pi}]]=[[{omega}]]$, and, therefore, that
    $[[{pi.C}]] = [[{pi.C1 * pi.C2}]]$. By \cref{lem:inversion},
    there exist $[[{Q1}]]$ and $[[{Q2}]]$ such that $[[Q1|- omega.C1]]$,
    $[[Q2|- omega.C2]]$ and $[[{Q}]]=[[{Q1 * Q2}]]$. By induction
    hypothesis, we get $[[{Q1}]] = [[{omega.Q1'}]]$ and $[[{Q2}]] = [[{omega.Q2'}]]$
    such that $[[Q1' |- C1]]$ and $[[Q2' |- C2]]$. From which it
    follows that $[[omega.Q1'*omega.Q2' |- C1]]$ and
    $[[omega.Q1'*omega.Q2' |- C1]]$ (by
    \cref{lem:wanteds:weakening}) and, finally,
    $[[{Q}]]=[[{omega.Q}]]$ (by \cref{lem:wanteds:module-action})
    and $[[Q |- C1 & C2]]$.
  \item If $[[{C}]]=[[{rho.(Q1 => C')}]]$, then
    $[[{pi.C}]] = [[{(pi.rho). (Q1 => C')}]]$. The result follows
    immediately by \cref{lem:inversion}.
  \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{lem:generation-soundness}]
  By induction on $[[G |-> e : t ~> C]]$
  \begin{description}
  \item[\rref*{G-Var}] We have
    \begin{itemize}
    \item $[[G1 = x :_1 forall as. Q =o u]]$
    \item $[[G1 + omega.G2 |-> x : u[ts/as] ~> Q[ts/as] ]]$
    \item $[[Q_g |- Q[ts/as] ]]$
    \end{itemize}
    Therefore, by rules~\rref*{E-Var} and~\rref*{E-Sub}, it follows
    immediately that $[[Q_g ; G1 + omega.G2 |- x : u[ts/as] ]]$
  \item[\rref*{G-Abs}] We have
    \begin{itemize}
    \item $[[G |-> \x. e : t0 ->_pi t ~> C]]$
    \item $[[Q_g |- C]]$
    \item $[[G, x:_pi t0 |-> e : t ~> C]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_g; G, x:_pi t0 |- e : t]]$
    \end{itemize}
    From which follows that $[[Q_g; G |- \x. e : t0 ->_pi t]]$.
  \item[\rref*{G-Let}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x = e1 in e2 : t ~> pi.C1 * C2]]$
    \item $[[Q_g |- pi.C1 * C2]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \end{itemize}
    By \cref{lem:inversion,lem:wanted:demote}, there exist $[[{Q1}]]$ and $[[{Q_2}]]$ such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 |- C2]]$
    \item $[[{Q_g}]] = [[{pi.Q_1 * Q_2}]]$
    \end{itemize}
    By induction hypothesis we have
    \begin{itemize}
    \item $[[Q_1 ; G1 |- e1 : t1]]$
    \item $[[Q_2; G2, x:_pi  t1 |- e1 : t1]]$
    \end{itemize}
    From which follows that $[[Q_g; pi.G1+G2 |- let_pi x = e1 in e2 :
    t]]$.
  \item[\rref*{G-LetSig}] We have
    \begin{itemize}
    \item $[[pi.G1+G2 |-> let_pi x : forall as. Q =o t1 = e1 in e2 : t ~>
      C2 * pi.(Q => C1)]]$
    \item $[[Q_g |- C2 * pi.(Q => C1)]]$
    \item $[[G1 |-> e1 : t1 ~> C1]]$
    \item $[[G2, x:_pi forall as. Q =o t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By \cref{lem:inversion,lem:wanted:demote}, there exist $[[{Q1}]]$, $[[{Q2}]]$ such
    that
    \begin{itemize}
    \item $[[Q2 |- C2]]$
    \item $[[Q1*Q |- C]]$
    \item $[[{Q_g}]] = [[{pi.Q1*Q2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1*Q;G1 |- e1 : t1]]$
    \item $[[Q2; G2, x:_pi forall as. Q =o t1 |- e2 : t]]$
    \end{itemize}
    Hence $[[Q_g; pi.G1+G2 |- let_pi x : forall as. Q =o t1 = e1 in e2 : t]]$
  \item[\rref*{G-App}] \info{Most of the linearity problems are in the App
      rule. Unpack is also relevant.}
    We have
    \begin{itemize}
    \item $[[G1+pi.G2 |-> e1 e2 : t ~> C1 * pi.C2]]$
    \item $[[Q_g |- C1 * pi.C2]]$
    \item $[[G1 |-> e1 : t2 ->_pi t ~> C1]]$
    \item $[[G2 |-> e2 : t2 ~> C2]]$
    \end{itemize}
    By \cref{lem:inversion,lem:wanted:demote}, there exist $[[{Q1}]]$, $[[{Q2}]]$ such that
    \begin{itemize}
    \item $[[Q1 |- C1]]$
    \item $[[Q2 |- C2]]$
    \item $[[{Q_g}]] = [[{Q1 * pi.Q2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q1; G1 |- e1 : t2 ->_pi t]]$
    \item $[[Q2; G2 |- e2 : t2]]$
    \end{itemize}
    Hence $[[Q_g; G1+pi.G2 |- e1 e2 : t]]$.
  \item[\rref*{G-Pack}] We have
    \begin{itemize}
    \item $[[G |-> pack e : exists as. t o= Q ~> C * Q[us/as] ]]$
    \item $[[Q_g |- C * Q[us/as] ]]$
    \item $[[G |-> e : t[us/as] ~> C]]$
    \end{itemize}
    By \cref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Q_2}]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C]]$
    \item $[[Q_2 |- Q[us/as] ]]$
    \item $[[{Q_g}]] = [[{Q_1*Q_2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1 ; G |- e : t[us/as] ]]$
    \end{itemize}
    So we have $[[Q_1 * Q[us/as] ; G |- pack e : exists as. t o=
    Q]]$. By rule~\rref*{E-Sub}, we conclude
    $[[Q_g ; omega.G |- pack e : exists as. t o= Q]]$.
  \item[\rref*{G-Unpack}] We have
    \begin{itemize}
    \item $[[G1+G2 |-> unpack x = e1 in e2 : t ~> C1 * 1.(Q' => C2)]]$
    \item $[[Q_g |- C1 * 1.(Q' => C2)]]$
    \item $[[G1 |-> e1 : exists as. t1 o= Q' ~> C1]]$
    \item $[[G2, x:_pi t1 |-> e2 : t ~> C2]]$
    \end{itemize}
    By \cref{lem:inversion}, there exist $[[{Q_1}]]$, $[[{Q_2}]]$
    such that
    \begin{itemize}
    \item $[[Q_1 |- C1]]$
    \item $[[Q_2 * Q' |- C2]]$
    \item $[[{Q_g}]] = [[{Q1 * Q2}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q_1; G1 |- e1 : exists as. t1 o= Q']]$
    \item $[[Q_2*Q ; G2 |- e2 : t]]$
    \end{itemize}
    Therefore $[[Q_g ; G1 + G2 |- unpack x = e1 in e2 : t]]$.
  \item[\rref*{G-Case}] We have
    \begin{itemize}
    \item $[[pi.G + D |-> case_pi e of {alts} : t ~> pi.C * && Ci]]$
    \item $[[Q_g |- pi.C * && Ci]]$
    \item $[[G |-> e : T ss ~> C]]$
    \item For each $i$, $[[D, <xi:_(pi.pii) ui[ss/as]> |-> ei : t ~> Ci]]$
    \end{itemize}
    By repeated uses of \cref{lem:inversion} as well as \cref{lem:wanted:demote}, there exist
    $[[{Q}]]$, $[[{Q'}]]$ such that
    \begin{itemize}
    \item $[[Q |- C]]$
    \item For each $i$, $[[Q' |- Ci]]$
    \item $[[{Q_g}]] = [[{pi.Q * Q'}]]$
    \end{itemize}
    By induction hypothesis
    \begin{itemize}
    \item $[[Q; G |- e : T ss]]$
    \item For each $i$, $[[Q';D, <xi:_(pi.pii) ui[ss/as]> |- ei : t]]$
    \end{itemize}
    Therefore $[[Q_g ; pi.G + D |- case_pi e of {alts} : t]]$.
  \end{description}
\end{proof}

\begin{proof}[Proof of \cref{lem:solver-soundness}]
  By induction on $[[UCtx; LCtx_i |-s C ~> LCtx_o]]$
  \begin{description}
  \item[\rref*{S-Atom}] We have
  \begin{itemize}
          \item $[[UCtx ; LCtx_i |-s pi.q ~> LCtx_o]]$
          \item $[[UCtx ; LCtx_i |-simp pi.q ~> LCtx_o]]$
  \end{itemize}
  By \cref{prop:atomic-solver-soundness} we have
  \begin{enumerate}
  \item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
  \item $[[(UCtx, LCtx_i) ||- pi.q * (emptyset, LCtx_o)]]$
  \end{enumerate}
  Then by \rref*{C-Dom} we have $[[(UCtx, LCtx_i) |- pi.q * (emptyset, LCtx_o)]]$.
  \item[\rref*{S-Add}] We have
  \begin{itemize}
          \item $[[UCtx ; LCtx_i |-s C1 & C2 ~> LCtx_o]]$
          \item $[[UCtx ; LCtx_i |-s C1 ~> LCtx_o]]$
          \item $[[UCtx ; LCtx_i |-s C2 ~> LCtx_o]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$
          \item $[[(UCtx, LCtx_i) |- C1 * (emptyset, LCtx_o)]]$
          \item $[[(UCtx, LCtx_i) |- C2 * (emptyset, LCtx_o)]]$
  \end{itemize}
  Then by \rref*{C-With} we have $[[(UCtx, LCtx_i) |- C1 & C2 * (emptyset, LCtx_o)]]$.
  \item[\rref*{S-Mult}] We have
  \begin{itemize}
          \item $[[UCtx ; LCtx_i |-s C1 * C2 ~> LCtx_o]]$
          \item $[[UCtx ; LCtx_i |-s C1 ~> LCtx_o']]$
          \item $[[UCtx ; LCtx_o' |-s C2 ~> LCtx_o]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[{LCtx_o}]]\subseteq [[{LCtx_o'}]]$
          \item $[[{LCtx_o'}]]\subseteq [[{LCtx_i}]]$
          \item $[[(UCtx, LCtx_i) |- C1 * (emptyset, LCtx_o')]]$
          \item $[[(UCtx, LCtx_o') |- C2 * (emptyset, LCtx_o)]]$
  \end{itemize}
  Then by transitivity of $\subseteq$ we have $[[{LCtx_o}]]\subseteq [[{LCtx_i}]]$, and
  by \rref*{C-Tensor} we have
  $[[(UCtx, LCtx_i) * (UCtx , LCtx_o') |- C1 * C2 * (emptyset, LCtx_o') * (emptyset, LCtx_o)]]$
   by \cref{lem:inversion} we have $[[(UCtx, LCtx_i) |- C1 * C2 * (emptyset, LCtx_o)]]$.
  \item[\rref*{S-ImplOne}] We have
  \begin{itemize}
          \item $[[UCtx ; LCtx_i |-s 1.((UCtx0, LCtx0) => C) ~> LCtx_o]]$
          \item $[[UCtx \u UCtx0 ; LCtx_i \u LCtx0 |-s C ~> LCtx_o]]$
          \item $[[LCtx_o \subseteq LCtx_i]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[(UCtx \u UCtx0, LCtx_i \u LCtx0) |- C * (emptyset, LCtx_o)]]$
          \item $[[LCtx_o \subseteq LCtx_i \u LCtx0]]$
  \end{itemize}
  Then we know that
  $[[{(emptyset, LCtx_i)}]] = [[{(emptyset, LCtx_o) * (emptyset, LCtx_i')}]]$ for
  some $[[{LCtx_i'}]]$.
  Then by \cref{lem:inversion} we know that $[[(UCtx \u UCtx0, LCtx_i' \u LCtx0) |- C]]$ and by
  \rref*{C-Impl} we have $[[(UCtx, LCtx_i') |- 1.((UCtx0, LCtx0) => C)]]$.
  Finally, by \rref*{C-Tensor} we conclude that $[[(UCtx, LCtx_i) |- 1.((UCtx0, LCtx0) => C) * (emptyset, LCtx_o)]]$
  \item[\rref*{S-ImplMany}] We have
  \begin{itemize}
          \item $[[UCtx ; LCtx_i |-s omega.((UCtx0, LCtx0) => C) ~> LCtx_i]]$
          \item $[[UCtx \u UCtx0 ; LCtx0 |-s C ~> emptyset]]$
  \end{itemize}
  By induction hypothesis we have
  \begin{itemize}
          \item $[[(UCtx \u UCtx0, LCtx0) |- C * (emptyset, emptyset)]]$
  \end{itemize}
  Then by \cref{lem:inversion} we have $[[(UCtx \u UCtx0, LCtx0) |- C]]$ and by
  \rref*{C-Impl} $[[(UCtx, emptyset) |- omega.((UCtx0, LCtx0) => C)]]$ and finally by
  \rref{C-Tensor} we have $[[(UCtx, LCtx_i) |- omega.((UCtx0, LCtx0) => C) * (emptyset, LCtx_i)]]$.
  $[[LCtx_i \subseteq LCtx_i]]$ holds trivially.
  \end{description}
\end{proof}

\begin{lemma}[Weakening of wanteds]\label{lem:wanteds:weakening}
  If $[[Q |- C]]$, then $[[omega.Q'*Q |- C]]$
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the derivation of
  $[[Q |- C]]$, using the corresponding property on the
  simple-constraint entailment relation from
  \cref{def:entailment-relation}, for the \rref*{C-Dom} case.
\end{proof}

\begin{lemma}\label{lem:wanteds:module-action}
  The following equality holds: $[[{pi.(rho.C)}]]=[[{(pi.rho).C}]]$.
\end{lemma}
\begin{proof}
  This is proved by a straightforward induction on the structure of
  $[[{C}]]$, using \cref{lem:simples:monoid-action} for the case
  $[[{C}]]=[[{Q}]]$.
\end{proof}
\end{document}

% Local Variables:
% ispell-dictionary: "british"
% End:


% LocalWords:  sequent typechecker idempotence polymorphism desugar
% LocalWords:  desugaring ghc OutsideIn quotiented gadt typeable
% LocalWords:  combinator sigils equalities wanteds intuitionistic
% LocalWords:  sequents implicational deallocate deallocating monadic
% LocalWords:  deallocated instantiations desugars
